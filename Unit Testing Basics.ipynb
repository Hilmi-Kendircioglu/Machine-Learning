{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Unit Testing Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unit Test Libraries**\n",
    "\n",
    "- pytest\n",
    "\n",
    "- unittest\n",
    "\n",
    "- nosetests\n",
    "\n",
    "- doctest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spotting and fixing bugs**\n",
    "\n",
    "To find bugs in functions, you need to follow a four step procedure.\n",
    "\n",
    "- Write unit tests.\n",
    "\n",
    "- Run them.\n",
    "\n",
    "- Read the test result report and spot the bugs.\n",
    "\n",
    "- Fix the bugs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your first unit test using pytest\n",
    "\n",
    "# Import the pytest package\n",
    "import pytest\n",
    "\n",
    "# Import the function convert_to_int()\n",
    "from preprocessing_helpers import convert_to_int\n",
    "\n",
    "# Complete the unit test name by adding a prefix\n",
    "def test_on_string_with_one_comma():\n",
    "  # Complete the assert statement\n",
    "  assert convert_to_int(\"2,081\") == 2081"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running unit tests\n",
    "\n",
    "!pytest test_convert_to_int.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An exception is raised when running the unit test. This could be an AssertionError raised by the assert statement or another exception, e.g. NameError, which is raised before the assert statement can run. \n",
    "\n",
    "If you get an AssertionError, this means the function has a bug and you should fix it. If you get another exception, e.g. NameError, this means that something else is wrong with the unit test code and you should fix it so that the assert statement can actually run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convert_to_int() function is defined in the file preprocessing_helpers.py. \n",
    "\n",
    "The unit test is available in the test module test_convert_to_int.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spotting and fixing bugs\n",
    "\n",
    "def convert_to_int(string_with_comma):\n",
    "    return int(string_with_comma.replace(\",\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guess function's purpose by reading unit tests\n",
    "\n",
    "!cat test_row_to_list.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All Benefits**\n",
    "\n",
    "- Time savings\n",
    "\n",
    "- Improved documentation\n",
    "\n",
    "- More trust\n",
    "\n",
    "- Reduced downtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Intermediate Unit Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theoretical structure of an assertion \n",
    "\n",
    "assert boolean_expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The optional message argument\n",
    "\n",
    "assert boolean_expression, message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a message to a unit test\n",
    "\n",
    "# test module: test_row_to_list.py\n",
    "\n",
    "import pytest\n",
    "\n",
    "\n",
    "def test_for_missing_area_with_message():\n",
    "    \n",
    "    actual = row_to_list(\"\\t293,410\\n\")\n",
    "    \n",
    "    expected = None\n",
    "    \n",
    "    message = (\"row_to_list('\\t293,410\\n') \"\n",
    "               \"returned {0} instead \"\n",
    "               \"of {1}\".format(actual, expected)\n",
    "              )\n",
    "    \n",
    "    assert actual is expected, message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "from preprocessing_helpers import convert_to_int\n",
    "\n",
    "def test_on_string_with_one_comma():\n",
    "    \n",
    "    test_argument = \"2,081\"\n",
    "   \n",
    "    expected = 2081\n",
    "    \n",
    "    actual = convert_to_int(test_argument)\n",
    "    \n",
    "    # Format the string with the actual return value\n",
    "    message = \"convert_to_int('2,081') should return the int 2081, but it actually returned {0}\".format(actual)\n",
    "    \n",
    "    # Write the assert statement which prints message on failure\n",
    "    assert actual == expected, message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beware of float return values: Use pytest.approx() to wrap expected return value.\n",
    "\n",
    "assert 0.1 + 0.1 + 0.1 == pytest.approx(0.3)\n",
    "\n",
    "assert np.array([0.1+0.1, 0.1+0.1+0.1]) == pytest.approx(np.array([0.2,0.3])) <- Numpy arrays containing floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pytest\n",
    "from as_numpy import get_data_as_numpy_array\n",
    "\n",
    "def test_on_clean_file():\n",
    "    \n",
    "  expected = np.array([[2081.0, 314942.0],\n",
    "                       [1059.0, 186606.0],\n",
    "  \t\t\t\t\t   [1148.0, 206186.0]\n",
    "                       ]\n",
    "                      )\n",
    "\n",
    "  actual = get_data_as_numpy_array(\"example_clean_data.txt\", num_columns=2)\n",
    "    \n",
    "  message = \"Expected return value: {0}, Actual return value: {1}\".format(expected, actual)\n",
    "\n",
    "  # Complete the assert statement\n",
    "  assert actual == pytest.approx(expected), message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytest.approx() function not only works for NumPy arrays containing floats, but also for lists and dictionaries containing floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple assertions in one unit test\n",
    "\n",
    "# test_module: test_convert_to_int.py\n",
    "\n",
    "import test_on_string_with_one_comma():\n",
    "    \n",
    "    return_value = convert_to_int(\"2,081\")\n",
    "    \n",
    "    assert isinstance(return_value, int)\n",
    "    \n",
    "    assert return_value == 2081"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_on_six_rows():\n",
    "    example_argument = np.array([[2081.0, 314942.0], [1059.0, 186606.0],\n",
    "                                 [1148.0, 206186.0], [1506.0, 248419.0],\n",
    "                                 [1210.0, 214114.0], [1697.0, 277794.0]]\n",
    "                                )\n",
    "    \n",
    "    # Fill in with training array's expected number of rows\n",
    "    expected_training_array_num_rows = 4\n",
    "    \n",
    "    # Fill in with testing array's expected number of rows\n",
    "    expected_testing_array_num_rows = 2\n",
    "    \n",
    "    actual = split_into_training_and_testing_sets(example_argument)\n",
    "   \n",
    "    # Write the assert statement checking training array's number of rows\n",
    "    assert actual[0].shape[0] == expected_training_array_num_rows, \"The actual number of rows in the training array is not {}\".format(expected_training_array_num_rows)\n",
    "   \n",
    "    # Write the assert statement checking testing array's number of rows\n",
    "    assert actual[1].shape[0] == expected_testing_array_num_rows, \"The actual number of rows in the testing array is not {}\".format(expected_testing_array_num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing for exceptions instead of return values\n",
    "\n",
    "# Test if split_into_training_and_testing_set() raises ValueError with one dimensional argument. \n",
    "\n",
    "def test_valueerror_on_one_dimensional_argument():\n",
    "    example_argument = np.array([2081, 314942, 1059, 186606, 1148, 206186])\n",
    "    with pytest.raises(ValueError):\n",
    "    split_into_training_and_testing_sets(example_argument)\n",
    "    \n",
    "# If function raises expected ValueError ,test will pass.\n",
    "\n",
    "# If function is buggy and does not raise ValueError ,test will fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theoretical Structure of a with Statement\n",
    "\n",
    "with pytest.raises(ValueError):  <--- context_manager\n",
    "    # <--- Does nothing on entering the context\n",
    "    print(\"This is part of the context\") # any code inside is the context\n",
    "    # <--- if context raised ValueError, silence it.\n",
    "    # <--- if the context did not raise ValueError, raise an exception. \n",
    "    \n",
    "with pytest.raises(ValueError):\n",
    "    raise ValueError # context exits with ValueError\n",
    "# <--- pytest.raises(ValueError) silences it\n",
    "\n",
    "with pytest.raises(ValueError):\n",
    "    pass # context exits without raising a Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the error message\n",
    "\n",
    "def test_valueerror_on_one_dimensional_argument():\n",
    "    \n",
    "    example_argument = np.array([2081, 314942, 1059, 186606, 1148, 206186])\n",
    "    \n",
    "    with pytest.raises(ValueError) as exception_info: # store the exception\n",
    "        \n",
    "    split_into_training_and_testing_sets(example_argument)\n",
    "    \n",
    "    # Check if ValueError contains correct message\n",
    "    assert exception_info.match(\"Argument data array must be two dimensional. \"\n",
    "                                \"Got 1 dimensional array instead!\"\n",
    "                                )\n",
    "\n",
    "# exception_info stores the ValueError.\n",
    "# exception_info.match(expected_msg) checks if expected_msg is present in the actual error message.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice the context manager\n",
    "\n",
    "*** Complete the with statement by filling in with a context manager that will silence the ValueError raised in the context.\n",
    "\n",
    "import pytest\n",
    "\n",
    "# Fill in with a context manager that will silence the ValueError\n",
    "with pytest.raises(ValueError):\n",
    "    raise ValueError\n",
    "    \n",
    "*** Complete the with statement with a context manager that raises Failed if no OSError is raised in the context.\n",
    "\n",
    "import pytest\n",
    "\n",
    "try:\n",
    "    # Fill in with a context manager that raises Failed if no OSError is raised\n",
    "    with pytest.raises(OSError):\n",
    "        raise ValueError\n",
    "except:\n",
    "    print(\"pytest raised an exception because no OSError was raised in the context.\")\n",
    "    \n",
    "*** Extend the with statement so that any raised ValueError is stored in the variable exc_info.\n",
    "\n",
    "import pytest\n",
    "\n",
    "# Store the raised ValueError in the variable exc_info\n",
    "with pytest.raises(ValueError) as exc_info:\n",
    "    raise ValueError(\"Silence me!\")\n",
    "    \n",
    "*** Write an assert statement to check if the raised ValueError contains the message \"Silence me!\".\n",
    "\n",
    "import pytest\n",
    "\n",
    "with pytest.raises(ValueError) as exc_info:\n",
    "    raise ValueError(\"Silence me!\")\n",
    "# Check if the raised ValueError contains the correct message\n",
    "assert exc_info.match(\"Silence me!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unit test a ValueError**\n",
    "\n",
    "Sometimes, you want a function to raise an exception when called on bad arguments. This prevents the function from returning nonsense results or hard-to-interpret exceptions. This is an important behavior which should be unit tested.\n",
    "\n",
    "Remember the function split_into_training_and_testing_sets()? It takes a NumPy array containing housing area and prices as argument. The function randomly splits the array row wise into training and testing arrays in the ratio 3:1, and returns the resulting arrays in a tuple.\n",
    "\n",
    "If the argument array has only 1 row, the testing array will be empty. To avoid this situation, you want the function to not return anything, but raise a ValueError with the message \"Argument data_array must have at least 2 rows, it actually has just 1\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pytest\n",
    "from train import split_into_training_and_testing_sets\n",
    "\n",
    "def test_on_one_row():\n",
    "    test_argument = np.array([[1382.0, 390167.0]])\n",
    "    # Fill in with a context manager for checking ValueError\n",
    "    # Store information about raised ValueError in exc_info()\n",
    "    with pytest.raises(ValueError) as exc_info():\n",
    "      split_into_training_and_testing_sets(test_argument)\n",
    "    # Check if the raised ValueError contains the correct message\n",
    "    assert exc_info.match(expected_error_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The well tested function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best practice is to pick a few from each of the following categories of arguments, which are called BAD ARGUMENTS, SPECIAL ARGUMENTS, and NORMAL ARGUMENTS. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bad Arguments: Arguments for which the function raises an exception instead of returning a value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Special Arguments: Boundary values and for some arguments, function uses special logic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal Arguments: In production, the function will be most frequently called with normal arguments. Therefore, this case needs to be tested thoroughly, and testing with just one normal argument is not enough. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write unit test for boundary values\n",
    "\n",
    "import pytest\n",
    "from preprocessing_helpers import row_to_list\n",
    "\n",
    "def test_on_no_tab_no_missing_value():    # (0, 0) boundary value\n",
    "    # Assign actual to the return value for the argument \"123\\n\"\n",
    "    actual = row_to_list(\"123\\n\")\n",
    "    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n",
    "    \n",
    "def test_on_two_tabs_no_missing_value():    # (2, 0) boundary value\n",
    "    actual = row_to_list(\"123\\t4,567\\t89\\n\")\n",
    "    # Complete the assert statement\n",
    "    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n",
    "    \n",
    "def test_on_one_tab_with_missing_value():    # (1, 1) boundary value\n",
    "    actual = row_to_list(\"\\t4,567\\n\")\n",
    "    # Format the failure message\n",
    "    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write unit test for special behavior\n",
    "\n",
    "import pytest\n",
    "from preprocessing_helpers import row_to_list\n",
    "\n",
    "def test_on_no_tab_with_missing_value():    # (0, 1) case\n",
    "    # Assign to the actual return value for the argument \"\\n\"\n",
    "    actual = row_to_list(\"\\n\")\n",
    "    # Write the assert statement with a failure message\n",
    "    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)\n",
    "    \n",
    "def test_on_two_tabs_with_missing_value():    # (0, 1) case\n",
    "    # Assign to the actual return value for the argument \"123\\t\\t89\\n\"\n",
    "    actual = row_to_list(\"123\\t\\t89\\n\")\n",
    "    # Write the assert statement with a failure message\n",
    "    assert actual is None, \"Expected: None, Actual: {0}\".format(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write unit test for normal arguments\n",
    "\n",
    "import pytest\n",
    "from preprocessing_helpers import row_to_list\n",
    "\n",
    "def test_on_normal_argument_1():\n",
    "    actual = row_to_list(\"123\\t4,567\\n\")\n",
    "    # Fill in with the expected return value for the argument \"123\\t4,567\\n\"\n",
    "    expected = [\"123\", \"4,567\"]\n",
    "    assert actual == expected, \"Expected: {0}, Actual: {1}\".format(expected, actual)\n",
    "    \n",
    "def test_on_normal_argument_2():\n",
    "    actual = row_to_list(\"1,059\\t186,606\\n\")\n",
    "    expected = [\"1,059\", \"186,606\"]\n",
    "    # Write the assert statement along with a failure message\n",
    "    assert actual == expected, \"Expected: {0}, Actual: {1}\".format(expected, actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test Driven Development (TDD)**\n",
    "\n",
    "1. Write unit tests and fix requirements\n",
    "\n",
    "2. Run tests and watch it fail\n",
    "\n",
    "3. Implement function and run tests again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TDD: Requirement collection --> Special Argument\n",
    "\n",
    "def test_with_no_comma():\n",
    "    actual = convert_to_int(\"756\")\n",
    "    # Complete the assert statement\n",
    "    assert actual == 756, \"Expected: 756, Actual: {0}\".format(actual)\n",
    "    \n",
    "def test_with_one_comma():\n",
    "    actual = convert_to_int(\"2,081\")\n",
    "    # Complete the assert statement\n",
    "    assert actual == 2081, \"Expected: 2081, Actual: {0}\".format(actual)\n",
    "    \n",
    "def test_with_two_commas():\n",
    "    actual = convert_to_int(\"1,034,891\")\n",
    "    # Complete the assert statement\n",
    "    assert actual == 1034891, \"Expected: 1034891, Actual: {0}\".format(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TDD: Implement the function\n",
    "\n",
    "def convert_to_int(integer_string_with_commas):\n",
    "    \n",
    "    comma_separated_parts = integer_string_with_commas.split(\",\")\n",
    "    \n",
    "    for i in range(len(comma_separated_parts)):\n",
    "        \n",
    "        # Write an if statement for checking missing commas\n",
    "        if len(comma_separated_parts[i]) > 3:\n",
    "            return None\n",
    "        \n",
    "        # Write the if statement for incorrectly placed commas\n",
    "        if i != 0 and len(comma_separated_parts[i]) != 3:\n",
    "            return None\n",
    "        \n",
    "        integer_string_without_commas = \"\".join(comma_separated_parts)\n",
    "    try:\n",
    "        return int(integer_string_without_commas)\n",
    "    \n",
    "    # Fill in with the correct exception for float valued argument strings\n",
    "    except ValueError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Test Organization and Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tests folder mirrors the application folder.\n",
    "\n",
    "Python module and test module correspondence.\n",
    "\n",
    "Test class is just a simple container for tests of a specific function. The name of the class should be in CamelCase, and should always start with \"Test\". The best way to name a test class is to follow the 'Test' with the name of the function.\n",
    "\n",
    "Final test directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test class\n",
    "\n",
    "import pytest\n",
    "import numpy as np\n",
    "\n",
    "from models.train import split_into_training_and_testing_sets\n",
    "\n",
    "# Declare the test class\n",
    "class TestSplitIntoTrainingAndTestingSets(object):\n",
    "    # Fill in with the correct mandatory argument\n",
    "    def test_on_one_row(self):\n",
    "        test_argument = np.array([[1382.0, 390167.0]])\n",
    "        with pytest.raises(ValueError) as exc_info:\n",
    "            split_into_training_and_testing_sets(test_argument)\n",
    "        expected_error_msg = \"Argument data_array must have at least 2 rows, it actually has just 1\"\n",
    "        assert exc_info.match(expected_error_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Organization:\n",
    "    \n",
    "- The centerpiece is the tests folder, which holds all tests for the project.\n",
    "\n",
    "- The folder contains mirror packages, each of which contain a test module. \n",
    "\n",
    "- Test modules contain many test classes.\n",
    "\n",
    "- A test class is just a container for unit tests for a particular function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Running all the tests:\n",
    "    \n",
    "simply change the tests directory --- > cd tests\n",
    "\n",
    "run the command --- > pytest\n",
    "\n",
    "* Recurses into directory subtree of tests/ . \n",
    "\n",
    "  - Filenames starting with test_ → test module. \n",
    "    \n",
    "  - Classnames starting with Test → test class. \n",
    "\n",
    "  - Function names starting with test_ → unit test. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytest -x ---> stop after first failure. It can save and time. \n",
    "\n",
    "pytest data/test_preprocessing_helpers.py ---> Running tests in a test module\n",
    "\n",
    "During automatic test discovery, pytest assigns a node ID to every test class and unit test that it encounters. \n",
    "\n",
    "- The node ID of a test class is the path to the test module followed by the name of the test class, separated by two colons. \n",
    "\n",
    "- The node ID of a unit test follows the same format, with the unit test name added to the end using another double colon separator. \n",
    "\n",
    "pytest data/test_preprocessing_helpers.py::TestRowToList ---> Run the test class TestRowToList\n",
    "\n",
    "pytest data/test_preprocessing_helpers.py::TestRowToList::test_on_one_tab_with_missing_value ---> Run single unit test function (pytest data/test_preprocessing_helpers.py::TestRowToList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The -k option\n",
    "\n",
    "pytest -k \"pattern\" ---> Runs all tests whose node ID matches the pattern. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Supports Python logical operators\n",
    "\n",
    "pytest -k \"TestSplit and not test_on_one_row\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In real life, the !pytest or !pytest -x command is often used in CI servers. It can also be useful if there is a major update to the code base, which changes many application modules at the same time. Running all tests is the only way to check if anything was broken due to the update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running test Classes\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def split_into_training_and_testing_sets(data_array):\n",
    "    dim = data_array.ndim\n",
    "    if dim != 2:\n",
    "        raise ValueError(\"Argument data_array must be two dimensional. Got {0} dimensional array instead!\".format(dim))\n",
    "    num_rows = data_array.shape[0]\n",
    "    if num_rows < 2:\n",
    "        raise ValueError(\"Argument data_array must have at least 2 rows, it actually has just {0}\".format(num_rows))\n",
    "    # Fill in with the correct float\n",
    "    num_training = int(0.75 * data_array.shape[0])\n",
    "    permuted_indices = np.random.permutation(data_array.shape[0])\n",
    "    return data_array[permuted_indices[:num_training], :], data_array[permuted_indices[num_training:], :]\n",
    "\n",
    "!pytest models/test_train.py::TestSplitIntoTrainingAndTestingSets ---> run all the tests in this test class using node IDs\n",
    "        \n",
    "!pytest models/test_train.py::TestSplitIntoTrainingAndTestingSets::test_on_six_rows ---> run only the previously failing test test_on_six_rows() using node IDs.\n",
    "                \n",
    "!pytest -k \"SplitInto\" ---> run the tests in TestSplitIntoTrainingAndTestingSets using keyword expressions     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected Failures \n",
    "\n",
    "xfail: marking tests as \"expected to fail\" ---> @pytest.mark.xfail --> on top of function/class\n",
    "    \n",
    "# Expected failures, but conditionally \n",
    "Test that are expected to fail:\n",
    "    - on certain Python versions\n",
    "    - on certain platforms like windows\n",
    "    \n",
    "@pytest.mark.skipif(boolean_expression)\n",
    "    - if boolean_expression is True, then test is skipped.\n",
    "\n",
    "import sys\n",
    "@pytest.mark.skipif(sys.version_info > (2,7), reason='requires Python 2.7')\n",
    "\n",
    "# The -r option  ---> Showing reason in the test result report\n",
    "\n",
    "pytest -r[set_of_characters]\n",
    "\n",
    "pytest -rs ---> it will show us tests that were skipped in the short test summary section near the end. \n",
    "\n",
    "# Optional reason argument to xfail \n",
    "@pytest.mark.xfail(reason=\"\"Using TDD, train_model() is not implemented\")\n",
    "                   \n",
    "pytest -rx ---> It will only show the tests that are xfailed along with the reason in the test summary info.\n",
    "\n",
    "!pytest -rsx ---> It will show the reason for both skipped tests and tests that are expected to fail in the test result report. \n",
    "                   \n",
    "#Note: If we are skipping and xfailing multiple tests, note that these decorators can be applied to entire test classes as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous Integration and Code Coverage\n",
    "\n",
    "# Build Status Badge\n",
    "\n",
    "This badge uses a Continuous Integration server, which runs all tests automatically whenever we push a commit to Github. \n",
    "\n",
    "It shows whether tests are currently passing or failing. We will use Travis CI as our CI server. \n",
    "\n",
    "Step-1: Create a configuration file\n",
    "    \n",
    "repository root\n",
    "|-- src\n",
    "|-- tests\n",
    "|-- .travis.yml\n",
    "\n",
    "Contents of .travis.yml\n",
    "language: python\n",
    "python:\n",
    "    - \"\"3.6\"\n",
    "install:\n",
    "    - pip install -e .\n",
    "script:\n",
    "    - pytest tests\n",
    "    \n",
    "Step-2: Push the file to GitHub\n",
    "\n",
    "git add .travis.yml\n",
    "git push origin master\n",
    "\n",
    "Step-3: Install the Travis CI app\n",
    "    Go to Marketplace\n",
    "    Search for Travis CI and click on it. \n",
    "    Install the app\n",
    "    We will be redirected to Travis CI, where we should login using our GitHub account. \n",
    "    Every commit leads to a build. \n",
    "    From now on, whenever we push a commit to the GitHub repo, we should see a build appearing in the Travis CI dashboard. \n",
    "\n",
    "Step-4: Showing the build status badge\n",
    "    When the build finishes, the badge appears here. \n",
    "    Click on the badge.\n",
    "    Choose Markdown from the dropdown\n",
    "    Paste the markdown code in the README file on GitHub. \n",
    "    Tis adds the badge to the GitHub repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Coverage Badge\n",
    "\n",
    "The code coverage badge indicates the percentage of our application code that gets run when we run the test suite. \n",
    "\n",
    "This badge comes from a service called Codecov that integrates seamlessly with GitHub and Travis CI. \n",
    "\n",
    "Step-1: Modify the Travis CI configuration file\n",
    "\n",
    "language: python \n",
    "python:   - \"3.6\" \n",
    "install:   \n",
    "    - pip install -e .   \n",
    "    - pip install pytest-cov codecov    # Install packages for code coverage report \n",
    "script:   \n",
    "    - pytest --cov=src tests            # Point to the source directory \n",
    "after_success:\n",
    "    - codecov                           # uploads report to codecov.io\n",
    "    \n",
    "Step-2: Install Codecov\n",
    "\n",
    "Step-3: Showing the badge in Github "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing Models, Plots and Much More"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beyond assertion: setup and teardown\n",
    "\n",
    "Step-1: Setup\n",
    "    Setup brings the environment to a state where testing can begin.\n",
    "\n",
    "Step-2: Assert\n",
    "    We call the function\n",
    "\n",
    "Step-3: Teardown\n",
    "    Remove previous files so that the next run of the test gets a clean environment.\n",
    "    It cleans any modification to the environment and brings back to the intial state. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fixture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In pytest, the setup and teardown is placed outside the test, in a function called a fixture. \n",
    " pytest keeps the fixtures separate from the tests as this encourages reusing fixtures for tests that need the same/similar setup and teardown code.\n",
    "\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "def my_fixture():\n",
    "    # Do setup here\n",
    "    \n",
    "    yield data    # Use yield instead of return\n",
    "    \n",
    "    # Do teardown here  <---- This section runs only when the test has finished executing. \n",
    "    os.remove(....)\n",
    "    os.remove(....)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The test can access this data by calling the fixture passed as an argument.\n",
    "\n",
    "def test_something(my_fixture):\n",
    "    ...\n",
    "    data = my_fixture\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "There is a built-in pytest fixture called tmpdir, which is useful when dealing with files. \n",
    "\n",
    "The built-in tmpdir fixture\n",
    "- Setup: create a temporary directory\n",
    "- Teardown: Delete the temporary directory along with contents\n",
    "    \n",
    "We can pass this fixture as an argument to our fixture. \n",
    "This is called fixture chaining, which results in the setup of tmpdir to be called first, followed by the setup of our fixture. \n",
    "When the test finishes, the teardown of our fixture is called first, followed by the teardown of tmpdir. \n",
    "\n",
    "The teardown of tmpdir will delete all files in the temporary directory whene the test ends. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a fixture for a clean data file\n",
    "\n",
    "# Add a decorator to make this function a fixture\n",
    "@pytest.fixture\n",
    "def clean_data_file():\n",
    "    file_path = \"clean_data_file.txt\"\n",
    "    with open(file_path, \"w\") as f:\n",
    "        f.write(\"201\\t305671\\n7892\\t298140\\n501\\t738293\\n\")\n",
    "    yield file_path\n",
    "    os.remove(file_path)\n",
    "    \n",
    "# Pass the correct argument so that the test can use the fixture\n",
    "def test_on_clean_file(clean_data_file):\n",
    "    expected = np.array([[201.0, 305671.0], [7892.0, 298140.0], [501.0, 738293.0]])\n",
    "    # Pass the clean data file path yielded by the fixture as the first argument\n",
    "    actual = get_data_as_numpy_array(clean_data_file, 2)\n",
    "    assert actual == pytest.approx(expected), \"Expected: {0}, Actual: {1}\".format(expected, actual) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a fixture for an empty data file\n",
    "\n",
    "@pytest.fixture\n",
    "def empty_file():\n",
    "    # Assign the file path \"empty.txt\" to the variable\n",
    "    file_path = \"empty.txt\"\n",
    "    open(file_path, \"w\").close()\n",
    "    # Yield the variable file_path\n",
    "    yield file_path\n",
    "    # Remove the file in the teardown\n",
    "    os.remove(file_path)\n",
    "    \n",
    "def test_on_empty_file(self, empty_file):\n",
    "    expected = np.empty((0, 2))\n",
    "    actual = get_data_as_numpy_array(empty_file, 2)\n",
    "    assert actual == pytest.approx(expected), \"Expected: {0}, Actual: {1}\".format(expected, actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixture chaining using tmpdir\n",
    "The built-in tmpdir fixture is very useful when dealing with files in setup and teardown. tmpdir combines seamlessly with user defined fixture via fixture chaining.\n",
    "\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "# Add the correct argument so that this fixture can chain with the tmpdir fixture\n",
    "def empty_file(tmpdir):\n",
    "    # Use the appropriate method to create an empty file in the temporary directory\n",
    "    file_path = tmpdir.join(\"empty.txt\")\n",
    "    open(file_path, \"w\").close()\n",
    "    yield file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mocking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mocking: Testing functions independently of its dependencies\n",
    "    \n",
    "Packages:\n",
    "    pytest-mock: install using ---> pip install pytest-mock\n",
    "    unittest.mock: Python standard library package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea of mocking is to replace potentially buggy dependencies such as row_to_list() with the object unittest.mock.MagicMock(), but only during testing.\n",
    "\n",
    "This replacement is done using a fixture called mocker,and calling its patch method right at the beginning of the test ter_on_raw_data(), which we wrote in the last lesson. (mocker.patch())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the arguments\n",
    "\n",
    "call_args_list attribute returns a list of arguments that the mock was called with, wrapped in a convenience object called called(). This convenient object can be imported from unittest.mock as call. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Program a bug-free dependency\n",
    "\n",
    "# Define a function convert_to_int_bug_free\n",
    "def convert_to_int_bug_free(comma_separated_integer_string):\n",
    "    # Assign to the dict holding the correct return values\n",
    "    return_values = {\"1,801\": 1801,\n",
    "                     \"201,411\": 201411,\n",
    "                     \"2,002\": 2002,\n",
    "                     \"333,209\": 333209,\n",
    "                     \"1990\": None,\n",
    "                     \"782,911\": 782911,\n",
    "                     \"1,285\": 1285,\n",
    "                     \"389129\": None,\n",
    "                     }\n",
    "    # Return the correct result using the dict return_values\n",
    "    return return_values[comma_separated_integer_string]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock a dependency\n",
    "\n",
    "# Add the correct argument to use the mocking fixture in this test\n",
    "def test_on_raw_data(self, raw_and_clean_data_file, mocker):\n",
    "    raw_path, clean_path = raw_and_clean_data_file \n",
    "    # Replace the dependency with the bug-free mock\n",
    "    convert_to_int_mock = mocker.patch(\"data.preprocessing_helpers.convert_to_int\",\n",
    "                                       side_effect=convert_to_int_bug_free)    \n",
    "    preprocess(raw_path, clean_path)\n",
    "    # Check if preprocess() called the dependency correctly\n",
    "    assert convert_to_int_mock.call_args_list == [call(\"1,801\"), call(\"201,411\"), call(\"2,002\"), call(\"333,209\"),\n",
    "                                                  call(\"1990\"),  call(\"782,911\"), call(\"1,285\"), call(\"389129\")\n",
    "                                                  ]\n",
    "    with open(clean_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    first_line = lines[0]\n",
    "    assert first_line == \"1801\\\\t201411\\\\n\"\n",
    "    second_line = lines[1]\n",
    "    assert second_line == \"2002\\\\t333209\\\\n\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing on linear data\n",
    "\n",
    "The model_test() function, which measures how well the model fits unseen data, returns a quantity called r2 which is very difficult to compute in the general case. Therefore, you need to find special testing sets where computing r2 is easy.\n",
    "\n",
    "One important special case is when the model fits the testing set perfectly. This means that all the data points fall exactly on the best fit line. In other words, the testing set is perfectly linear. One such testing set is printed out in the IPython console for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pytest\n",
    "from models.train import model_test\n",
    "\n",
    "def test_on_perfect_fit():\n",
    "    # Assign to a NumPy array containing a linear testing set\n",
    "    test_argument = np.array([[1.0, 3.0], [2.0, 5.0], [3.0, 7.0]])\n",
    "    # Fill in with the expected value of r^2 in the case of perfect fit\n",
    "    expected = 1.0\n",
    "    # Fill in with the slope and intercept of the model\n",
    "    actual = model_test(test_argument, slope=2.0, intercept=1.0)\n",
    "    # Complete the assert statement\n",
    "    assert actual == pytest.approx(expected), \"Expected: {0}, Actual: {1}\".format(expected, actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing on circular data\n",
    "Another special case where it is easy to guess the value of r2 is when the model does not fit the testing dataset at all. In this case, r2 takes its lowest possible value 0.0.\n",
    "\n",
    "The plot shows such a testing dataset and model. The testing dataset consists of data arranged in a circle of radius 1.0. The x and y co-ordinates of the data is shown on the plot. The model corresponds to a straight line y=0.\n",
    "\n",
    "As one can easily see, the straight line does not fit the data at all. In this particular case, the value of r2 is known to be 0.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_on_circular_data(self):\n",
    "    theta = pi/4.0\n",
    "    # Assign to a NumPy array holding the circular testing data\n",
    "    test_argument = np.array([[1.0, 0.0], [cos(theta), sin(theta)],\n",
    "                              [0.0, 1.0],\n",
    "                              [cos(3 * theta), sin(3 * theta)],\n",
    "                              [-1.0, 0.0],\n",
    "                              [cos(5 * theta), sin(5 * theta)],\n",
    "                              [0.0, -1.0],\n",
    "                              [cos(7 * theta), sin(7 * theta)]]\n",
    "                             )\n",
    "    # Fill in with the slope and intercept of the straight line\n",
    "    actual = model_test(test_argument, slope=0.0, intercept=0.0)\n",
    "    # Complete the assert statement\n",
    "    assert actual == pytest.approx(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Plots\n",
    "\n",
    "Don't test properties individually\n",
    "matplotlib.figure.Figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Testing Strategies for Plots\n",
    "\n",
    "The idea involves two steps - a one-time baseline generation and testing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytest-mlp\n",
    "\n",
    "Since images generated on different operating systems look slightly different, we need to use a pytest plugin called pytest-mpl for image comparisons. \n",
    "\n",
    "pip install pytest-mpl\n",
    "\n",
    "pytest expects baseline images to be stored in a folder called baseline relative to the test module_test_plots.\n",
    "\n",
    "---> Generating the baseline image\n",
    "\n",
    "!pytest -k \"test_plot_for_linear_data\"\n",
    "        --mpl-generate-path\n",
    "        visualization/baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Run the test\n",
    "\n",
    "!pytest -k \"test_plot_for_linear_data\" --mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Reading failure reports\n",
    "\n",
    "!pytest -k \"test_plot_for_linear_data\" --mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

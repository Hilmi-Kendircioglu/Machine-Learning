{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(df, target_var, size=0.3, state=0):\n",
    "    \"\"\"\n",
    "    Given dataset is splitted into train and test set.\n",
    "        Parameters:\n",
    "            df (DataFrame): Any DataFrame\n",
    "            target_var (str): Labeled feature for predicting/classifying. \n",
    "            size (float): Test size. Default value is 0.3.\n",
    "            state (int): Random state for reproducibility purpose. \n",
    "        Returns:\n",
    "            X_train, X_test, y_train, y_test (Array): Splitted X and y DataFrames for training/set sets. \n",
    "    \"\"\"\n",
    "    # separate dataset into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df.drop(labels=[target_var], axis=1),\n",
    "                                                        df[target_var], test_size= size, random_state=0)\n",
    "    \n",
    "    # Data Shape\n",
    "    print(f'X_train : {X_train.shape}, X_test: {X_test.shape}')\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test set\n",
    "X_train, X_test, y_train, y_test = data_split(df, target_var, size=0.3, state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Filter Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.a. Basic Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.a.1 Remove Constant Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Constant features are those that show the same value, just one value, for all the observations of the dataset. This is the same value for all the rows of the dataset. These features provide no information that allows a machine learning model to discriminate or predict a target.\n",
    " \n",
    "To identify constant features, we can use the VarianceThreshold function from Sklearn or write a snippet code for finding them.  Variance threshold from sklearn is a simple baseline approach to feature selection. It removes all features which variance doesn’t meet some threshold. By default, it removes all zero-variance features, i.e., features that have the same value in all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_constants(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Removes constant features in the dataframe. \n",
    "        Parameters:\n",
    "            X_train (DataFrame): Original Train Set \n",
    "            X_test (DataFrame) : Original Test Set\n",
    "        Returns: \n",
    "            X_train, X_test (DataFrame): Train/Test Sets after removing constants. \n",
    "    \"\"\"\n",
    "    print(f'Before removing constant features: X_train : {X_train.shape}, X_test: {X_test.shape}')\n",
    "    \n",
    "    # fit finds the features with zero variance\n",
    "    sel = VarianceThreshold(threshold=0)\n",
    "    sel.fit(X_train) \n",
    "    \n",
    "    # Get the number of constant features \n",
    "    # get_support is a boolean vector that indicates which features are retained.\n",
    "    # if we sum over get_support, we get the number of features that are not constant\n",
    "    constant_features = [x for x in X_train.columns if x not in X_train.columns[sel.get_support()]]\n",
    "    constant_sum = len(constant_features)\n",
    "    \n",
    "    print(f'Number of constant features in the data is {constant_sum}')\n",
    "    print(f'Constant Features : {constant_features}')\n",
    "                         \n",
    "    # Transform function to reduce the training and test sets. \n",
    "    X_train = sel.transform(X_train)\n",
    "    X_test = sel.transform(X_test)\n",
    "        \n",
    "    print(f'After removing constants: X_train : {X_train.shape}, X_test: {X_test.shape}')\n",
    "        \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative way for removing constant features\n",
    "def remove_constants(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Removes constant features in the dataframe. \n",
    "        Parameters:\n",
    "            X_train (DataFrame): Original Train Set \n",
    "            X_test (DataFrame) : Original Test Set\n",
    "        Returns: \n",
    "            X_train, X_test (DataFrame): Train/Test Sets after removing constants. \n",
    "    \"\"\"\n",
    "    print(f'Before removing constant features: X_train : {X_train.shape}, X_test: {X_test.shape}')\n",
    "    \n",
    "    # Get the number of constant features \n",
    "    constant_features = [feat for feat in X_train.columns if X_train[feat].std() == 0]\n",
    "    constant_sum = len(constant_features)\n",
    "    \n",
    "    print(f'Number of constant features in the data is {constant_sum}')\n",
    "    print(f'Constant Features : {constant_features}')\n",
    "    \n",
    "    # Drop these columns from the train and test sets\n",
    "    X_train.drop(labels=constant_features, axis=1, inplace=True)\n",
    "    X_test.drop(labels=constant_features, axis=1, inplace=True)\n",
    "    \n",
    "    print(f'After removing constant features: X_train : {X_train.shape}, X_test: {X_test.shape}')\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both varianceThreshold and the snippet of code, which are provided above, for numerical variables. What can we do to find constant categorical variables?\n",
    "\n",
    "One alternatively is to encode the categories as numbers and then use the code above. But then you will put effort in pre-processing variables that are not informative.\n",
    "\n",
    "Alternatively, you can use the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing constant features for categorical variables\n",
    "def remove_constants(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Removes constant features in the dataframe. \n",
    "        Parameters:\n",
    "            X_train (DataFrame): Original Train Set \n",
    "            X_test (DataFrame) : Original Test Set\n",
    "        Returns: \n",
    "            X_train, X_test (DataFrame): Train/Test Sets after removing constants. \n",
    "    \"\"\"\n",
    "    print(f'Before removing constant features: X_train : {X_train.shape}, X_test: {X_test.shape}')\n",
    "\n",
    "    # Convert all columns to categorical type\n",
    "    X_train = X_train.astype('O')\n",
    "    \n",
    "    # Find those columns that contain only 1 label\n",
    "    constant_features = [feat for feat in X_train.columns if len(X_train[feat].unique()) == 1]\n",
    "    constant_sum = len(constant_features)\n",
    "    \n",
    "    print(f'Number of constant features in the data is {constant_sum}')\n",
    "    print(f'Constant Features : {constant_features}')\n",
    "    \n",
    "    # Drop these columns from the train and test sets\n",
    "    X_train.drop(labels=constant_features, axis=1, inplace=True)\n",
    "    X_test.drop(labels=constant_features, axis=1, inplace=True)\n",
    "    \n",
    "    print(f'After removing constant features: X_train : {X_train.shape}, X_test: {X_test.shape}')\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove constant features\n",
    "X_train, X_test = remove_constants(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.a.2 Remove quasi-constant features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quasi-constant features are those that show the same value for the great majority of the observations of the dataset. In general, these features provide little if any information that allows a machine learning model to discriminate or predict a target. But there can be exceptions. So you should be careful when removing these types of features. (where a single value occupies more than 99.98% population)\n",
    "\n",
    "First, remove constant features. This will allow a better visualization of the quasi-constant ones.  To identify constant features, we can use the VarianceThreshold function from sklearn, or we can code it ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_quasi_constants(X_train, X_test, level=0.01):\n",
    "    \"\"\"\n",
    "    Removes quasi-constant features (those that show the same value \n",
    "    for the great majority of the observations of the dataset) in the dataframe. \n",
    "        Parameters:\n",
    "            X_train (DataFrame): Original/Constants Removed Train Set \n",
    "            X_test (DataFrame) : Original/Constants Removed Test Set\n",
    "            level (float): variance threshold. Default value is 0.01, which indicates 99% of observations approximately\n",
    "        Returns:\n",
    "            X_train, X_test (DataFrame): Train/Test Sets after removing quasi-constant features \n",
    "    \"\"\" \n",
    "    print(f'Before removing quasi-constant features: X_train : {X_train.shape}, X_test: {X_test.shape}')\n",
    "    \n",
    "    # Fit finds the features with low variance\n",
    "    sel = VarianceThreshold(threshold=level)  \n",
    "    sel.fit(X_train) \n",
    "    \n",
    "    quasi_constant_features = [x for x in X_train.columns if x not in X_train.columns[sel.get_support()]]\n",
    "    quasi_constant_sum = len(quasi_constant_features)\n",
    "    \n",
    "    print(f'Number of quasi-constant features in the data is {quasi_constant_sum}')\n",
    "    print(f'Quasi-Constant Features : {quasi_constant_features}')\n",
    "    \n",
    "    # Remove quasi-constant features\n",
    "    X_train = sel.transform(X_train)\n",
    "    X_test = sel.transform(X_test)\n",
    "    \n",
    "    print(f'After removing quasi-constant features: X_train : {X_train.shape}, X_test: {X_test.shape}')\n",
    "        \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative way\n",
    "def remove_quasi_constants(X_train, X_test, threshold=0.998):\n",
    "    \"\"\"\n",
    "    Removes quasi-constant features (those that show the same value \n",
    "    for the great majority of the observations of the dataset) in the dataframe. \n",
    "        Parameters:\n",
    "            X_train (DataFrame): Original/Constants Removed Train Set \n",
    "            X_test (DataFrame) : Original/Constants Removed Test Set\n",
    "            level (float): variance threshold. Default value is 0.01, which indicates 99% of observations approximately\n",
    "        Returns:\n",
    "            X_train, X_test (DataFrame): Train/Test Sets after removing quasi-constant features                                                                                                                                              \n",
    "    \"\"\"\n",
    "    print(f'Before removing quasi-constant features: X_train : {X_train.shape}, X_test: {X_test.shape}')\n",
    "       \n",
    "    # Get the number of quasi-constant features\n",
    "    \n",
    "    quasi_constant_feat = []\n",
    "    for feature in X_train.columns:\n",
    "        # find the predominant value\n",
    "        predominant = (X_train[feature].value_counts() / np.float(len(X_train))).sort_values(ascending=False).values[0]\n",
    "\n",
    "        # evaluate predominant feature\n",
    "        if predominant > threshold:\n",
    "            quasi_constant_feat.append(feature)\n",
    "    quasiconstant_sum = len(quasi_constant_feat)\n",
    "    \n",
    "    print(f'Number of quasi-constant features in the data is {quasiconstant_sum}')\n",
    "    print(f'Quasi-Constant Features : {quasi_constant_feat}')\n",
    "    \n",
    "    # Drop these columns from the train and test sets\n",
    "    X_train.drop(labels=quasi_constant_feat, axis=1, inplace=True)\n",
    "    X_test.drop(labels=quasi_constant_feat, axis=1, inplace=True)\n",
    "    \n",
    "    print(f'After removing quasi-constants: X_train : {X_train.shape}, X_test: {X_test.shape}')\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Quasi-constants features\n",
    "X_train, X_test = remove_quasi_constants(X_train, X_test, level=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.a.3 Remove duplicated features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often datasets contain one or more features that show the same values across all the observations. This means that both features are in essence identical. In addition, it is not unusual to introduce duplicated features after performing one hot encoding of categorical variables, particularly when using several highly cardinal variables.\n",
    "\n",
    " *** Duplicated features may arise after one hot encoding of categorical variables.\n",
    " \n",
    "Note: Finding duplicated features is a computationally costly operation in Python, therefore depending on the size of your dataset, you might not always be able to perform it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this function only for small datasets since transposing data is computationally expensive.\n",
    "\n",
    "def remove_duplicated(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Removes duplicate features (those that show the same value \n",
    "    for the great majority of tduphe observations of the dataset) in the dataframe. \n",
    "        Parameters:\n",
    "            X_train (DataFrame): Original/Quasi-Constants Removed Train Set \n",
    "            X_test (DataFrame) : Original/Quasi-Constants Removed Test Set\n",
    "        Returns:\n",
    "            X_train, X_test (DataFrame): Train/Test Sets after removing duplicate features   \n",
    "    \"\"\"\n",
    "    print(f'Before removing duplicated features: X_train : {X_train.shape}, X_test: {X_test.shape}')\n",
    "    \n",
    "    # Transpose the dataframe, so that the columns are the rows of the new dataframe\n",
    "    data_t = X_train.T\n",
    "    \n",
    "    # Check if there are duplicated rows (the columns of the original dataframe)\n",
    "    # This is a computionally expensive operation, so it might take a while\n",
    "    duplicated_sum = data_t.duplicated().sum()\n",
    "    \n",
    "    print(f'Number of duplicated features in the data is {duplicated_sum}')\n",
    "    \n",
    "    duplicated_features = data_t[data_t.duplicated()].index.values\n",
    "    duplicated_features = list(duplicated_features)\n",
    "    \n",
    "    print(f'Duplicated Features are {duplicated_features}')\n",
    "    \n",
    "    # Capture the duplicated features and transpose back to original X_train\n",
    "    X_train = data_t.drop_duplicates(keep='first').T\n",
    "    new_columns = list(X_train.columns.values)\n",
    "    X_test = X_test[new_columns]\n",
    "    \n",
    "    print(f'After removing duplicated features: X_train : {X_train.shape}, X_test: {X_test.shape}')\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this function only for BIG datasets.\n",
    "\n",
    "def remove_duplicated(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Removes duplicate features (those that show the same value \n",
    "    for the great majority of tduphe observations of the dataset) in the dataframe. \n",
    "        Parameters:\n",
    "            X_train (DataFrame): Original/Quasi-Constants Removed Train Set \n",
    "            X_test (DataFrame) : Original/Quasi-Constants Removed Test Set\n",
    "        Returns:\n",
    "            X_train, X_test (DataFrame): Train/Test Sets after removing duplicate features   \n",
    "    \"\"\"\n",
    "    print(f'Before removing duplicated features: X_train : {X_train.shape}, X_test: {X_test.shape}')\n",
    "    \n",
    "    # Check for duplicated features in the training set\n",
    "    duplicated_feat = []\n",
    "    for i in range(0, len(X_train.columns)):\n",
    "        if i % 10 == 0:  # this helps how the loop is going\n",
    "            print(i)\n",
    "\n",
    "        col_1 = X_train.columns[i]\n",
    "\n",
    "        for col_2 in X_train.columns[i + 1:]:\n",
    "            if X_train[col_1].equals(X_train[col_2]):\n",
    "                duplicated_feat.append(col_2)\n",
    "    \n",
    "    # Check if there are duplicated rows (the columns of the original dataframe)\n",
    "    duplicated_sum = len(set(duplicated_feat))\n",
    "    \n",
    "    print(f'Number of duplicated features in the data is {duplicated_sum}')\n",
    "    \n",
    "    print(f'Duplicated Features are {duplicated_feat}')\n",
    "    \n",
    "    # Drop duplicates \n",
    "    X_train.drop(labels=duplicated_feat, axis=1, inplace=True)\n",
    "    X_test.drop(labels=duplicated_feat, axis=1, inplace=True)\n",
    "    \n",
    "    print(f'After removing duplicated features: X_train : {X_train.shape}, X_test: {X_test.shape}')\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should be run before removing duplicates.\n",
    "\n",
    "def identify_duplicatedpairs(X_train):\n",
    "    \"\"\"\n",
    "    Identify which set of features are identical in the dataset. \n",
    "        Parameters:\n",
    "            X_train (DataFrame): X Training Dataset\n",
    "        Returns:\n",
    "            duplicated_feat (list): Duplicated feature pair list in the dataset. \n",
    "    \"\"\"\n",
    "    duplicated_feat = []\n",
    "    \n",
    "    for i in range(0, len(X_train.columns)):\n",
    "\n",
    "        col_1 = X_train.columns[i]\n",
    "\n",
    "        for col_2 in X_train.columns[i + 1:]:\n",
    "\n",
    "            # If the features are duplicated\n",
    "            if X_train[col_1].equals(X_train[col_2]):\n",
    "\n",
    "                #Print them\n",
    "                print(col_1)\n",
    "                print(col_2)\n",
    "                print()\n",
    "\n",
    "                # And then append the duplicated one to a list\n",
    "                duplicated_feat.append(col_2)\n",
    "                \n",
    "    return duplicated_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Putting All Basic Filtering Together**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_filter(df, target_var, size=0.3, level=0.01):\n",
    "    \"\"\"\n",
    "    Removes constant, quasi-constant and duplicated features in the dataframe. \n",
    "        Parameters:\n",
    "            df (DataFrame): Any DataFrame\n",
    "            target_var (str): Target variable feature name\n",
    "            size (float): test size in train-test split. Default value is 0.3\n",
    "            level (float): variance threshold. Default value is 0.01, which indicates 99% of observations approximately\n",
    "        Returns:\n",
    "            X_train, X_test, y_train, y_test (DataFrame): Splitted X and y DataFrames \n",
    "                                                          after applying basic filtering.\n",
    "    \"\"\"\n",
    "    # separate dataset into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df.drop(labels=[target_var], axis=1),\n",
    "        df[target_var], test_size= size, random_state=0)\n",
    "    \n",
    "    print(f'Before applying basic filtering: X_train : {X_train.shape}, X_test: {X_test.shape}')\n",
    "    \n",
    "    # Remove constant features \n",
    "    constant_features = [\n",
    "        feat for feat in X_train.columns if X_train[feat].std() == 0]\n",
    "    X_train.drop(labels=constant_features, axis=1, inplace=True)\n",
    "    X_test.drop(labels=constant_features, axis=1, inplace=True)\n",
    "\n",
    "    print(f'After removing constant features : X_train : {X_train.shape}, X_test: {X_test.shape}')\n",
    "    \n",
    "    # remove quasi-constant features\n",
    "    sel = VarianceThreshold(threshold=level)  \n",
    "    sel.fit(X_train)  \n",
    "    \n",
    "    features_to_keep = X_train.columns[sel.get_support()]\n",
    "    \n",
    "    X_train = sel.transform(X_train)\n",
    "    X_test = sel.transform(X_test)\n",
    "    \n",
    "    print(f'After removing quasi-constant features : X_train : {X_train.shape}, X_test: {X_test.shape}')\n",
    "    \n",
    "    # Transform the arrays back to dataframes\n",
    "    X_train= pd.DataFrame(X_train)\n",
    "    X_train.columns = features_to_keep\n",
    "\n",
    "    X_test= pd.DataFrame(X_test)\n",
    "    X_test.columns = features_to_keep\n",
    "    \n",
    "    # check for duplicated features in the training set\n",
    "    duplicated_feat = []\n",
    "    for i in range(0, len(X_train.columns)):\n",
    "        if i % 10 == 0:  # this helps me understand how the loop is going\n",
    "            print(i)\n",
    "\n",
    "        col_1 = X_train.columns[i]\n",
    "\n",
    "        for col_2 in X_train.columns[i + 1:]:\n",
    "            if X_train[col_1].equals(X_train[col_2]):\n",
    "                duplicated_feat.append(col_2)\n",
    "                \n",
    "    X_train.drop(labels=duplicated_feat, axis=1, inplace=True)\n",
    "    X_test.drop(labels=duplicated_feat, axis=1, inplace=True)\n",
    "    \n",
    "    print(f'After removing duplicated features : X_train : {X_train.shape}, X_test: {X_test.shape}')\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.b. Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Correlation is a measure of the linear relationship of 2 or more variables.\n",
    "\n",
    "• Through correlation, we can predict one variable from the other.\n",
    "\n",
    "• Good variables are highly correlated with the target.\n",
    "\n",
    "• Correlated predictor variables provide redundant information.\n",
    "\n",
    "• Variables should be correlated with the target but uncorrelated among themselves.\n",
    "\n",
    "• The central hypothesis is that good feature sets contain features that are highly correlated with the class, yet uncorrelated with each other.\n",
    "\n",
    "• Correlated features do not necessarily affect model accuracy per se. High dimensionality does.\n",
    "\n",
    "• If 2 features are highly correlated, the second one will add little information over the previous one: removing it helps reduce dimension. \n",
    "\n",
    "• Correlation affects model interpretability: linear models.\n",
    "\n",
    "• Different classifiers show different sensitivity to correlation.\n",
    "\n",
    "• Pearson’s coefficient values vary between -1 and 1:\n",
    "\n",
    "       1 is highly correlated: the more of variable x1, the more of x2\n",
    "   \n",
    "      -1 is highly anti-correlated: the more of variable x1, the less of x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_correlation(X_train):\n",
    "    \"\"\"\n",
    "    Visualise correlated features. It builds the correlation matrix, which examines the \n",
    "    correlation of all features (for all possible feature combinations) and then \n",
    "    visualise the correlation matrix using seaborn. All the categorical variables are encoded into numbers\n",
    "        Parameters:\n",
    "            X_train (DataFrame): X train set\n",
    "        Returns:\n",
    "            plot (plot): heatmap correlation matrix. The red squares correspond to highly \n",
    "            correlated features (>0.8). The diagonal represents the correlation of a feature \n",
    "            with itself, therefore the value is 1.\n",
    "    \"\"\"\n",
    "    # Visualize heatmap\n",
    "    corrmat = X_train.corr()\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(11,11)\n",
    "    plot = sns.heatmap(corrmat)\n",
    "    \n",
    "    return plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation Feature Selection evaluates subsets of features on the basis of the following hypothesis: \"Good feature subsets contain features highly correlated with the target, yet uncorrelated to each other\".\n",
    "\n",
    "You may demonstrate how to select features based on correlation using 2 procedures. \n",
    "\n",
    "The first one is a brute force function that finds correlated features without any further insight. The second procedure finds groups of correlated features. Often, more than 2 features are correlated with each other. We can find groups of 3, 4 or more features that are correlated. By identifying these groups, we can then select from each group, which feature we want to keep, and which ones we want to remove.\n",
    "\n",
    "The second approach looks to identify groups of highly correlated features. And then, we can make further investigation within these groups to decide which feature we keep and which one we remove.  When investigating groups, we can check the missingness in the correlated group and select the correlated feature which has less missing value to keep in the data, and drop the rest of the correlated features. \n",
    "\n",
    "The other approach to build a machine learning algorithm using all the features from the correlated list, and select the more predictive one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.b.1 Brute Force Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlated_cols(X_train, threshold=0.8):\n",
    "    \"\"\"\n",
    "    Select highly correlated features and remove the first feature that is correlated \n",
    "    with anything else without any other insight.\n",
    "        Parameters:\n",
    "            X_train (DataFrame): X_train of Any DataFrame\n",
    "            threshold (int): Correlation coefficient. The default value is 0.8.\n",
    "        Returns:\n",
    "            col_corr (set): The names of correlated columns. \n",
    "    \"\"\"\n",
    "    col_corr = set()  # Set of all the names of correlated columns\n",
    "    corr_matrix = dataset.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n",
    "                colname = corr_matrix.columns[i]  # getting the name of column\n",
    "                col_corr.add(colname)\n",
    "    \n",
    "    corr_colnum = len(col_corr)\n",
    "    \n",
    "    print(f'There are {corr_colnum} features which are highly correlated with others in the dataset.')\n",
    "    \n",
    "    # Converting set to list \n",
    "    col_corr = list(col_corr)\n",
    "    \n",
    "    return col_corr\n",
    "\n",
    "\n",
    "def remove_correlatedcols(X_train, X_test, col_corr):\n",
    "    \"\"\"\n",
    "    Remove correlated features in the dataset. \n",
    "        Parameters:\n",
    "            X_train (DataFrame): X_train of Any DataFrame\n",
    "            X_test (DataFrame): X_test of Any DataFrame\n",
    "            col_corr (list): The list of correlated columns, originated from above correlated_cols functions. \n",
    "        Returns:\n",
    "            X_train, X_test (DataFrame): Splitted X and y DataFrames after removing correlated features.    \n",
    "    \"\"\"\n",
    "    # Before dropping correlated features\n",
    "    print(f'Before dropping correlated features: X_train : {X_train.shape} and X_test : {X_test.shape}.')\n",
    "    \n",
    "    # Drop correlated features\n",
    "    X_train.drop(labels=col_corr, axis=1, inplace=True)\n",
    "    X_test.drop(labels=col_corr, axis=1, inplace=True)\n",
    "\n",
    "    # Shape of data after dropping correlated features\n",
    "    print(f'After dropping correlated features: X_train : {X_train.shape} and X_test shape : {X_test.shape}.')\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.b.2 Second Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify groups of highly correlated features. And then, make further investigation within these groups to decide which feature to keep and which one to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_corrdf(X_train):\n",
    "    \"\"\"\n",
    "    Builds a dataframe with the correlation between features. Remember that the absolute value of the correlation\n",
    "    coefficient is important and not the sign. \n",
    "        Parameters:\n",
    "            X_train (DataFrame): X_train of Any DataFrame\n",
    "        Returns:\n",
    "            corrmat (DataFrame): Correlation Dataframe between features    \n",
    "    \"\"\" \n",
    "    # Build a dataframe with the correlation between features\n",
    "    corrmat = X_train.corr()\n",
    "    corrmat = corrmat.abs().unstack() # absolute value of corr coef\n",
    "    corrmat = corrmat.sort_values(ascending=False)\n",
    "    corrmat = corrmat[corrmat >= 0.8]\n",
    "    corrmat = corrmat[corrmat < 1]\n",
    "    corrmat = pd.DataFrame(corrmat).reset_index()\n",
    "    corrmat.columns = ['feature1', 'feature2', 'corr']\n",
    "    \n",
    "    return corrmat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_corrgroups(corrmat):\n",
    "    \"\"\"\n",
    "    Find groups of correlated features. \n",
    "        Parameters:\n",
    "            corrmat (DataFrame): Correlation Dataframe between features, which is output of the function above.  \n",
    "        Returns:\n",
    "            correlated_groups (list): list of correlated features    \n",
    "    \"\"\"\n",
    "    # Builds a dataframe with the correlation between features \n",
    "    corrmat = build_corrdf(df, numerical_vars, target_var, size=0.3)\n",
    "    \n",
    "    # find groups of correlated features\n",
    "    grouped_feature_ls = []\n",
    "    correlated_groups = []\n",
    "\n",
    "    for feature in corrmat.feature1.unique():\n",
    "        if feature not in grouped_feature_ls:\n",
    "\n",
    "            # find all features correlated to a single feature\n",
    "            correlated_block = corrmat[corrmat.feature1 == feature]\n",
    "            grouped_feature_ls = grouped_feature_ls + list(\n",
    "                correlated_block.feature2.unique()) + [feature]\n",
    "\n",
    "            # append the block of features to the list\n",
    "            correlated_groups.append(correlated_block)\n",
    "\n",
    "    print(f'There are {len(correlated_groups)} correlated groups out of {X_train.shape[1]} total features')\n",
    "    \n",
    "    return correlated_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_correlated_features(correlated_groups):\n",
    "    \"\"\"\n",
    "    Visualise each correlated group. Some groups contain only 2 correlated features, \n",
    "    some other groups present several features that are correlated among themselves.\n",
    "        Parameters:\n",
    "            correlated_groups (List): List of correlated features which is output of the function above. \n",
    "        Returns:\n",
    "            None --- > print correlated feature groups\n",
    "    \"\"\"\n",
    "    # Visualize each group\n",
    "    for group in correlated_groups:\n",
    "        print(group)\n",
    "        print()\n",
    "        \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_correlatedfeat(correlated_groups, ind_num, X_train, correlated_feature1):\n",
    "    \"\"\"\n",
    "    Explore correlated features within one specific group and print missing data.\n",
    "        Parameters:\n",
    "            correlated_groups (List): List of correlated features which is output of the function above. \n",
    "            ind_num (int): index number of correlated groups list\n",
    "            X_train (DataFrame): X_train of Any DataFrame\n",
    "            correlated_feature1 (str): Correlated feature 1 among correlated features\n",
    "        Returns:\n",
    "            None ---> prints missing data for correlated features within one specific group.\n",
    "    \"\"\"\n",
    "    # Investigate further features within one group.\n",
    "    group = correlated_groups[ind_num]\n",
    "    \n",
    "    # Among correlated features, we may select the features with less missing data as shown below.\n",
    "    for feature in list(group.feature2.unique())+[correlated_feature1]: \n",
    "        print(X_train[feature].isnull().sum())\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative method\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def select_predictivecorr(correlated_groups, ind_num, X_train, correlated_feature1):\n",
    "    \"\"\"\n",
    "    Build a machine learning algorithm using all the features from list above, and select the more predictive one, and \n",
    "    remove all the remaining features from this gorup from the dataset. \n",
    "        Parameters:\n",
    "            correlated_groups (List): List of correlated features which is output of the function above. \n",
    "            ind_num (int): index number of correlated groups list\n",
    "            X_train (DataFrame): X_train of Any DataFrame\n",
    "            correlated_feature1 (str): Correlated feature 1 among correlated features\n",
    "        Returns:\n",
    "            importance (DataFrame): Feature importance dataframe. \n",
    "    \"\"\"\n",
    "    # Investigate further features within one group.\n",
    "    group = correlated_groups[ind_num]\n",
    "    \n",
    "    features = list(group.feature2.unique())+[correlated_feature1] \n",
    "    rf = RandomForestClassifier(n_estimators=200, random_state=39, max_depth=4)\n",
    "    rf.fit(X_train[features].fillna(0), y_train)\n",
    "\n",
    "    # Get the feature importance attributed by the Random Forest model \n",
    "    # Select the highest feature important one, and remove all the remaining features from this group from the dataset.\n",
    "    importance = pd.concat([pd.Series(features), \n",
    "                            pd.Series(rf.feature_importances_)], axis=1)\n",
    "\n",
    "    importance.columns = ['feature', 'importance']\n",
    "    importance.sort_values(by='importance', ascending=False)\n",
    "    \n",
    "    return importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "\n",
    "None of the 2 procedures for removing correlated features are perfect, and some correlated features may escape the loops of code. So it might be worthwhile to check that after removing the correlated features, there are no correlated features left in the dataset. If there are, repeat the procedure to remove the remaining ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.c Statistical - Ranking Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These methods still evaluate each feature individually, in the light of the target, we intend them to predict.  Statistical – ranking methods:\n",
    "\n",
    "    -\tInformation Gain\n",
    "    -\tFisher Score\n",
    "    -\tUnivariate Tests\n",
    "    -\tUnivariate ROC-AUC / RMSE\n",
    "    \n",
    "**Two Steps:**\n",
    "\n",
    "    1-Rank features based on certain criteria / metric\n",
    "    2-Select features with highest rankings\n",
    "\n",
    "**Pros and Cons:**\n",
    "\n",
    "    -\tFast and not computationally expensive\n",
    "    -\tDoes not contemplate feature redundancy.  You would have to screen for duplicated and correlated features in previous or posterior steps. Also, these selection procedures do not contemplate feature interaction. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.c.1 Information Gain (Mutual Information)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mutual information measures how much information the presence/absence of a feature contributes to making the correct prediction on Y.\n",
    "\n",
    "Mutual information measures the information that X and Y share: It measures how much knowing one of these variables reduces uncertainty about the other. For example, if X and Y are independent, then knowing X does not give any information about Y and vice versa, so their mutual information is zero. At the other extreme, if X is a deterministic function of Y and Y is a deterministic function of X then all information conveyed by X is shared with Y: knowing X determines the value of Y and vice versa. As a result, in this case the mutual information is the same as the uncertainty contained in Y (or X) alone, namely the entropy of Y (or X). Moreover, this mutual information is the same as the entropy of X and as the entropy of Y. (A very special case of this is when X and Y are the same random variable.)\n",
    "\n",
    "Measures the mutual dependence of 2 variables\n",
    "\n",
    "Determines how similar the joint distribution p(X,Y) is to the products of individual distributions p(X)p(Y)\n",
    "    - If X and Y are independent, their MI is zero\n",
    "    - If X is deterministic of Y, the MI is the uncertainty in X.\n",
    "\n",
    "Using sklearn on a regression and classification problem, mutual information can be used to select features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_mi(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Visualize mutual information bar plot for both classification and regression. \n",
    "    This function is for classification example. You may shift it to regression by changing mi value as given below\n",
    "    in parameters section. \n",
    "    mi (series): Mutual information values. Use the function below according to classification/regression.\n",
    "                                            mi default for classification: mi = mutual_info_regression(X_train.fillna(0), y_train)\n",
    "                                            mi for for regression: mi = mutual_info_regression(X_train.fillna(0), y_train)\n",
    "        Parameters:\n",
    "            X_train (DataFrame): X_train of Any DataFrame\n",
    "            y_train (DataFrame): y_train of Any DataFrame\n",
    "        Returns:\n",
    "            None ---> Plort MI in ordered manner.\n",
    "    \"\"\"\n",
    "    # Add the variable names and order the features\n",
    "    mi = mutual_info_classif(X_train.fillna(0), y_train)\n",
    "    \n",
    "    mi = pd.Series(mi)\n",
    "    mi.index = X_train.columns\n",
    "    mi.sort_values(ascending=False)\n",
    "    \n",
    "    # Plot the ordered MI values per feature\n",
    "    mi.sort_values(ascending=False).plot.bar(figsize=(20, 8))\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cut-off threshold to select features is arbitrary. One could choose a certain value of MI after studying the plot above. An alternative and most frequent way of selecting features is to select the top 10, or top 20 features, or the features in the the top 10th percentile of the MI value distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutualinfo_classification(X_train, y_train, n):\n",
    "    \"\"\"\n",
    "    This function applies only on classification models.\n",
    "    Calculate the mutual information between the variables and the target.\n",
    "    This returns the mutual information value of each feature.\n",
    "    The smaller the value the less information the feature has about the target.\n",
    "        Parameters:\n",
    "            X_train (DataFrame): X_train of Any DataFrame\n",
    "            y_train (DataFrame): y_train of Any DataFrame\n",
    "            n (int): Indicating highest top n features in terms of feature selection. \n",
    "                     Generally top 10 or 20 features are selected. \n",
    "        Returns:\n",
    "            selected_features (list): list of selected features according to mutual information. \n",
    "    \"\"\"\n",
    "    # Select the top n features\n",
    "    sel_ = SelectKBest(mutual_info_classif, k=n).fit(X_train.fillna(0), y_train)\n",
    "    X_train.columns[sel_.get_support()]\n",
    "    X_train = sel_.transform(X_train.fillna(0))\n",
    "    \n",
    "    # Define selected features\n",
    "    selected_features = list(X_train.columns.values)\n",
    "    \n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutualinfo_regression(X_train, y_train, percent):\n",
    "    \"\"\"\n",
    "    This function applies only on regression models.\n",
    "    Calculate the mutual information between the variables and the target.\n",
    "    This returns the mutual information value of each feature.\n",
    "    The smaller the value the less information the feature has about the target.\n",
    "        Parameters:\n",
    "            X_train (DataFrame): X_train of Any DataFrame\n",
    "            y_train (DataFrame): y_train of Any DataFrame\n",
    "            percent (int): Indicating highest top percentile of features in terms of feature selection. \n",
    "        Returns:\n",
    "            selected_features (list): list of selected features according to mutual information.    \n",
    "    \"\"\"    \n",
    "    # Select the top n percentile\n",
    "    sel_ = SelectKBest(mutual_info_regression, percentile=percent).fit(X_train.fillna(0), y_train)\n",
    "    X_train.columns[sel_.get_support()]\n",
    "    X_train = sel_.transform(X_train.fillna(0))\n",
    "    \n",
    "    # Define selected features\n",
    "    selected_features = list(X_train.columns.values)\n",
    "    \n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.c.2 Fisher Score - Chi-square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute chi-squared stats between each non-negative feature and class. \n",
    "\n",
    "- This score should be used to evaluate categorical variables in a classification task.\n",
    "\n",
    "It compares the observed distribution of the different classes of target Y among the different categories of the feature, against the expected distribution of the target classes, regardless of the feature categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fisher_score(X_train, y_train):\n",
    "    \"\"\"\n",
    "    This function is suited for categorical variables. \n",
    "    Calculate the chi2 p_value between each of the variables and the target. \n",
    "    It returns 2 arrays, one contains the F-Scores which are then evaluated against\n",
    "    the chi2 distribution to obtain the pvalue, and the pvalues are in the second array.\n",
    "        Parameters:\n",
    "            X_train (DataFrame): X_train of Any DataFrame\n",
    "            y_train (DataFrame): y_train of Any DataFrame\n",
    "        Returns:\n",
    "            pvalues (Series): P values for each variable. The smaller p-value,\n",
    "                              the more significant the feature is to predict \n",
    "                              the target variable. \n",
    "    \"\"\"\n",
    "    # Calculate the chi2 and p_value between each of the variables and target. \n",
    "    f_score = chi2(X_train.fillna(0), y_train)\n",
    "    \n",
    "    # let's add the variable names and order it for clearer visualisation\n",
    "    pvalues = pd.Series(f_score[1])\n",
    "    pvalues.index = X_train.columns\n",
    "    pvalues = pvalues.sort_values(ascending=False)\n",
    "    \n",
    "    return pvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind, that contrarily to MI, where we were interested in the higher MI values, for Fisher score, the smaller the p_value, the more significant the feature is to predict the target in the dataset. \n",
    "\n",
    "**Note**\n",
    "\n",
    "One thing to keep in mind when using Fisher score or univariate selection methods, is that in very big datasets, most of the features will show a small p_value, and therefore look like they are highly predictive. This is in fact an effect of the sample size. So care should be taken when selecting features using these procedures. An ultra tiny p_value does not highlight an ultra-important feature, it rather indicates that the dataset contains too many samples. \n",
    "\n",
    "If the dataset contained several categorical variables, we could then combine this procedure with SelectKBest or SelectPercentile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.c.3 Univariate Tests - ANOVA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Univariate feature selection works by selecting the best features based on univariate statistical tests (ANOVA). The methods based on F-test estimate the degree of linear dependency between two random variables. They assume a linear relationship between the feature and the target. These methods also assume that the variables follow a Gaussian distribution.\n",
    "\n",
    "These may not always be the case for the variables in your dataset, so if looking to implement these procedure, you will need to corroborate these assumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_selection import f_classif, f_regression\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For classification models\n",
    "def univariate_anova(X_train, y_train, n):\n",
    "    \"\"\"\n",
    "    Calculate the univariate statistical measure between each of the variables and the target\n",
    "    similarly to chi2, the output is the array of f-scores and an array of pvalues, \n",
    "    which are the ones we will compare.\n",
    "        Parameters:\n",
    "            X_train (DataFrame): X_train of Any DataFrame\n",
    "            y_train (DataFrame): y_train of Any DataFrame\n",
    "            n (int): Indicating highest top n features in terms of feature selection. \n",
    "                     Generally top 10 or 20 features are selected. \n",
    "        Returns:\n",
    "            univariate_plot (plot): Plot for p value for each feature. \n",
    "            selected_features (list): list of selected features according to univariate tests-ANOVA\n",
    "    \"\"\"    \n",
    "    # Calculate the univariate statistical measure between each of the variables and the target\n",
    "    univariate = f_classif(X_train.fillna(0), y_train)\n",
    "    \n",
    "    # Add the variable names and order it for clearer visualisation\n",
    "    univariate = pd.Series(univariate[1])\n",
    "    univariate.index = X_train.columns\n",
    "    univariate.sort_values(ascending=False, inplace=True)\n",
    "    \n",
    "    # Plot the p values\n",
    "    univariate_plot = univariate.sort_values(ascending=False).plot.bar(figsize=(20, 8))\n",
    "    \n",
    "    # Select the top n features\n",
    "    sel_ = SelectKBest(f_classif, k=n).fit(X_train.fillna(0), y_train)\n",
    "    X_train.columns[sel_.get_support()]\n",
    "    X_train = sel_.transform(X_train.fillna(0))\n",
    "    \n",
    "    # Define selected features\n",
    "    selected_features = list(X_train.columns.values)\n",
    "    \n",
    "    return univariate_plot, selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the lower the p_value, the most predictive the feature is in principle. There are a few features that do not seem to have predictive power according to this tests, which are those on the left with pvalues above 0.05. Given that typically in statistics one uses a pvalue of 0.05 as a cut-off, generally believed taht those features with pvalue > 0.05 are indeed not important. However, keep in mind that this test assumes a linear relationship, so it might also be the case that the feature is related to the target but not in a linear manner.\n",
    "\n",
    "Further investigation is needed if we want to know the true nature of the relationship between feature and target.\n",
    "\n",
    "In big datasets, it is not unusual that the p-values of the different features are really small. This does not say as much about the relevance of the feature. Mostly it indicates that it is a big the dataset.\n",
    "\n",
    "Once again, where we put the cut-off to select features is a bit arbitrary. One way is to select the top 10, 20 features. Alternatively, the top 10th percentile. For this, you can use anova in combination with SelectKBest or SelectPercentile from sklearn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For regression models\n",
    "def univariate_anova(X_train, y_train, n):\n",
    "    \"\"\"\n",
    "    Calculate the univariate statistical measure between each of the variables and the target\n",
    "    similarly to chi2, the output is the array of f-scores and an array of pvalues, \n",
    "    which are the ones we will compare.\n",
    "        Parameters:\n",
    "            X_train (DataFrame): X_train of Any DataFrame\n",
    "            y_train (DataFrame): y_train of Any DataFrame\n",
    "            percent (int): Indicating highest top percentile of features in terms of feature selection. \n",
    "        Returns:\n",
    "            univariate_plot (plot): Plot for p value for each feature. \n",
    "            selected_features (list): list of selected features according to univariate tests-ANOVA\n",
    "    \"\"\"    \n",
    "    # Calculate the univariate statistical measure between each of the variables and the target\n",
    "    univariate = f_regression(X_train.fillna(0), y_train)\n",
    "    \n",
    "    # Add the variable names and order it for clearer visualisation\n",
    "    univariate = pd.Series(univariate[1])\n",
    "    univariate.index = X_train.columns\n",
    "    univariate.sort_values(ascending=False, inplace=True)\n",
    "    \n",
    "    # Plot the p values\n",
    "    univariate_plot = univariate.sort_values(ascending=False).plot.bar(figsize=(20, 8))\n",
    "    \n",
    "    # Select the top n percentile\n",
    "    sel_ = SelectPercentile(f_regression, percentile=percent).fit(X_train.fillna(0), y_train)\n",
    "    X_train.columns[sel_.get_support()]\n",
    "    X_train = sel_.transform(X_train.fillna(0))\n",
    "    \n",
    "    # Define selected features\n",
    "    selected_features = list(X_train.columns.values)\n",
    "    \n",
    "    return univariate_plot, selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.c.4 Univariate - ROC AUC / RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This procedure works as follows:\n",
    "\n",
    "- First, it builds one decision tree per feature, to predict the target\n",
    "- Second, it makes predictions using the decision tree and the mentioned feature\n",
    "- Third, it ranks the features according to the machine learning metric (roc-auc or mse)\n",
    "- It selects the highest ranked features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def univariate_rocauc(X_train, X_test, y_train, y_test, cut_off=0.5):\n",
    "    \"\"\"\n",
    "    Measures the dependence of 2 variables via using machine learning. \n",
    "    Suited for all types of variables. \n",
    "    Makes no assumption on the distribution of the variables.\n",
    "    The higher ROC AUC score, variable is better to predict the target variable.\n",
    "        Parameters:\n",
    "            X_train, X_test, y_train, y_test (DataFrame): Splitted X and y DataFrames for training/set sets. \n",
    "            cut_off (float): threshold for ROC-AUC score. The default value is 0.5 which indicates\n",
    "                             below that thresold is worse than random. \n",
    "        Returns:\n",
    "            roc_plot (plot): roc values plot in descending order. \n",
    "            selected_featureslist (list): list of selected features according to univariate tests-ROC AUC\n",
    "    \"\"\"\n",
    "    # loop to build a tree, make predictions and get the roc-auc for each feature of the train set\n",
    "    roc_values = []\n",
    "    for feature in X_train.columns:\n",
    "        clf = DecisionTreeClassifier()\n",
    "        clf.fit(X_train[feature].fillna(0).to_frame(), y_train)\n",
    "        y_scored = clf.predict_proba(X_test[feature].fillna(0).to_frame())\n",
    "        roc_values.append(roc_auc_score(y_test, y_scored[:, 1]))\n",
    "    \n",
    "    # Add the variable names and order it for clearer visualisation\n",
    "    roc_values = pd.Series(roc_values)\n",
    "    roc_values.index = X_train.columns\n",
    "    roc_values = roc_values.sort_values(ascending=False)\n",
    "    \n",
    "    # Plot\n",
    "    roc_plot = roc_values.sort_values(ascending=False).plot.bar(figsize=(20, 8))\n",
    "    \n",
    "    # Check how many features show a roc-auc value higher than random\n",
    "    # a roc auc value of 0.5 indicates random decision\n",
    "    selected_features = roc_values[roc_values > cut_off]\n",
    "    print(f'{len(selected_features)} features show a roc-auc value higher than {cut_off}.')\n",
    "    \n",
    "    selected_featureslist = selected_features.tolist()\n",
    "    \n",
    "    return roc_plot, selected_featureslist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may tune the parameters of the Decision Tree and get better predictions. It is up to you. But remember that the key here is not to make ultra predictive Decision Trees, rather to use them to screen quickly for important features. So I would recommend you don't spend too much time tuning. Doing cross validation with sklearn should be very straight forward to get a more accurate measure of the roc-auc per feature.\n",
    "\n",
    "Once again, where we put the cut-off to select features is a bit arbitrary, other than > 0.5. It will be up to you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def univariate_rmse(X_train, X_test, y_train, y_test, cut_off=0.5):\n",
    "    \"\"\"\n",
    "    Measures the dependence of 2 variables via using machine learning. \n",
    "    Suited for all types of variables. \n",
    "    Makes no assumption on the distribution of the variables.\n",
    "    The lower RMSE score, variable is better  to predict the target variable.\n",
    "        Parameters:\n",
    "            X_train, X_test, y_train, y_test (DataFrame): Splitted X and y DataFrames for training/set sets. \n",
    "            cut_off (float): threshold for RMSE score. For the mse, where to put the \n",
    "                             cut-off is arbitrary. It depends on how many features \n",
    "                             you would like to end up with.\n",
    "        Returns:\n",
    "            mse_plot (plot): mse values plot in descending order. \n",
    "            selected_featureslist (list): list of selected features according to univariate tests-RMSE\n",
    "    \"\"\" \n",
    "     # loop to build a tree, make predictions and get the mse for each feature of the train set\n",
    "    mse_values = []\n",
    "    for feature in X_train.columns:\n",
    "        clf = DecisionTreeRegressor()\n",
    "        clf.fit(X_train[feature].fillna(0).to_frame(), y_train)\n",
    "        y_scored = clf.predict(X_test[feature].fillna(0).to_frame())\n",
    "        mse_values.append(mean_squared_error(y_test, y_scored))\n",
    "    \n",
    "    # Add the variable names and order it for clearer visualisation\n",
    "    mse_values = pd.Series(mse_values)\n",
    "    mse_values.index = X_train.columns\n",
    "    mse_values.sort_values(ascending=False)\n",
    "    \n",
    "    # Plot\n",
    "    mse_plot = mse_values.sort_values(ascending=False).plot.bar(figsize=(20, 8))\n",
    "    \n",
    "    # Check how many features show a rmse value lower than cut-off value\n",
    "    selected_features = mse_values[mse_values < cut_off]\n",
    "    print(f'{len(selected_features)} features show a rmse value lower than {cut_off}.')\n",
    "    \n",
    "    selected_featureslist = selected_features.tolist()\n",
    "    \n",
    "    return mse_plot, selected_featureslist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that for regression, the smaller the mse, the better the model performance is. So in this case, we need to select from the right to the left.\n",
    "\n",
    "For the mse, where to put the cut-off is arbitrary as well. It depends on how many features you would like to end up with.\n",
    "\n",
    "It is good practice to use this method when you have an enormous amount of features and need to start reducing the feature space quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.c.5 Select Features by mean encoding (Alternative Filter Methods: Alternative non-mainstream method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will describe the feature selection approach undertaken by data scientists at the University of Melbourne for the [KDD 2009](http://www.kdd.org/kdd-cup/view/kdd-cup-2009) data science competition. The task consisted in predicting churn based on a dataset with a huge number of features.\n",
    "\n",
    "The authors describe this procedure as an aggressive non-parametric feature selection procedure, that is based in contemplating the relationship between the feature and the target. Therefore, this method should be classified as a filter method.\n",
    "\n",
    "**The procedure consists in the following steps**:\n",
    "\n",
    "For each categorical variable:\n",
    "\n",
    "    1) Separate into train and test\n",
    "\n",
    "    2) Determine the mean value of the target within each label of the categorical variable using the train set\n",
    "\n",
    "    3) Use that mean target value per label as the prediction in the test set and calculate the roc-auc.\n",
    "\n",
    "For each numerical variable:\n",
    "\n",
    "    1) Separate into train and test\n",
    "    \n",
    "    2) Divide the variable into 100 quantiles\n",
    "\n",
    "    3) Calculate the mean target within each quantile using the training set \n",
    "\n",
    "    4) Use that mean target value / bin as the prediction on the test set and calculate the roc-auc\n",
    "\n",
    "\n",
    "The authors quote the following advantages of the method:\n",
    "\n",
    "- Speed: computing mean and quantiles is direct and efficient\n",
    "- Stability respect to scale: extreme values for continuous variables do not skew the predictions\n",
    "- Comparable between categorical and numerical variables\n",
    "- Accommodation of non-linearities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For categorical features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples mentioned below is given based on the Titanic Dataset for understanding better. \n",
    "\n",
    "First, feature selection procedure over categorical variables will be demonstrated. The Titanic dataset contains 4 categorical variables, which are Sex, Pclass, Cabin and Embarked, which is defined as `categorical_list` in the function.\n",
    "\n",
    "In the next cell I create a function that calculates the mean of Survival (and this is equivalent to the probability of survival) of the passenger, within each label of a categorical variable. It creates a dictionary, using the training set only, that maps each label of the training set variable, to a probability of survival.\n",
    "\n",
    "Then, the function replaces the label both in train and test set, by the probability of survival. It is like making a prediction on the outcome, by using only the label of the variable.\n",
    "\n",
    "In this way, the function replaces the original strings, by probabilities.\n",
    "\n",
    "The bottom line of this method is that we use just the label of the variable to estimate the probability of survival of the passenger. A bit like \"Tell me which one was your Cabin, and I will tell you your probability of Survival\".\n",
    "\n",
    "If the labels of a categorical variable and therefore the categorical variable are good predictors, then, we should obtain a roc-auc above 0.5 for that variable, when we evaluate those probabilities with the real outcome, which is whether the passenger survived or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_encoding(df_train, df_test, categorical_list, target_var):\n",
    "    \"\"\"\n",
    "    Calculates the mean of target variable (and this is equivalent to the probability of target variable) of the each record, \n",
    "    within each label of a categorical variable. It creates a dictionary, using the training set only, \n",
    "    that maps each label of the training set variable, to a probability of target variable.\n",
    "        Parameters:\n",
    "            df_train (DataFrame): Train Dataset. \n",
    "            df_test (DataFrame): Test Dataset\n",
    "            categorical_list (List): List of categorical features in the dataset. \n",
    "            target_var (str): Name of the target variable \n",
    "        Returns:\n",
    "            df_train_temp (DataFrame): X_train after mean encoding function\n",
    "            df_test_temp (DataFrame): X_test after mean encoding function\n",
    "    \"\"\"\n",
    "    # temporary copy of the original dataframes\n",
    "    df_train_temp = df_train.copy()\n",
    "    df_test_temp = df_test.copy()\n",
    "    \n",
    "    for col in categorical_list:\n",
    "        # make a dictionary mapping labels / categories to the mean target for that label\n",
    "        risk_dict = df_train.groupby([col])[target_var].mean().to_dict()\n",
    "        \n",
    "        # re-map the labels\n",
    "        df_train_temp[col] = df_train[col].map(risk_dict)\n",
    "        df_test_temp[col] = df_test[col].map(risk_dict)\n",
    "    \n",
    "    # drop the target\n",
    "    df_train_temp.drop(['Survived'], axis=1, inplace=True)\n",
    "    df_test_temp.drop(['Survived'], axis=1, inplace=True)     \n",
    "    \n",
    "    return df_train_temp, df_test_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rocauc(categorical_list, y_test, df_test_temp):\n",
    "    \"\"\"\n",
    "    Use that mean target value per label as the prediction in the test set \n",
    "    and calculate the roc-auc, using the probabilities that we used to\n",
    "    replace the labels, and comparing it with the true target.\n",
    "        Parameters:\n",
    "            df_test_temp(DataFrame): X_test data after categorical variables are mapped\n",
    "                                     with labels.  \n",
    "            y_test (array): y_test after train and test split.         \n",
    "            categorical_list (list): list of categorical features in the DataFrame.\n",
    "        Returns:\n",
    "            m1 (Series): ROC-AUC score series for each categorical feature.  \n",
    "    \"\"\"\n",
    "    roc_values = []\n",
    "    for feature in categorical_list:\n",
    "        roc_values.append(roc_auc_score(y_test, df_test_temp[feature]))\n",
    "    \n",
    "    # Make series for easy visualization\n",
    "    m1 = pd.Series(roc_values)\n",
    "    m1.index = categorical_list\n",
    "    m1.sort_values(ascending=False)\n",
    "    \n",
    "    return m1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For numerical features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The procedure is exactly the same, but it requires one additional first step which is to divide the continuous variable into bins. The authors of the method divide the variable in 100 quantiles, that is 100 bins. In principle, you could divide the variable in less bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_encoding(n, feature, X_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Calculates the mean of target variable (and this is equivalent to the probability of target variable) of the each record, \n",
    "    within each label of a categorical variable. It creates a dictionary, using the training set only, \n",
    "    that maps each label of the training set variable, to a probability of target variable.\n",
    "        Parameters:\n",
    "            n (int): number of bin labels\n",
    "            feature (str): Specific numerical feature\n",
    "            X_train (DataFrame): X_train set\n",
    "            X_test (DataFrame): X_test set\n",
    "            y_test (DataFrame): y_test set            \n",
    "        Returns:\n",
    "            score (float): ROC-AUC value\n",
    "    \"\"\"\n",
    "    # Divide specific numerical feature in n bins. Use the qcut (quantile cut)\n",
    "    # function from pandas, which indicates that you want n-1 cutting points, thus 10 bins.\n",
    "    # retbins= True indicates that you want to capture the limits of\n",
    "    # each interval (so I can then use them to cut the test set)\n",
    "\n",
    "    # create 10 labels, one for each quantile\n",
    "    # instead of having the quantile limits, the new variable\n",
    "    # will have labels in its bins\n",
    "    labels = ['Q' + str(i + 1) for i in range(0, n)]\n",
    "\n",
    "    X_train['feature_binned'], intervals = pd.qcut(\n",
    "        X_train[feature],\n",
    "        n,\n",
    "        labels=labels,\n",
    "        retbins=True,\n",
    "        precision=3,\n",
    "        duplicates='drop')\n",
    "\n",
    "    # Use the boundaries calculated above to bin the testing set\n",
    "    X_test['feature_binned'] = pd.cut(x = X_test[feature], bins=intervals, labels=labels)\n",
    "    \n",
    "    # in order to replace the NaN values by a new category\n",
    "    # called \"Missing\", first need to recast the variables as objects\n",
    "    X_train['feature_binned'] = X_train['feature_binned'].astype('O')\n",
    "    X_test['feature_binned'] = X_test['feature_binned'].astype('O')\n",
    "    \n",
    "    # Replace the missing values with a new category\n",
    "    X_train['feature_binned'].fillna('Missing', inplace=True)\n",
    "    X_test['feature_binned'].fillna('Missing', inplace=True)\n",
    "    \n",
    "    # Create a dictionary that maps the bins to the mean of survival\n",
    "    risk_dict = X_train.groupby(['feature_binned'])['Survived'].mean().to_dict()\n",
    "\n",
    "    # Re-map the labels, and replace the bins by the probability of survival\n",
    "    X_train['feature_binned'] = X_train['feature_binned'].map(risk_dict)\n",
    "    X_test['feature_binned'] = X_test['feature_binned'].map(risk_dict)    \n",
    "    \n",
    "    # Calculate a roc-auc value, using the probabilities that we used to\n",
    "    # replace the labels, and comparing it with the true target:\n",
    "    score = roc_auc_score(y_test, X_test['feature_binned'])\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The authors mention that by using this method, you are able to compare directly numerical with categorical variables. In a sense this is true, however we need to keep in mind, that categorical variables may or may not (and typically they will not) show the same percentage of observations per label. However, when we divide a numerical variable into quantile bins, we guarantee that each bin shows the same percentage of observations.\n",
    "\n",
    "Alternatively, instead of binning into quantiles, we can bin into equal-distance bins.The way to do this, is to calculate the max value - min value range and divide that distance into the amount of bins we want to construct. That would determine the cut-points for the bins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Wrapper Methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Greedy search algorithms.\n",
    "\n",
    "- Utilize a specific classifier to select the optimal set of features\n",
    "\n",
    "- Sequential feature selection algorithms add or remove one feature at the time based on the classifier performance until a feature subset of the desired size k is reached, or any other desired criteria is met"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.a Step Forward Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential feature selection algorithms are a family of greedy search algorithms that are used to reduce an initial d-dimensional feature space to a k-dimensional feature subspace where k < d.\n",
    "\n",
    "Step forward feature selection starts by evaluating all features individually and selects the one that generates the best performing algorithm, according to a pre-set evaluation criteria. In the second step, it evaluates all possible combinations of the selected feature and a second feature, and selects the pair that produce the best performing algorithm based on the same pre-set criteria.\n",
    "\n",
    "The pre-set criteria can be the roc_auc for classification and the r squared for regression for example.\n",
    "\n",
    "This selection procedure is called greedy, because it evaluates all possible single, double, triple and so on feature combinations. Therefore, it is quite computationally expensive, and sometimes, if feature space is big, even unfeasible.\n",
    "\n",
    "There is a special package for python that implements this type of feature selection: `mlxtend`.\n",
    "\n",
    "In the mlxtend implementation of the step forward feature selection, the stopping criteria is an arbitrarily set number of features. So the search will finish when we reach the desired number of selected features.\n",
    "\n",
    "This is somewhat arbitrary because we may be selecting a subopimal number of features, or likewise, a high number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before starting, find and remove correlated features in the dataset and then \n",
    "# execute step forward selection --- Look at the Filter-Correlation section!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stepforwardselect(X_train, y_train, \n",
    "                      performance_score='roc_auc', n=10):\n",
    "    \"\"\"\n",
    "    Step forward feature selection which indicates that to select n features from\n",
    "    the total, and that to select those features based on the optimal roc_auc\n",
    "        Parameters:\n",
    "            X_train (DataFrame): X_train set\n",
    "            y_train (DataFrame): y_train set\n",
    "            performance_score (str): For classification dataset default value is 'roc_auc'\n",
    "            n (int): Total number of selected features. Default value is 10.              \n",
    "        Returns:\n",
    "            selected_feat (list): list of selected features according to step forward feature selection\n",
    "    \"\"\"\n",
    "    # Step forward feature selection by selectin n features from the total, and \n",
    "    # that select those features based on the optimal roc_auc\n",
    "    sfs1 = SFS(RandomForestClassifier(n_jobs=4), \n",
    "               k_features=n, \n",
    "               forward=True,\n",
    "               floating=False, \n",
    "               verbose=2,\n",
    "               scoring=performance_score,\n",
    "               cv=3)\n",
    "    \n",
    "    sfs1 = sfs1.fit(np.array(X_train.fillna(0)), y_train)\n",
    "    selected_feat= X_train.columns[list(sfs1.k_feature_idx_)]\n",
    "\n",
    "    return selected_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stepforwardselect(X_train, y_train, \n",
    "                      performance_score='r2', n=10):\n",
    "    \"\"\"\n",
    "    Step forward feature selection which indicates that to select n features from\n",
    "    the total, and that to select those features based on the optimal r2\n",
    "        Parameters:\n",
    "            X_train (DataFrame): X_train set\n",
    "            y_train (DataFrame): y_train set\n",
    "            performance_score (str): For classification dataset default value is 'r2'\n",
    "            n (int): Total number of selected features. Default value is 10.              \n",
    "        Returns:\n",
    "            selected_feat (list): list of selected features according to step forward feature selection\n",
    "    \"\"\"\n",
    "    # Step forward feature selection by selectin n features from the total, and \n",
    "    # that select those features based on the optimal roc_auc\n",
    "    sfs1 = SFS(RandomForestRegressor(), \n",
    "               k_features=n, \n",
    "               forward=True,\n",
    "               floating=False, \n",
    "               verbose=2,\n",
    "               scoring=performance_score,\n",
    "               cv=3)\n",
    "    \n",
    "    sfs1 = sfs1.fit(np.array(X_train.fillna(0)), y_train)\n",
    "    selected_feat= X_train.columns[list(sfs1.k_feature_idx_)]\n",
    "\n",
    "    return selected_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.b Step Backwards Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential feature selection algorithms are a family of greedy search algorithms that are used to reduce an initial d-dimensional feature space to a k-dimensional feature subspace where k < d.\n",
    "\n",
    "Step backward feature selection starts by fitting a model using all features. Then it removes one feature. It will remove the one that produces the highest performing algorithm for a certain evaluation criteria. In the second step, it will remove a second feature, the one that again produces the best performing algorithm. And it proceeds, removing feature after feature, until a certain criteria is met.\n",
    "\n",
    "The pre-set criteria can be the roc_auc for classification and the r squared for regression for example.\n",
    "\n",
    "This selection procedure is called greedy, because it evaluates all possible n, and then n-1 and n-2 and so on feature combinations. Therefore, it is quite computationally expensive, and sometimes, if feature space is big, even unfeasible.\n",
    "\n",
    "There is a special package for python that implements this type of feature selection:` mlxtend`.\n",
    "\n",
    "In the mlxtend implementation of the step backward feature selection, the stopping criteria is an arbitrarily set number of features. So the search will finish when we reach the desired number of selected features.\n",
    "\n",
    "This is somewhat arbitrary because we may be selecting a subopimal number of features, or likewise, a high number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before starting find and remove correlated features --- Look at the Filter-Correlation section!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stepbackwardselect(X_train, y_train, \n",
    "                      performance_score='roc_auc', n=10):\n",
    "    \"\"\"\n",
    "    Step Backwards feature selection which indicates that to select n features from\n",
    "    the total, and that to select those features based on the optimal roc_auc\n",
    "        Parameters:\n",
    "            X_train (DataFrame): X_train set\n",
    "            y_train (DataFrame): y_train set\n",
    "            performance_score (str): For classification dataset default value is 'roc_auc'\n",
    "            n (int): Total number of selected features. Default value is 10.  \n",
    "        Returns:\n",
    "            selected_feat (list): list of selected features according to step backward feature selection\n",
    "    \"\"\"\n",
    "    # Step backwards feature selection by selectin n features from the total, and \n",
    "    # that select those features based on the optimal roc_auc/r2\n",
    "    sfs1 = SFS(selection_model, \n",
    "               k_features=n, \n",
    "               forward=False, \n",
    "               floating=False, \n",
    "               verbose=2,\n",
    "               scoring=performance_score,\n",
    "               cv=3)\n",
    "\n",
    "    sfs1 = sfs1.fit(np.array(X_train.fillna(0)), y_train)\n",
    "    selected_feat= X_train.columns[list(sfs1.k_feature_idx_)]\n",
    "\n",
    "    return selected_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stepbackwardselect(X_train, y_train, \n",
    "                      performance_score='r2', n=10):\n",
    "    \"\"\"\n",
    "    Step Backwards feature selection which indicates that to select n features from\n",
    "    the total, and that to select those features based on the optimal r2\n",
    "        Parameters:\n",
    "            X_train (DataFrame): X_train set\n",
    "            y_train (DataFrame): y_train set\n",
    "            performance_score (str): For regression dataset default value is 'r2'\n",
    "            n (int): Total number of selected features. Default value is 10.  \n",
    "        Returns:\n",
    "            selected_feat (list): list of selected features according to step backward feature selection\n",
    "    \"\"\"\n",
    "    # Step backwards feature selection by selectin n features from the total, and \n",
    "    # that select those features based on the optimal r2\n",
    "    sfs1 = SFS(RandomForestRegressor(), \n",
    "               k_features=n, \n",
    "               forward=False, \n",
    "               floating=False, \n",
    "               verbose=2,\n",
    "               scoring=performance_score,\n",
    "               cv=3)\n",
    "\n",
    "    sfs1 = sfs1.fit(np.array(X_train.fillna(0)), y_train)\n",
    "    selected_feat= X_train.columns[list(sfs1.k_feature_idx_)]\n",
    "\n",
    "    return selected_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.c Exhaustive Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential feature selection algorithms are a family of greedy search algorithms that are used to reduce an initial d-dimensional feature space to a k-dimensional feature subspace where k < d.\n",
    "\n",
    "In an exhaustive feature selection the best subset of features is selected, over all possible feature subsets, by optimizing a specified performance metric for a certain machine learning algorithm. For example, if the classifier is a logistic regression and the dataset consists of 4 features, the algorithm will evaluate all 15 feature combinations as follows:\n",
    "\n",
    "- all possible combinations of 1 feature\n",
    "- all possible combinations of 2 features\n",
    "- all possible combinations of 3 features\n",
    "- all the 4 features\n",
    "- and select the one that results in the best performance (e.g., classification accuracy) of the logistic regression classifier.\n",
    "\n",
    "This is another greedy algorithm as it evaluates all possible feature combinations. It is quite computationally expensive, and sometimes, `if feature space is big, even unfeasible`.\n",
    "\n",
    "There is a special package for python that implements this type of feature selection: `mlxtend`.\n",
    "\n",
    "In the mlxtend implementation of the exhaustive feature selection, the stopping criteria is an arbitrarily set number of features. So the search will finish when we reach the desired number of selected features.\n",
    "\n",
    "This is somewhat arbitrary because we may be selecting a subopimal number of features, or likewise, a high number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and remove correlated features in order to reduce the feature space a bit so that the algorithm takes shorter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exhaustiveselect(X_train, y_train, n_max,\n",
    "                     performance_score='roc_auc', n_min=1):\n",
    "    \"\"\"\n",
    "    Exhaustive feature selection which indicates that to select n features from\n",
    "    the total, and that to select those features based on the optimal roc_auc\n",
    "        Parameters:\n",
    "            X_train (DataFrame): X_train set\n",
    "            y_train (DataFrame): y_train set\n",
    "            performance_score (str): For classification dataset, the default value is 'roc_auc'\n",
    "            n_min (int): Minimum number of selected features. Default value is 1.\n",
    "            n_max (int): Maximum number of selected features. \n",
    "        Returns:\n",
    "            selected_feat (list): list of selected features according to exaustive feature selection\n",
    "    \"\"\"\n",
    "    # Exhaustive feature selection by selecting n features from the total, and \n",
    "    # that select those features based on the optimal roc_auc/r2\n",
    "    efs1 = EFS(RandomForestClassifier(n_jobs=4), \n",
    "               min_features=n_min,\n",
    "               max_features=n_max, \n",
    "               scoring=performance_score,\n",
    "               print_progress=True,\n",
    "               cv=2)\n",
    "\n",
    "    efs1 = efs1.fit(np.array(X_train[X_train.columns[0:n_max]].fillna(0)), y_train)\n",
    "\n",
    "    selected_feat= X_train.columns[list(efs1.best_idx_)]\n",
    "\n",
    "    return selected_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exhaustiveselect(X_train, y_train, n_max,\n",
    "                     performance_score='r2', n_min=1):\n",
    "    \"\"\"\n",
    "    Exhaustive feature selection which indicates that to select n features from\n",
    "    the total, and that to select those features based on the optimal r2\n",
    "        Parameters:\n",
    "            X_train (DataFrame): X_train set\n",
    "            y_train (DataFrame): y_train set\n",
    "            performance_score (str): For regression dataset, the default value is 'r2'\n",
    "            n_min (int): Minimum number of selected features. Default value is 1.\n",
    "            n_max (int): Maximum number of selected features. \n",
    "        Returns:\n",
    "            selected_feat (list): list of selected features according to exaustive feature selection\n",
    "    \"\"\"\n",
    "    # Exhaustive feature selection by selecting n features from the total, and \n",
    "    # that select those features based on the optimal r2\n",
    "    efs1 = EFS(RandomForestRegressor(n_jobs=4), \n",
    "               min_features=n_min,\n",
    "               max_features=n_max, \n",
    "               scoring=performance_score,\n",
    "               print_progress=True,\n",
    "               cv=3)\n",
    "\n",
    "    efs1 = efs1.fit(np.array(X_train[X_train.columns[0:n_max]].fillna(0)), y_train)\n",
    "\n",
    "    selected_feat= X_train.columns[list(efs1.best_idx_)]\n",
    "\n",
    "    return selected_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exhaustive search is very computationally expensive. Unless you have access to a multicore or distributed computer system I don't see how you can get the most out of it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Embedded Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.a Lasso Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularisation consists in adding a penalty to the different parameters of the machine learning model to reduce the freedom of the model and in other words to avoid overfitting. In linear model regularisation, the penalty is applied over the coefficients that multiply each of the predictors. From the different types of regularisation, Lasso or l1 has the property that is able to shrink some of the coefficients to zero. Therefore, that feature can be removed from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_regularization(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Scale features and use lasso regularization to shrink the coefficients of unimportant features. \n",
    "    This function is for only classification dataset. \n",
    "        Parameters:\n",
    "            X_train (DataFrame): X_train set\n",
    "            y_train (DataFrame): y_train set      \n",
    "        Returns:\n",
    "            X_train_selected (array): X_train after the dataset is splitted as train/test set and removed \n",
    "                                      redundant features via applying lasso regularization technique.\n",
    "            X_test_selected (array): X_test after the dataset is splitted as train/test set and removed \n",
    "                                     redundant features via applying lasso regularization technique.\n",
    "    \"\"\"\n",
    "    print(f'Data shape before applying lasso regularization:\\n\n",
    "          X_train : {X_train.shape}, X_test: {X_test.shape}')\n",
    "    \n",
    "    # linear models benefit from feature scaling\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train.fillna(0))\n",
    "    \n",
    "    # Use the selectFromModel object from sklearn, which will select in theory \n",
    "    # the features which coefficients are non-zero\n",
    "    sel_ = SelectFromModel(LogisticRegression(C=1, penalty='l1'))\n",
    "    sel_.fit(scaler.transform(X_train.fillna(0)), y_train)\n",
    "    \n",
    "    # Make a list with the selected features\n",
    "    selected_feat = X_train.columns[(sel_.get_support())]\n",
    "\n",
    "    print('Total features: {}'.format((X_train.shape[1])))\n",
    "    print('Selected features: {}'.format(len(selected_feat)))\n",
    "    print('Features with coefficients shrank to zero: {}'.format(\n",
    "        np.sum(sel_.estimator_.coef_ == 0)))\n",
    "    \n",
    "    # Identify the removed features\n",
    "    removed_feats = X_train.columns[(sel_.estimator_.coef_ == 0).ravel().tolist()]\n",
    "    print('Removed features: {}'.format(removed_feats))\n",
    "    \n",
    "    # Remove the features from the training and testing set\n",
    "    X_train_selected = sel_.transform(X_train.fillna(0))\n",
    "    X_test_selected = sel_.transform(X_test.fillna(0))\n",
    "\n",
    "    print(f'Data shape before applying lasso regularization:\\n\n",
    "          X_train : {X_train_selected.shape}, X_test: {X_test_selected.shape}')\n",
    "\n",
    "    return X_train_selected, X_test_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_regularization(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Scale features and use lasso regularization to shrink the coefficients of unimportant features. \n",
    "    This function is for only regression dataset. \n",
    "        Parameters:\n",
    "            X_train (DataFrame): X_train set\n",
    "            y_train (DataFrame): y_train set      \n",
    "        Returns:\n",
    "            X_train_selected (array): X_train after the dataset is splitted as train/test set and removed \n",
    "                                      redundant features via applying lasso regularization technique.\n",
    "            X_test_selected (array): X_test after the dataset is splitted as train/test set and removed \n",
    "                                     redundant features via applying lasso regularization technique.\n",
    "    \"\"\"\n",
    "    print(f'Data shape before applying lasso regularization:\\n\n",
    "          X_train : {X_train.shape}, X_test: {X_test.shape}')\n",
    "    \n",
    "    # linear models benefit from feature scaling\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train.fillna(0))\n",
    "    \n",
    "    # Use the selectFromModel object from sklearn, which will select in theory \n",
    "    # the features which coefficients are non-zero\n",
    "    sel_ = SelectFromModel(Lasso(alpha=100))\n",
    "    sel_.fit(scaler.transform(X_train.fillna(0)), y_train)\n",
    "    \n",
    "    # Make a list with the selected features\n",
    "    selected_feat = X_train.columns[(sel_.get_support())]\n",
    "\n",
    "    print('Total features: {}'.format((X_train.shape[1])))\n",
    "    print('Selected features: {}'.format(len(selected_feat)))\n",
    "    print('Features with coefficients shrank to zero: {}'.format(\n",
    "        np.sum(sel_.estimator_.coef_ == 0)))\n",
    "    \n",
    "    # Identify the removed features\n",
    "    removed_feats = X_train.columns[(sel_.estimator_.coef_ == 0).ravel().tolist()]\n",
    "    print('Removed features: {}'.format(removed_feats))\n",
    "    \n",
    "    # Remove the features from the training and testing set\n",
    "    X_train_selected = sel_.transform(X_train.fillna(0))\n",
    "    X_test_selected = sel_.transform(X_test.fillna(0))\n",
    "\n",
    "    print(f'Data shape before applying lasso regularization:\\n\n",
    "          X_train : {X_train_selected.shape}, X_test: {X_test_selected.shape}')\n",
    "\n",
    "    return X_train_selected, X_test_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, both for linear and logistic regression we used the Lasso regularisation to remove non-important features from the dataset. Keep in mind that increasing the penalisation will increase the number of features removed. Therefore, you will need to keep an eye and monitor that you don't set a penalty too high so that to remove even important features, or too low and then not remove non-important features.\n",
    "\n",
    "Having said this, if the penalty is too high and important features are removed, you should notice a drop in the performance of the algorithm and then realise that you need to decrease the regularisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.b Regression Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression is a straightforward approach for predicting a quantitative response Y on the basis of a different predictor variable X1, X2, ... Xn. It assumes that there is a linear relationship between X(s) and Y. Mathematically, we can write this linear relationship as Y ≈ β0 + β1X1 + β2X2 + ... + βnXn.\n",
    "\n",
    "**The magnitude of the coefficients is directly influenced by the scale of the features**. Therefore, to compare coefficients across features, it is importance to have all features within the same scale. This is why, normalisation is important for variable importance and feature selection in linear models. Normalisation is important as well for model performance.\n",
    "\n",
    "In addition, Linear Regression makes the following assumptions over the predictor variables X:\n",
    "- Linear relationship with the outcome Y\n",
    "- Multivariate normality (X should follow a Gaussian distribution)\n",
    "- No or little multicollinearity (Xs should not be linearly related to one another)\n",
    "- Homoscedasticity (variance should be the same)\n",
    "\n",
    "Homoscedasticity, also known as homogeneity of variance, describes a situation in which the error term (that is, the “noise” or random disturbance in the relationship between the independent variables (Xs) and the dependent variable (Y)) is the same across all values of the independent variables.\n",
    "\n",
    "Therefore, there are a lot of assumptions that need to be met in order to make a fair comparison of the features by using only their regression coefficients.\n",
    "\n",
    "In addition, these coefficients may be penalised by regularisation, therefore being smaller than if we were to compare only that individual feature with the target.\n",
    "\n",
    "Having said this, you can still select features based on linear regression coefficients, provided you keep all of these in mind at the time of analysing the outcome.\n",
    "\n",
    "Personally, this is not favorite selection method of choice, although it useful to interpret the output of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Regression Coefficients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_coefficients_regression(X_train, y_train, binsize=50):\n",
    "    \"\"\"\n",
    "    Train a Linear regression and selectfeatures with higher coefficients all in one line of code.\n",
    "    The LinearRegression object from sklearn is a non-regularised linear method. \n",
    "    It fits by matrix multiplication and not gradient descent. Therefore no need to specify penalty and other parameters.\n",
    "    Select features which coefficient are greater than the mean of all feature coefficients.\n",
    "        Parameters:\n",
    "            X_train (DataFrame): X_train set\n",
    "            y_train (DataFrame): y_train set  \n",
    "            binsize (int): Bin in the histogram. Default value is 50.\n",
    "        Returns:\n",
    "            selected_feat (list): List of features which are selected after applying linear \n",
    "                                  coefficients feature selection technique. \n",
    "            selected_plot (plot): Histogram plot for selected features showing coefficient \n",
    "    \"\"\"\n",
    "    print(f'Before applying linear coefficients feature selection technique: X_train : {X_train.shape}, X_test: {X_test.shape}')\n",
    "    \n",
    "    # linear models benefit from feature scaling\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train.fillna(0))\n",
    "    \n",
    "    # Train a Linear regression and select features with higher coefficients\n",
    "    sel_ = SelectFromModel(LinearRegression())\n",
    "    sel_.fit(scaler.transform(X_train.fillna(0)), y_train)\n",
    "    \n",
    "    # Create variable for selected features\n",
    "    selected_feat = X_train.columns[(sel_.get_support())]\n",
    "    \n",
    "    # Compare the amount selected features with the amount of features \n",
    "    # which coefficient is above the mean coefficient\n",
    "    print('Total features: {}'.format((X_train.shape[1])))\n",
    "    print('Selected features: {}'.format(len(selected_feat)))\n",
    "    print('Features with coefficients greater than the mean coefficient: {}'.format(\n",
    "        np.sum(np.abs(sel_.estimator_.coef_) > np.abs(sel_.estimator_.coef_).mean())))\n",
    "    \n",
    "    # Plot the histogram of absolute coefficients\n",
    "    select_plot = pd.Series(np.abs(sel_.estimator_.coef_).ravel()).hist(bins=binsize)\n",
    "    \n",
    "    print(f'After applying linear coefficients feature selection technique: X_train : {X_train[selected_feat].shape}, X_test: {X_test[selected_feat].shape}')    \n",
    "\n",
    "    return selected_feat, selected_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression Coefficients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_coefficients_classif(X_train, y_train, size=0.3):\n",
    "    \"\"\"\n",
    "    Specify the Logistic Regression model, and select the Ridge Penalty (l2). \n",
    "    Evaluate the coefficient magnitude itself and not whether lasso shrinks coefficients to zero\n",
    "    Avoid regularisation at all, so the coefficients are not affected (modified) by the penalty of the regularisation\n",
    "    Use the selectFromModel object from sklearn to automatically select the features.\n",
    "    It will select all the coefficients which absolute values are greater than the mean.\n",
    "    We can of course select a different threshold. Visit the documentation in sklearn to learn how to change this parameter.\n",
    "        Parameters:\n",
    "            X_train (DataFrame): X_train set\n",
    "            y_train (DataFrame): y_train set   \n",
    "        Returns:\n",
    "            selected_feat (list): List of features which are selected after applying linear \n",
    "                                  coefficients feature selection technique. \n",
    "            selected_plot (plot): Plot for selected features showing coefficient \n",
    "    \"\"\"\n",
    "    # Separate train and test sets\n",
    "    print(f'Before applying logistic regression coefficients feature selection technique: X_train : {X_train.shape}, X_test: {X_test.shape}')\n",
    "    \n",
    "    # linear models benefit from feature scaling\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train.fillna(0))\n",
    "    \n",
    "    # Do the model fitting and feature selection\n",
    "    # Set C to 1000, to avoid regularisation. \n",
    "    # Evaluate the coefficient magnitude itself and not whether lasso shrinks coefficients to zero\n",
    "    sel_ = SelectFromModel(LogisticRegression(C=1000, penalty='l2')) \n",
    "    sel_.fit(scaler.transform(X_train.fillna(0)), y_train)\n",
    "    \n",
    "    # Create variable for selected features\n",
    "    selected_feat = X_train.columns[(sel_.get_support())]\n",
    "    \n",
    "    # Compare the amount selected features with the amount of features \n",
    "    # which coefficient is above the mean coefficient\n",
    "    print('Total features: {}'.format((X_train.shape[1])))\n",
    "    print('Selected features: {}'.format(len(selected_feat)))\n",
    "    print('Features with coefficients greater than the mean coefficient: {}'.format(\n",
    "        np.sum(np.abs(sel_.estimator_.coef_) > np.abs(sel_.estimator_.coef_).mean())))\n",
    "    \n",
    "    select_plot = pd.Series(np.abs(sel_.estimator_.coef_).ravel()).hist()\n",
    "    \n",
    "    return select_plot, selected_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.c Tree Derived Variable Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees \n",
    "- Most popular machine learning algorithms \n",
    "- Highly accurate \n",
    "- Good generalization (low overfitting) \n",
    "- Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.c.1 Random Forest Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Random Forests consist of several hundreds of individual decision trees \n",
    "- The impurity decrease for each feature is averaged across trees\n",
    "\n",
    "Limitations:\n",
    "    \n",
    "- Correlated features show equal or similar importance \n",
    "- Correlated features importance is lower than the real importance, determined when tree is built in absence of correlated counterparts \n",
    "- Highly cardinal variables show greater importance (trees are biased to this type of variables)\n",
    "\n",
    "Procedure:\n",
    "    \n",
    "Build a random forest \n",
    "- Determine feature importance \n",
    "- Select the features with highest importance \n",
    "- There is a scikit-learn implementation for this\n",
    "\n",
    "Recursive feature elimination \n",
    "- Build random forests \n",
    "- Calculate feature importance \n",
    "- Remove least important feature \n",
    "- Repeat till a condition is met\n",
    "\n",
    "If the feature removed is correlated to another feature in the dataset, by removing the correlated feature, the true importance of the other feature will be revealed its importance will increase.\n",
    "\n",
    "Random forests are one the most popular machine learning algorithms. They are so successful because they provide in general a good predictive performance, low overfitting and easy interpretability. This interpretability is given by the fact that it is straightforward to derive the importance of each variable on the tree decision. In other words, it is easy to compute how much each variable is contributing to the decision.\n",
    "\n",
    "Random forests consist of 4-12 hundred decision trees, each of them built over a random extraction of the observations from the dataset and a random extraction of the features. Not every tree sees all the features or all the observations, and this guarantees that the trees are de-correlated and therefore less prone to over-fitting. Each tree is also a sequence of yes-no questions based on a single or combination of features. At each node (this is at each question), the three divides the dataset into 2 buckets, each of them hosting observations that are more similar among themselves and different from the ones in the other bucket. Therefore, the importance of each feature is derived by how \"pure\" each of the buckets is.\n",
    "\n",
    "For classification, the measure of impurity is either the Gini impurity or the information gain/entropy. For regression the measure of impurity is variance. Therefore, when training a tree, it is possible to compute how much each feature decreases the impurity. The more a feature decreases the impurity, the more important the feature is. In random forests, the impurity decrease from each feature can be averaged across trees to determine the final importance of the variable.\n",
    "\n",
    "To give you a better intuition, features that are selected at the top of the trees are in general more important than features that are selected at the end nodes of the trees, as generally the top splits lead to bigger information gains.\n",
    "\n",
    "Note\n",
    "- Random Forests and decision trees in general give preference to features with high cardinality\n",
    "- Correlated features will be given equal or similar importance, but overall reduced importance compared to the same tree built without correlated counterparts.\n",
    "\n",
    "Where we put the cut-off to select features is a bit arbitrary. One way is to select the top 10, 20 features. Alternatively, the top 10th percentile. For this, you can use mutual info in combination with SelectKBest or SelectPercentile from sklearn. \n",
    "Selecting features by using tree derived feature importance is a very srtaightforward, fast and generally accurate way of selecting good features for machine learning. In particular, if you are going to build tree methods.\n",
    "\n",
    "However, as I said, correlated features will show in a tree similar and lowered importance, compared to what their importance would be if the tree was built without correlated counterparts. In situations like this, it is better to select features recursively, rather than altogether."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.c.1.a Embedded Random Forest Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomforest_select(X_train, y_train, num_trees=100):\n",
    "    \"\"\"\n",
    "    Specify the Random Forest instance, indicating the number of trees.\n",
    "    Use the selectFromModel object from sklearn to automatically select the features\n",
    "    SelectFrom model will select those features which importance is greater than the mean importance of all the features \n",
    "    by default, but you can alter this threshold if you want to.\n",
    "        Parameters:\n",
    "            X_train (DataFrame): X_train set\n",
    "            y_train (DataFrame): y_train set   \n",
    "            num_trees (int): The number of trees in the forest. The default value is 100.\n",
    "        Returns:\n",
    "            selected_feat (list): List of features which are selected after applying linear \n",
    "                                  coefficients feature selection technique.\n",
    "            importance_plot (plot): Plot for feature importance.      \n",
    "    \"\"\"\n",
    "    print(f'Before applying random forest feature selection technique:\\n\n",
    "          X_train : {X_train.shape}, X_test: {X_test.shape}')\n",
    "    \n",
    "    # Do the model fitting and feature selection\n",
    "    sel_ = SelectFromModel(RandomForestClassifier(n_estimators=num_trees))\n",
    "    sel_.fit(X_train.fillna(0), y_train)\n",
    "    \n",
    "    # Create variable for selected features\n",
    "    selected_feat = X_train.columns[(sel_.get_support())]\n",
    "    \n",
    "    # Compare the amount selected features with the amount of features \n",
    "    # which coefficient is above the mean coefficient\n",
    "    print('Total features: {}'.format((X_train.shape[1])))\n",
    "    print('Selected features: {}'.format(len(selected_feat)))\n",
    "    print('Features with coefficients greater than the mean coefficient: {}'.format(\n",
    "        np.sum(sel_.estimator_.feature_importances_ > sel_.estimator_.feature_importances_.mean())))\n",
    "    \n",
    "    # Plot the distribution of importance\n",
    "    importance_plot = pd.Series(sel_.estimator_.feature_importances_.ravel()).hist()\n",
    "    \n",
    "    return selected_feat, importance_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomforest_select(X_train, y_train, num_trees=100):\n",
    "    \"\"\"\n",
    "    Specify the Random Forest instance, indicating the number of trees.\n",
    "    Use the selectFromModel object from sklearn to automatically select the features\n",
    "    SelectFrom model will select those features which importance is greater than the mean importance of all the features \n",
    "    by default, but you can alter this threshold if you want to.\n",
    "        Parameters:\n",
    "            X_train (DataFrame): X_train set\n",
    "            y_train (DataFrame): y_train set   \n",
    "            num_trees (int): The number of trees in the forest. The default value is 100.\n",
    "        Returns:\n",
    "            selected_feat (list): List of features which are selected after applying linear \n",
    "                                  coefficients feature selection technique.\n",
    "            importance_plot (plot): Plot for feature importance.      \n",
    "    \"\"\"\n",
    "    print(f'Before applying random forest feature selection technique:\\n\n",
    "          X_train : {X_train.shape}, X_test: {X_test.shape}')\n",
    "    \n",
    "    # Do the model fitting and feature selection\n",
    "    sel_ = SelectFromModel(RandomForestRegressor(n_estimators=num_trees))\n",
    "    sel_.fit(X_train.fillna(0), y_train)\n",
    "    \n",
    "    # Create variable for selected features\n",
    "    selected_feat = X_train.columns[(sel_.get_support())]\n",
    "    \n",
    "    # Compare the amount selected features with the amount of features \n",
    "    # which coefficient is above the mean coefficient\n",
    "    print('Total features: {}'.format((X_train.shape[1])))\n",
    "    print('Selected features: {}'.format(len(selected_feat)))\n",
    "    print('Features with coefficients greater than the mean coefficient: {}'.format(\n",
    "        np.sum(sel_.estimator_.feature_importances_ > sel_.estimator_.feature_importances_.mean())))\n",
    "    \n",
    "    # Plot the distribution of importance\n",
    "    importance_plot = pd.Series(sel_.estimator_.feature_importances_.ravel()).hist()\n",
    "    \n",
    "    return selected_feat, importance_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting features by using tree derived feature importance is a very srtaightforward, fast and generally accurate way of selecting good features for machine learning. In particular, if you are going to build tree methods.\n",
    "\n",
    "However, as I said, correlated features will show in a tree similar and lowered importance, compared to what their importance would be if the tree was built without correlated counterparts.\n",
    "\n",
    "In situations like this, it is better to select features recursively, rather than altogether."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.c.1.b Recursive Feature Selection using Random Forest Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forests assign equal or similar importance to features that are highly correlated. In addition, when features are correlated, the importance assigned is lower than the importance attributed to the feature itself, should the tree be built without the correlated counterparts.\n",
    "\n",
    "Therefore, instead of eliminating features based on importance by brute force like we did in the previous lecture, we may get a better selection by removing one feature at a time, and recalculating the importance on each round.\n",
    "\n",
    "This method is an hybrid between embedded and wrapper methods: it is based on computation derived when fitting the model, but it also requires fitting several models.\n",
    "\n",
    "The cycle is as follows:\n",
    "\n",
    "- Build random forests using all features\n",
    "- Remove least important feature\n",
    "- Build random forests and recalculate importance\n",
    "- Repeat until a criteria is met\n",
    "\n",
    "In this situation, when a feature that is highly correlated to another one is removed, then, the importance of the remaining feature increases. This may lead to a better subset feature space selection. On the downside, building several random forests is quite time consuming, in particular if the dataset contains a high number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomforest_recursive(X_train, y_train, num_trees=100, n=10):\n",
    "    \"\"\"\n",
    "    Specify the Random Forest instance, indicating the number of trees.\n",
    "    Use the selectFromModel object from sklearn to automatically select the features\n",
    "    SelectFrom model will select those features which importance is greater than the mean importance of all the features by default, but you can alter this threshold if you want to\n",
    "        Parameters:\n",
    "            X_train (DataFrame): X_train set\n",
    "            y_train (DataFrame): y_train set   \n",
    "            num_trees (int): The number of trees in the forest. The default value is 100.\n",
    "            n (int): Number of feature selected. The default value is 10.\n",
    "        Returns:\n",
    "            selected_feat (list): List of features which are selected after applying random forest \n",
    "                                  selection technique. \n",
    "    \"\"\"\n",
    "    print(f'Before applying random forest recursive feature selection technique:\\n\n",
    "          X_train : {X_train.shape}, X_test: {X_test.shape}')\n",
    "    \n",
    "    # Do the model fitting and feature selection\n",
    "    sel_ = RFE(RandomForestClassifier(n_estimators=num_trees), n_features_to_select=n)\n",
    "    sel_.fit(X_train.fillna(0), y_train)\n",
    "    \n",
    "    # Create variable for selected features\n",
    "    selected_feat = X_train.columns[(sel_.get_support())]\n",
    "    \n",
    "    return selected_feat "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting features recursively may not add any advantage to the random forest algorithm. And yet it took a massive amount of time to run. Keep this in mind at the time of selecting with method you are going to use.\n",
    "\n",
    "In experience, RFE from sklearn does not bring forward a massive advantage respect to the SelectFromModel method, and personally I tend to use the second to select my features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.c.2 Gradient Boosted Trees Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature importance calculated in the same way:\n",
    "\n",
    "- Biased to highly cardinal features \n",
    "- Importance is susceptible to correlated features \n",
    "- Interpretability of feature importance is not so straightforward: \n",
    "- Later trees fit to the errors of the first trees, therefore feature importance is not necessarily proportional on the influence of the feature on the outcome, rather on the mistakes of the previous trees. \n",
    "- Averaging across trees may not add much information on true relation between feature and target\n",
    "\n",
    "Similarly to selecting features using Random Forests derived feature importance, you can select features based on the importance derived by gradient boosted trees. And you can do that in one go, or in a recursive manner, depending on how much time you have, how many features are in the dataset, and whether they are correlated or not.\n",
    "\n",
    "Same as with the random forest derived importance feature selection, the recursive procedure did not add any advantage over the altogether selection. And it took a substantial amount of time to compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_selection import SelectFromModel, RFE\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.c.2.a Gradient Boosted Trees Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientboost_select(X_train, X_test, y_train):\n",
    "    \"\"\"\n",
    "    Select features based on the importance derived by gradient boosted trees.\n",
    "        Parameters:\n",
    "            X_train (DataFrame): X_train set\n",
    "            X_test (DataFrame): X_test set\n",
    "            y_train (DataFrame): y_train set \n",
    "        Returns:\n",
    "            selected_feat (list): List of features which are selected after applying gradient \n",
    "                                  boosted feature importance technique.                    \n",
    "    \"\"\"\n",
    "    print(f'Data shape before applying gradient boosted trees feature selection technique:\\n\n",
    "          X_train : {X_train.shape}, X_test: {X_test.shape}')\n",
    "    \n",
    "    # Select features all together in one go by contemplating their importance \n",
    "    # after fitting only 1 gradient boosted tree\n",
    "    sel_ = SelectFromModel(GradientBoostingClassifier())\n",
    "    sel_.fit(X_train.fillna(0), y_train)\n",
    "    \n",
    "    # Create variable for selected features\n",
    "    selected_feat = X_train.columns[(sel_.get_support())]\n",
    "    \n",
    "    return selected_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.c.2.b Recursive Feature Selection using Gradient Boosted Trees Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientboost_select(X_train, X_test, y_train):\n",
    "    \"\"\"\n",
    "    Select features recursively based on the importance derived by gradient boosted trees.\n",
    "        Parameters:\n",
    "            X_train (DataFrame): X_train set\n",
    "            X_test (DataFrame): X_test set\n",
    "            y_train (DataFrame): y_train set \n",
    "        Returns:\n",
    "            selected_feat_rfe (list): List of features which are selected after applying gradient \n",
    "                                      boosted feature importance technique recursively.                       \n",
    "    \"\"\"\n",
    "    print(f'Data shape before applying random forest feature selection technique:\\n\n",
    "          X_train : {X_train.shape}, X_test: {X_test.shape}')\n",
    "    \n",
    "    # Select features all together in one go by contemplating their importance \n",
    "    # after fitting only 1 gradient boosted tree\n",
    "    sel_ = SelectFromModel(GradientBoostingClassifier())\n",
    "    sel_.fit(X_train.fillna(0), y_train)\n",
    "    \n",
    "    # Create variable for selected features\n",
    "    selected_feat = X_train.columns[(sel_.get_support())]\n",
    "    \n",
    "    # Select features recursively for comparison\n",
    "    sel_ = RFE(GradientBoostingClassifier(), n_features_to_select=len(selected_feat))\n",
    "    sel_.fit(X_train.fillna(0), y_train)\n",
    "    \n",
    "    # Create variable for selected features after recursive selection\n",
    "    selected_feat_rfe = X_train.columns[(sel_.get_support())]\n",
    "    \n",
    "    return selected_feat_rfe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Gradient Boosting model and compare train and test set performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gradientboosting(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Create a function to build gradient boosted trees and compare \n",
    "    performance in train and test set.\n",
    "        Parameters:\n",
    "            X_train (array): X_train after the dataset is splitted as train/test set and removed \n",
    "                             redundant features via applying gradient boosted feature \n",
    "                             importance technique.\n",
    "            X_test (array):  X_test after the dataset is splitted as train/test set and removed \n",
    "                             redundant features via applying gradient boosted feature \n",
    "                             importance technique.\n",
    "            y_train (array): y_train after the dataset is splitted as train and test set.\n",
    "            y_test (array):  y_test after the dataset is splitted as train and test set. \n",
    "        Returns:\n",
    "            none --> print train and test set model performance\n",
    "    \"\"\"\n",
    "    rf = GradientBoostingClassifier(\n",
    "        n_estimators=200, random_state=0, max_depth=4)\n",
    "    rf.fit(X_train, y_train)\n",
    "    print('Train set')\n",
    "    pred = rf.predict_proba(X_train)\n",
    "    print('Random Forests roc-auc: {}'.format(\n",
    "        roc_auc_score(y_train, pred[:, 1])))\n",
    "    print('Test set')\n",
    "    pred = rf.predict_proba(X_test)\n",
    "    print('Random Forests roc-auc: {}'.format(\n",
    "        roc_auc_score(y_test, pred[:, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features selected recursively \n",
    "# does not add avantage over the altogether selection and takes substantial amount of time to compute.\n",
    "run_gradientboosting(X_train[selected_feat_rfe].fillna(0),\n",
    "                     X_test[selected_feat_rfe].fillna(0),\n",
    "                     y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features selected altogether\n",
    "run_gradientboosting(X_train[selected_feat].fillna(0),\n",
    "                     X_test[selected_feat].fillna(0),\n",
    "                     y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as with the random forest derived importance feature selection, the recursive procedure may not add any advantage over the altogether selection. And it took a substantial amount of time to compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hybrid Methods (Combination of Wrapper and Embedded Methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.a Shuffling Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A popular method of feature selection consists in random shuffling the values of a specific variable and determining how that permutation affects the performance metric of the machine learning algorithm. In other words, the idea is to permute the values of each feature, one at the time, and measure how much the permutation decreases the accuracy, or the roc_auc, or the mse of the machine learning model. If the variables are important, this is, highly predictive, a random permutation of their values will decrease dramatically any of these metrics. Contrarily, non-important / non-predictive variables, should have little to no effect on the model performance metric we are assessing.\n",
    "\n",
    "**Note:**\n",
    "\n",
    "Random Forest model is used here, however this method is useful for any machine learning algorithm. In fact, the importance of the features are determined specifically for the algorithm used. Therefore, different algorithms may return different subsets of important features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For classification \n",
    "def shuffling_features_classif(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Permute the values of each feature, one at the time, and measure how much the permutation decreases the accuracy, \n",
    "    or the roc_auc, or the mse of the machine learning model. \n",
    "    If the variables are important, this is, highly predictive, a random permutation of their values will decrease \n",
    "    dramatically any of these metrics. Contrarily, non-important/non-predictive variables, \n",
    "    should have little to no effect on the model performance metric we are assessing.\n",
    "        Parameters:\n",
    "            X_train (DataFrame): X_train set\n",
    "            X_test (DataFrame): X_test set\n",
    "            y_train (DataFrame): y_train set \n",
    "            y_test (DataFrame): y_test set\n",
    "        Returns:\n",
    "            feature_importance (DataFrame): A dataframe consist of all 'feature' and 'auc_drop' columns.\n",
    "                                            Those features, which auc_drop is greater that 0 (feature_importance.auc_drop>0),\n",
    "                                            cause a drop in the performance of the model when their values were permuted. \n",
    "                                            This means that we could select those features and discard the rest, \n",
    "                                            and should keep the original model performance.\n",
    "                                            Note: if you want to have the list of important features, after runing this function,\n",
    "                                                  feature_importance[feature_importance.auc_drop>0]['feature'] script will \n",
    "                                                  return it. \n",
    "    \"\"\"\n",
    "    print(f'Data shape before applying feature shuffling technique:\\n\n",
    "          X_train : {X_train.shape}, X_test: {X_test.shape}')\n",
    "    \n",
    "    # The first step to determine feature importance by feature shuffling\n",
    "    # is to build the machine learning model for which we want to select features\n",
    "    # In this case, Random Forests model is built, but remember that \n",
    "    # you can use this procedure for any other machine learning algorithm\n",
    "    # Few and shallow trees is built to avoid overfitting\n",
    "    \n",
    "    model = RandomForestClassifier(n_estimators=50, max_depth=2, random_state=2909, n_jobs=4)\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train.fillna(0), y_train)\n",
    "    \n",
    "    # print roc-auc in train and testing sets\n",
    "    print('train auc score: ',\n",
    "          roc_auc_score(y_train, (rf.predict_proba(X_train.fillna(0)))[:, 1]))\n",
    "    print('test auc score: ',\n",
    "          roc_auc_score(y_test, (rf.predict_proba(X_test.fillna(0)))[:, 1]))\n",
    "\n",
    "    # Shuffle one by one, each feature of the dataset and then use the dataset with \n",
    "    # the shuffled variable to make predictions using the model which is trained above\n",
    "\n",
    "    # Overall train roc-auc: using all the features\n",
    "    train_auc = roc_auc_score(y_train, (rf.predict_proba(X_train.fillna(0)))[:, 1])\n",
    "\n",
    "    # Dictionary to capture the features and the drop in auc that they cause when shuffled\n",
    "    feature_dict = {}\n",
    "\n",
    "    # Selection  logic\n",
    "    for feature in X_train.columns:\n",
    "        X_train_c = X_train.copy()\n",
    "\n",
    "        # Shuffle individual feature\n",
    "        X_train_c[feature] = X_train_c[feature].sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "        # Make prediction with shuffled feature and calculate roc-auc\n",
    "        shuff_auc = roc_auc_score(y_train, (rf.predict_proba(X_train_c.fillna(0)))[:, 1])\n",
    "\n",
    "        # Save the drop in roc-auc\n",
    "        feature_dict[feature] = (train_auc - shuff_auc)\n",
    "        \n",
    "    # Transform the dictionary into a pandas dataframe for easy manipulation\n",
    "    feature_importance = pd.Series(feature_dict).reset_index()\n",
    "    feature_importance.columns = ['feature', 'auc_drop']\n",
    "    \n",
    "    # Sort the dataframe according to the drop in performance caused by feature shuffling\n",
    "    feature_importance.sort_values(by=['auc_drop'], ascending=False, inplace=True)\n",
    "    \n",
    "    print(f'Number of features that cause a drop in performance when shuffled: ', feature_importance[feature_importance.auc_drop>0].shape[0])\n",
    "    \n",
    "    return feature_importance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For regression\n",
    "def shuffling_features_regress(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Permute the values of each feature, one at the time, and measure how much the permutation decreases the accuracy, \n",
    "    or the roc_auc, or the mse of the machine learning model. \n",
    "    If the variables are important, this is, highly predictive, a random permutation of their values will decrease \n",
    "    dramatically any of these metrics. Contrarily, non-important/non-predictive variables, \n",
    "    should have little to no effect on the model performance metric we are assessing.\n",
    "        Parameters:\n",
    "            X_train (DataFrame): X_train set\n",
    "            X_test (DataFrame): X_test set\n",
    "            y_train (DataFrame): y_train set \n",
    "            y_test (DataFrame): y_test set\n",
    "        Returns:\n",
    "            feature_importance (DataFrame): A dataframe consist of all 'feature' and 'rmse_drop' columns.\n",
    "                                            Those features, which rmse_drop is greater that 0 (feature_importance.rmse_drop>0),\n",
    "                                            cause a drop in the performance of the model when their values were permuted. \n",
    "                                            This means that we could select those features and discard the rest, \n",
    "                                            and should keep the original model performance.\n",
    "                                            Note: if you want to have the list of important features, after runing this function,\n",
    "                                                  feature_importance[feature_importance.rmse_drop>0]['feature'] script will \n",
    "                                                  return it. \n",
    "    \"\"\"\n",
    "    print(f'Data shape before applying feature shuffling technique:\\n\n",
    "          X_train : {X_train.shape}, X_test: {X_test.shape}')\n",
    "    \n",
    "    # The first step to determine feature importance by feature shuffling\n",
    "    # is to build the machine learning model for which we want to select features\n",
    "    # In this case, Random Forests model is built, but remember that \n",
    "    # you can use this procedure for any other machine learning algorithm\n",
    "    # Few and shallow trees are built to avoid overfitting\n",
    "\n",
    "    model = RandomForestRegressor(n_estimators=100, max_depth=3, random_state=2909, n_jobs=4)\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train.fillna(0), y_train)\n",
    "    \n",
    "    # print rmse in train and testing sets\n",
    "    print('train rmse: ',\n",
    "          np.sqrt(mean_squared_error(y_train, (rf.predict(X_train.fillna(0))))))\n",
    "    print('test rmse: ',\n",
    "          np.sqrt(mean_squared_error(y_test, (rf.predict(X_test.fillna(0))))))\n",
    "\n",
    "    # Shuffle one by one, each feature of the dataset and then use the dataset with \n",
    "    # the shuffled variable to make predictions using the model which is trained above\n",
    "\n",
    "    # overall train rmse: using all the features\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, (rf.predict(X_train.fillna(0)))))\n",
    "\n",
    "    # Dictionary to capture the features and the drop in rmse that they cause when shuffled\n",
    "    feature_dict = {}\n",
    "\n",
    "    # Selection  logic\n",
    "    for feature in X_train.columns:\n",
    "        X_train_c = X_train.copy()\n",
    "\n",
    "        # Shuffle individual feature\n",
    "        X_train_c[feature] = X_train_c[feature].sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "       # make prediction with shuffled feature and calculate rmse\n",
    "        shuff_rmse = np.sqrt(mean_squared_error(y_train, (rf.predict(X_train_c.fillna(0)))))\n",
    "\n",
    "        # store the drop in rmse\n",
    "        feature_dict[feature] = (train_rmse - shuff_rmse)\n",
    "        \n",
    "    # Transform the dictionary into a pandas dataframe for easy manipulation\n",
    "    feature_importance = pd.Series(feature_dict).reset_index()\n",
    "    feature_importance.columns = ['feature', 'rmse_drop']\n",
    "    \n",
    "    # Sort the dataframe according to the drop in performance caused by feature shuffling\n",
    "    feature_importance.sort_values(by=['rmse_drop'], ascending=False, inplace=True)\n",
    "    \n",
    "    print(f'Number of features that cause a drop in performance when shuffled: ', feature_importance[feature_importance.rmse_drop>0].shape[0])\n",
    "    \n",
    "    return feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize Important Features Distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotfeatureimp(feature_importance):\n",
    "    \"\"\"\n",
    "    Plot the distribution of importances for those features that are actually important.\n",
    "        Parameters:\n",
    "            feature_importance (DataFrame): DataFrame which is the product of shuffling_features function.\n",
    "        Returns:\n",
    "            plot (bar plot): Bar plot showing distribution of feature importance.\n",
    "    \"\"\"\n",
    "    # Capture the important features in a series for visualisation\n",
    "    temp = pd.Series(feature_importance[feature_importance.auc_drop>0]['auc_drop'])\n",
    "    temp.index = pd.Series(feature_importance[feature_importance.auc_drop>0]['feature'])\n",
    "\n",
    "    plot = pd.Series(temp).plot.bar(figsize=(15,6))\n",
    "    \n",
    "    return plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.b Hybrid Method: Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method consists of the following steps:\n",
    "\n",
    "1) Rank the features according to their importance derived from a machine learning algorithm: it can be tree importance, or LASSO / Ridge, or the linear / logistic regression coefficients.\n",
    "\n",
    "2) Remove one feature -the least important- and build a machine learning algorithm utilising the remaining features.\n",
    "\n",
    "3) Calculate a performance metric of your choice: roc-auc, mse, rmse, accuracy.\n",
    "\n",
    "4) If the metric decreases by more of an arbitrarily set threshold, then that feature is important and should be kept. Otherwise, we can remove that feature.\n",
    "\n",
    "5) Repeat steps 2-4 until all features have been removed (and therefore evaluated) and the drop in performance assessed.\n",
    "\n",
    "\n",
    "This is called as a hybrid method because:\n",
    "\n",
    "- it combines the importance derived from the machine learning algorithm like embedded methods,\n",
    "- and it removes as well one feature at a time, and calculates a new metric based on the new subset of features and the machine learning algorithm of choice, like wrapper methods.\n",
    "\n",
    "The difference between this method and the step backwards feature selection lies in that it does not remove all features first in order to determine which one to remove. It removes the least important one, based on the machine learning model derived important. And then, it makes an assessment as to whether that feature should be removed or not. So it removes each feature only once during selection, whereas step backward feature selection removes all the features at each step of selection.\n",
    "\n",
    "This method is therefore faster than wrapper methods and generally better than embedded methods. In practice it works extremely well. It does also account for correlations (depending on how stringent you set the arbitrary performance drop threshold). On the downside, the drop in performance assessed to decide whether the feature should be kept or removed, is set arbitrarily. The smaller the drop the more features will be selected, and vice versa.\n",
    "\n",
    "**Note** For the demonstration, XGBoost model is built here, however this method is useful for any machine learning algorithm. In fact, the importance of the features are determined specifically for the algorithm used. Therefore, different algorithms may return different subsets of important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For classification 1/2 - After this function, you should run following function \n",
    "def hybrid_recursive_eliminate_1(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Rank the features according to their importance derived from a machine learning algorithm: \n",
    "    it can be tree importance, or LASSO/Ridge, or the linear/logistic regression coefficients.\n",
    "        Parameters:\n",
    "            X_train (DataFrame): X_train set\n",
    "            X_test (DataFrame): X_test set\n",
    "            y_train (DataFrame): y_train set \n",
    "            y_test (DataFrame): y_test set\n",
    "        Returns:\n",
    "            features (Series): Features ranked according to their importance derived from a \n",
    "                               machine learning algorithm.\n",
    "            plot (Plot): Plot the importance of each feature, which is ranked from the \n",
    "                         least to the most important in bar graph. \n",
    "    \"\"\"\n",
    "    print(f'Data shape before applying hybrid recursive feature elimination technique:\\n\n",
    "          X_train : {X_train.shape}, X_test: {X_test.shape}')\n",
    "    \n",
    "    # The first step of this procedure  consists in building a machine learning algorithm using \n",
    "    # all the available features and then determine the importance of the features according\n",
    "    # to the algorithm.\n",
    "\n",
    "    # Set the seed for reproducibility\n",
    "    seed_val = 1000000000\n",
    "    np.random.seed(seed_val)\n",
    "\n",
    "    # Build initial model using all the features\n",
    "    model_all_features = xgb.XGBClassifier(nthread=10, max_depth=4, n_estimators=500, learning_rate=0.05)\n",
    "    model_all_features.fit(X_train, y_train)\n",
    "\n",
    "    # Calculate the roc-auc in the test set\n",
    "    y_pred_test = model_all_features.predict_proba(X_test)[:, 1]\n",
    "    auc_score_all = roc_auc_score(y_test, y_pred_test)\n",
    "    print('Test all features xgb ROC AUC=%f' % (auc_score_all))\n",
    "    \n",
    "    # Derive the importance of each feature and rank them from the least to the most important\n",
    "    features = pd.Series(model_all_features.feature_importances_)\n",
    "    features.index = X_train.columns\n",
    "    features.sort_values(ascending=True, inplace=True)\n",
    "    \n",
    "    # Plot feature importance \n",
    "    plot = features.plot.bar(figsize=(20,6))\n",
    "    \n",
    "    return features, plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For classification continue - 2/2\n",
    "def hybrid_recursive_eliminate_2(features, threshold=0.0005):\n",
    "    \"\"\"\n",
    "    1) Remove one feature -the least important- and build a machine learning algorithm utilising \n",
    "       the remaining features.\n",
    "    2) Calculate a performance metric of your choice: roc-auc, mse, rmse, accuracy.\n",
    "    3) If the metric decreases by more of an arbitrarily set threshold, then that feature is \n",
    "       important and should be kept. Otherwise, we can remove that feature.\n",
    "    4) Repeat steps 1-3 until all features have been removed (and therefore evaluated) and \n",
    "       the drop in performance assessed.\n",
    "            Parameters:\n",
    "                features (series): Features ranked according to their importance derived from a \n",
    "                                   machine learning algorithm. It is output of previous function.\n",
    "                threshold (float): Threshold for the drop in roc-auc. The default value is 0.0005.\n",
    "            Returns:\n",
    "                features_to_keep (list): Important features derived from hybrid recursive feature selection. \n",
    "    \"\"\"  \n",
    "    # Convert ordered features into list\n",
    "    features_list = list(features.index) \n",
    "    \n",
    "    # Removing one at a time all the features, from the least to the most important, \n",
    "    # and build an xgboost at each round. Once build the model, calculate the new roc-auc\n",
    "    # if the new roc-auc is smaller than the original one (with all the features), \n",
    "    # then that feature that was removed was important, and we should keep it.\n",
    "    # Otherwise, we should remove the feature\n",
    "\n",
    "    # recursive feature elimination:\n",
    "\n",
    "    # Arbitrarily set the drop in roc-auc\n",
    "    # if the drop is below this threshold, the feature will be removed\n",
    "    tol = threshold\n",
    "\n",
    "    print('Doing recursive feature elimination')\n",
    "\n",
    "    # Initialise a list where collecting the features we should remove\n",
    "    features_to_remove = []\n",
    "\n",
    "    # set a counter to know how far ahead the loop is going\n",
    "    count = 1\n",
    "\n",
    "    # Loop over all the features, in order of importance:\n",
    "    for feature in features_list:\n",
    "        print()\n",
    "        print('Testing feature: ', feature, ' which is feature ', count,\n",
    "              ' out of ', len(features))\n",
    "        count = count + 1\n",
    "\n",
    "        # Initialise model\n",
    "        model_int = xgb.XGBClassifier(nthread=10, max_depth=4, n_estimators=500, learning_rate=0.05)\n",
    "\n",
    "        # Fit model with all variables minus the removed features and the feature to be evaluated\n",
    "        model_int.fit(X_train.drop(features_to_remove + [feature], axis=1), y_train)\n",
    "\n",
    "        # Make a prediction over the test set\n",
    "        y_pred_test = model_int.predict_proba(\n",
    "            X_test.drop(features_to_remove + [feature], axis=1))[:, 1]\n",
    "\n",
    "        # Calculate the new roc-auc\n",
    "        auc_score_int = roc_auc_score(y_test, y_pred_test)\n",
    "        print('New Test ROC AUC={}'.format((auc_score_int)))\n",
    "\n",
    "        # Print the original roc-auc with all the features\n",
    "        print('All features Test ROC AUC={}'.format((auc_score_all)))\n",
    "\n",
    "        # Determine the drop in the roc-auc\n",
    "        diff_auc = auc_score_all - auc_score_int\n",
    "\n",
    "        # Compare the drop in roc-auc with the tolerance which was set previously\n",
    "        if diff_auc >= tol:\n",
    "            print('Drop in ROC AUC={}'.format(diff_auc))\n",
    "            print('keep: ', feature)\n",
    "            print\n",
    "        else:\n",
    "            print('Drop in ROC AUC={}'.format(diff_auc))\n",
    "            print('remove: ', feature)\n",
    "            print\n",
    "            # if the drop in the roc is small and remove the feature, \n",
    "            # need to set the new roc to the one based on the remaining features\n",
    "            auc_score_all = auc_score_int\n",
    "\n",
    "            # Append the feature to remove to the collecting list\n",
    "            features_to_remove.append(feature)\n",
    "\n",
    "    # Loop is finished, and evaluated all the features\n",
    "    print('DONE!!')\n",
    "    print('total features to remove: ', len(features_to_remove))\n",
    "\n",
    "    # Determine the features to keep (those we won't remove)\n",
    "    features_to_keep = [x for x in features_list if x not in features_to_remove]\n",
    "    print('total features to keep: ', len(features_to_keep))\n",
    "\n",
    "    return features_to_keep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may not be able to get this right from the beginning though, as we did here. This method of feature selection does require that you try a few different tolerances / thresholds until you find the right number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For regression 1/2 - After this function, you should run following function \n",
    "def hybrid_recursive_eliminate_1(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Rank the features according to their importance derived from a machine learning algorithm: \n",
    "    it can be tree importance, or LASSO/Ridge, or the linear/logistic regression coefficients.\n",
    "        Parameters:\n",
    "            X_train (DataFrame): X_train set\n",
    "            X_test (DataFrame): X_test set\n",
    "            y_train (DataFrame): y_train set \n",
    "            y_test (DataFrame): y_test set\n",
    "        Returns:\n",
    "            features (Series): Features ranked according to their importance derived from a \n",
    "                               machine learning algorithm.\n",
    "            plot (Plot): Plot the importance of each feature, which is ranked from the \n",
    "                         least to the most important in bar graph. \n",
    "    \"\"\"\n",
    "    print(f'Data shape before applying hybrid recursive feature elimination technique:\\n\n",
    "          X_train : {X_train.shape}, X_test: {X_test.shape}')\n",
    "    \n",
    "    # The first step of this procedure  consists in building a machine learning algorithm using \n",
    "    # all the available features and then determine the importance of the features according\n",
    "    # to the algorithm.\n",
    "\n",
    "    # Set the seed for reproducibility\n",
    "    seed_val = 1000000000\n",
    "    np.random.seed(seed_val)\n",
    "\n",
    "    # Build initial model using all the features\n",
    "    model_all_features = xgb.XGBRegressor(nthread=10, max_depth=4, n_estimators=500, learning_rate=0.05)\n",
    "    model_all_features.fit(X_train, y_train)\n",
    "\n",
    "    # Calculate the r2 in the test set\n",
    "    y_pred_test = model_all_features.predict(X_test)\n",
    "    r2_score_all = r2_score(y_test, y_pred_test)\n",
    "    print('Test all features xgb R2 = %f' % (r2_score_all))\n",
    "    \n",
    "    # Derive the importance of each feature and rank them from the least to the most important\n",
    "    features = pd.Series(model_all_features.feature_importances_)\n",
    "    features.index = X_train.columns\n",
    "    features.sort_values(ascending=True, inplace=True)\n",
    "    \n",
    "    # Plot feature importance \n",
    "    plot = features.plot.bar(figsize=(20,6))\n",
    "    \n",
    "    return features, plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For regression continue - 2/2\n",
    "def hybrid_recursive_eliminate_2(features, threshold=0.001):\n",
    "    \"\"\"\n",
    "    1) Remove one feature -the least important- and build a machine learning algorithm utilising \n",
    "       the remaining features.\n",
    "    2) Calculate a performance metric of your choice: roc-auc, mse, rmse, accuracy.\n",
    "    3) If the metric decreases by more of an arbitrarily set threshold, then that feature is \n",
    "       important and should be kept. Otherwise, we can remove that feature.\n",
    "    4) Repeat steps 1-3 until all features have been removed (and therefore evaluated) and \n",
    "       the drop in performance assessed.\n",
    "            Parameters:\n",
    "                features (series): Features ranked according to their importance derived from a \n",
    "                                   machine learning algorithm. It is output of previous function.\n",
    "                threshold (float): Threshold for the drop in r2. The default value is 0.001.\n",
    "            Returns:\n",
    "                features_to_keep (list): Important features derived from hybrid recursive feature selection. \n",
    "    \"\"\"  \n",
    "    # Convert ordered features into list\n",
    "    features_list = list(features.index) \n",
    "    \n",
    "    # Removing one at a time all the features, from the least to the most\n",
    "    # important, and build an xgboost at each round.\n",
    "    # Once we build the model, we calculate the new r2, if the new r2 is smaller than the original one\n",
    "    # (with all the features), then that feature that was removed was important, and we should keep it.\n",
    "    # Otherwise, we should remove the feature.\n",
    "\n",
    "    # Arbitrarily set the drop in r2\n",
    "    # if the drop is below this threshold, the feature will be removed\n",
    "    tol = threshold\n",
    "\n",
    "    print('Doing recursive feature elimination')\n",
    "\n",
    "    # Initialise a list where we will collect the features we should remove\n",
    "    features_to_remove = []\n",
    "\n",
    "    # Set a counter to know how far ahead the loop is going\n",
    "    count = 1\n",
    "\n",
    "    # Loop over all the features, in order of importance:\n",
    "    # Remember that features is the list of ordered features by importance\n",
    "    \n",
    "    for feature in features:\n",
    "        print()\n",
    "        print('testing feature: ', feature, ' which is feature ', count,\n",
    "              ' out of ', len(features))\n",
    "        count = count + 1\n",
    "\n",
    "        # Initialise model\n",
    "        model_int = xgb.XGBRegressor(nthread=10, max_depth=4, n_estimators=500, learning_rate=0.05)\n",
    "\n",
    "        # Fit model with all variables minus the removed features and the feature to be evaluated\n",
    "        model_int.fit(X_train.drop(features_to_remove + [feature], axis=1), y_train)\n",
    "\n",
    "        # Make a prediction over the test set\n",
    "        y_pred_test = model_int.predict(X_test.drop(features_to_remove + [feature], axis=1))\n",
    "\n",
    "        # Calculate the new r2\n",
    "        r2_score_int = r2_score(y_test, y_pred_test)\n",
    "        print('New Test r2 = {}'.format((r2_score_int)))\n",
    "\n",
    "        # Print the original r2 with all the features\n",
    "        print('All features Test r2 = {}'.format((r2_score_all)))\n",
    "\n",
    "        # Determine the drop in the r2\n",
    "        diff_r2 = r2_score_all - r2_score_int\n",
    "\n",
    "        # Compare the drop in r2 with the tolerance which is set previously\n",
    "        if diff_r2 >= tol:\n",
    "            print('Drop in r2 ={}'.format(diff_r2))\n",
    "            print('keep: ', feature)\n",
    "            print\n",
    "        else:\n",
    "            print('Drop in r2 = {}'.format(diff_r2))\n",
    "            print('remove: ', feature)\n",
    "            print\n",
    "            # if the drop in the r2 is small and we remove the feature, \n",
    "            # we need to set the new r2 to the one based on the remaining features\n",
    "            r2_score_all = r2_score_int\n",
    "\n",
    "            # Append the feature to remove to the collecting list\n",
    "            features_to_remove.append(feature)\n",
    "\n",
    "    # loop is finished, all the features were evaluated.\n",
    "    print('DONE!!')\n",
    "    print('Total features to remove: ', len(features_to_remove))\n",
    "\n",
    "    # Determine the features to keep (those we won't remove)\n",
    "    features_to_keep = [x for x in features if x not in features_to_remove]\n",
    "    print('total features to keep: ', len(features_to_keep))\n",
    "    \n",
    "    return features_to_keep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.c Hybrid Method: Recursive Feature Addition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method consists of the following steps:\n",
    "\n",
    "1) Rank the features according to their importance derived from a machine learning algorithm: it can be tree importance, or LASSO / Ridge, or the linear / logistic regression coefficients.\n",
    "\n",
    "2) Build a machine learning model with only 1 feature, the most important one, and calculate the model metric for performance.\n",
    "\n",
    "3) Add one feature -the most important- and build a machine learning algorithm utilising the added and any feature from previous rounds.\n",
    "\n",
    "4) Calculate a performance metric of your choice: roc-auc, mse, rmse, accuracy.\n",
    "\n",
    "5) If the metric increases by more than an arbitrarily set threshold, then that feature is important and should be kept. Otherwise, we can remove that feature.\n",
    "\n",
    "6) Repeat steps 2-5 until all features have been removed (and therefore evaluated) and the drop in performance assessed.\n",
    "\n",
    "\n",
    "This is called as a hybrid method because:\n",
    "\n",
    "- it combines the importance derived from the machine learning algorithm like embedded methods,\n",
    "- and it adds as well one feature at a time, and calculates a new metric based on the new subset of features and the machine learning algorithm of choice, like wrapper methods.\n",
    "\n",
    "The difference between this method and the step forward feature selection lies in that it does not add all possible features first, in order to determine which one to keep. It adds the most important one, based on the machine learning model derived important. And then, it makes an assessment as to whether that feature should be kept or not. And then it moves to the next feature.\n",
    "\n",
    "This method is therefore faster than wrapper methods and generally better than embedded methods. In practice it works extremely well. It does also account for correlations (depending on how stringent you set the arbitrary performance drop threshold). On the downside, the increase in performance assessed to decide whether the feature should be kept or removed, is set arbitrarily. The smaller the increase the more features will be selected, and vice versa.\n",
    "\n",
    "**Note** For the demonstration, XGBoost model is built, however this method is useful for any machine learning algorithm. In fact, the importance of the features are determined specifically for the algorithm used. Therefore, different algorithms may return different subsets of important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For classification 1/2 - After this function, you should run following function \n",
    "def hybrid_recursive_addition_1(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Rank the features according to their importance derived from a machine learning algorithm: \n",
    "    it can be tree importance, or LASSO/Ridge, or the linear/logistic regression coefficients.\n",
    "        Parameters:\n",
    "            X_train (DataFrame): X_train set\n",
    "            X_test (DataFrame): X_test set\n",
    "            y_train (DataFrame): y_train set \n",
    "            y_test (DataFrame): y_test set\n",
    "        Returns:\n",
    "            features (Series): Features ranked according to their importance derived from a \n",
    "                               machine learning algorithm.\n",
    "            plot (Plot): Plot the importance of each feature, which is ranked from the \n",
    "                         most to the least important in bar graph. \n",
    "    \"\"\"\n",
    "    print(f'Before applying hybrid recursive feature addition technique: X_train : {X_train.shape}, X_test: {X_test.shape}')\n",
    "\n",
    "    # Build a machine learning algorithm using all the available features\n",
    "    # and then determine the importance of the features according to the algorithm\n",
    "\n",
    "    # set the seed for reproducibility\n",
    "    seed_val = 1000000000\n",
    "    np.random.seed(seed_val)\n",
    "\n",
    "    # build initial model using all the features\n",
    "    model_all_features = xgb.XGBClassifier(nthread=10, max_depth=4, n_estimators=500, learning_rate=0.05)\n",
    "\n",
    "    model_all_features.fit(X_train, y_train)\n",
    "\n",
    "    # calculate the roc-auc in the test set\n",
    "    y_pred_test = model_all_features.predict_proba(X_test)[:, 1]\n",
    "    auc_score_all = roc_auc_score(y_test, y_pred_test)\n",
    "    print('Test all features xgb ROC AUC=%f' % (auc_score_all))\n",
    "\n",
    "    # Derive the importance of each feature and ranking them from the most to the least important\n",
    "    # Get feature name and importance\n",
    "    features = pd.Series(model_all_features.feature_importances_)\n",
    "    features.index = X_train.columns\n",
    "\n",
    "    # Sort the features by importance\n",
    "    features.sort_values(ascending=False, inplace=True)\n",
    "\n",
    "    # plot\n",
    "    plot = features.plot.bar(figsize=(20,6))\n",
    "    \n",
    "    return features, plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For classification continue - 2/2\n",
    "def hybrid_recursive_addition_2(features, threshold=0.0005):\n",
    "    \"\"\"\n",
    "    1) Build a machine learning model with only 1 feature, the most important one, \n",
    "       and calculate the model metric for performance.\n",
    "    2) Add one feature -the most important- and build a machine learning algorithm utilising \n",
    "       the added and any feature from previous rounds.\n",
    "    3) Calculate a performance metric of your choice: roc-auc, mse, rmse, accuracy.\n",
    "    4) If the metric increases by more than an arbitrarily set threshold, \n",
    "       then that feature is important and should be kept. Otherwise, we can remove that feature.\n",
    "    5) Repeat steps 1-4 until all features have been removed (and therefore evaluated) \n",
    "       and the drop in performance assessed.\n",
    "            Parameters:\n",
    "                features (series): Features ranked according to their importance derived from a \n",
    "                                   machine learning algorithm. It is output of previous function.\n",
    "                threshold (float): Threshold for the drop in roc-auc. The default value is 0.0005.\n",
    "            Returns:\n",
    "                features_to_keep (list): Important features derived from hybrid recursive feature selection. \n",
    "    \"\"\"  \n",
    "    # Convert ordered features into list\n",
    "    features_list = list(features.index) \n",
    "    \n",
    "    # Build a machine learning algorithm using only the most important feature\n",
    "\n",
    "    # Set the seed for reproducibility\n",
    "    seed_val = 1000000000\n",
    "    np.random.seed(seed_val)\n",
    "\n",
    "    # Build initial model using all the features\n",
    "    model_one_feature = xgb.XGBClassifier(nthread=10, max_depth=4, n_estimators=500, learning_rate=0.05)\n",
    "\n",
    "    # Train using only the most important feature\n",
    "    model_one_feature.fit(X_train[features[0]].to_frame(), y_train)\n",
    "\n",
    "    # Calculate the roc-auc in the test set\n",
    "    y_pred_test = model_one_feature.predict_proba(X_test[features[0]].to_frame())[:, 1]\n",
    "    auc_score_first = roc_auc_score(y_test, y_pred_test)\n",
    "    print('Test one feature xgb ROC AUC=%f' % (auc_score_first))\n",
    "    \n",
    "    # Adding one at a time all the features, from the most to the least important, and build an xgboost at each round.\n",
    "    # Once we build the model, we calculate the new roc-auc. if the new roc-auc is bigger than the original one\n",
    "    # (with one feature), then that feature that was added was important, and we should keep it.\n",
    "    # Otherwise, we should remove the feature\n",
    "\n",
    "    # recursive feature addition:\n",
    "\n",
    "    # Arbitrarily set the increase in roc-auc\n",
    "    # if the increase is above this threshold, the feature will be kept.\n",
    "    tol = 0.001\n",
    "\n",
    "    print('Doing recursive feature addition')\n",
    "\n",
    "    # Initialise a list where we will collect the features we should keep\n",
    "    features_to_keep = [features[0]]\n",
    "\n",
    "    # Set a counter to know how far ahead the loop is going\n",
    "    count = 1\n",
    "\n",
    "    # Loop over all the features, in order of importance:\n",
    "    # Remember that features is the list of ordered features by importance\n",
    "    for feature in features[1:]:\n",
    "        print()\n",
    "        print('testing feature: ', feature, ' which is feature ', count,\n",
    "              ' out of ', len(features))\n",
    "        count = count + 1\n",
    "\n",
    "        # Initialise model\n",
    "        model_int = xgb.XGBClassifier(nthread=10, max_depth=4, n_estimators=500, learning_rate=0.05)\n",
    "\n",
    "        # Fit model with the selected features and the feature to be evaluated\n",
    "        model_int.fit(X_train[features_to_keep + [feature] ], y_train)\n",
    "\n",
    "        # Make a prediction over the test set\n",
    "        y_pred_test = model_int.predict_proba(X_test[features_to_keep + [feature] ])[:, 1]\n",
    "\n",
    "        # Calculate the new roc-auc\n",
    "        auc_score_int = roc_auc_score(y_test, y_pred_test)\n",
    "        print('New Test ROC AUC={}'.format((auc_score_int)))\n",
    "\n",
    "        # Print the original roc-auc with one feature\n",
    "        print('All features Test ROC AUC={}'.format((auc_score_first)))\n",
    "\n",
    "        # Determine the increase in the roc-auc\n",
    "        diff_auc = auc_score_int - auc_score_first\n",
    "\n",
    "        # Compare the increase in roc-auc with the tolerance which is set previously\n",
    "        if diff_auc >= tol:\n",
    "            print('Increase in ROC AUC={}'.format(diff_auc))\n",
    "            print('keep: ', feature)\n",
    "            print\n",
    "            # if the increase in the roc is bigger than the threshold\n",
    "            # we keep the feature and re-adjust the roc-auc to the new value\n",
    "            # considering the added feature\n",
    "            auc_score_first = auc_score_int\n",
    "\n",
    "            # and we append the feature to keep to the list\n",
    "            features_to_keep.append(feature)\n",
    "        else:\n",
    "            # we ignore the feature\n",
    "            print('Increase in ROC AUC={}'.format(diff_auc))\n",
    "            print('remove: ', feature)\n",
    "            print\n",
    "\n",
    "    # Loop is finished, all the features are evaluated. \n",
    "    print('DONE!!')\n",
    "    print('total features to keep: ', len(features_to_keep))\n",
    "\n",
    "    return features_to_keep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice you may need to run a few runs of these method and find the right threshold, depending on how many features you are willing to include in your model and how accurate you would like it to be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For regression 1/2 - After this function, you should run following function \n",
    "def hybrid_recursive_addition_1(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Rank the features according to their importance derived from a machine learning algorithm: \n",
    "    it can be tree importance, or LASSO/Ridge, or the linear/logistic regression coefficients.\n",
    "        Parameters:\n",
    "            X_train (DataFrame): X_train set\n",
    "            X_test (DataFrame): X_test set\n",
    "            y_train (DataFrame): y_train set \n",
    "            y_test (DataFrame): y_test set\n",
    "        Returns:\n",
    "            features (Series): Features ranked according to their importance derived from a \n",
    "                               machine learning algorithm.\n",
    "            plot (Plot): Plot the importance of each feature, which is ranked from the \n",
    "                         most to the least important in bar graph. \n",
    "    \"\"\"\n",
    "    print(f'Before applying hybrid recursive feature addition technique: X_train : {X_train.shape}, X_test: {X_test.shape}')\n",
    "\n",
    "    # Build a machine learning algorithm using all the available features\n",
    "    # and then determine the importance of the features according to the algorithm\n",
    "\n",
    "    # set the seed for reproducibility\n",
    "    seed_val = 1000000000\n",
    "    np.random.seed(seed_val)\n",
    "\n",
    "    # build initial model using all the features\n",
    "    model_all_features = xgb.XGBRegressor(nthread=10, max_depth=4, n_estimators=500, learning_rate=0.05)\n",
    "\n",
    "    model_all_features.fit(X_train, y_train)\n",
    "\n",
    "    # calculate the r2 in the test set\n",
    "    y_pred_test = model_all_features.predict(X_test)\n",
    "    r2_score_all = r2_score(y_test, y_pred_test)\n",
    "    print('Test all features xgb R2 = %f' % (r2_score_all))\n",
    "\n",
    "    # Derive the importance of each feature and ranking them from the most to the least important\n",
    "    # Get feature name and importance\n",
    "    features = pd.Series(model_all_features.feature_importances_)\n",
    "    features.index = X_train.columns\n",
    "\n",
    "    # Sort the features by importance\n",
    "    features.sort_values(ascending=False, inplace=True)\n",
    "\n",
    "    # plot\n",
    "    plot = features.plot.bar(figsize=(20,6))\n",
    "    \n",
    "    return features, plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For regression continue - 2/2\n",
    "def hybrid_recursive_addition_2(features, threshold=0.001):\n",
    "    \"\"\"\n",
    "    1) Build a machine learning model with only 1 feature, the most important one, \n",
    "       and calculate the model metric for performance.\n",
    "    2) Add one feature -the most important- and build a machine learning algorithm utilising \n",
    "       the added and any feature from previous rounds.\n",
    "    3) Calculate a performance metric of your choice: roc-auc, mse, rmse, accuracy.\n",
    "    4) If the metric increases by more than an arbitrarily set threshold, \n",
    "       then that feature is important and should be kept. Otherwise, we can remove that feature.\n",
    "    5) Repeat steps 1-4 until all features have been removed (and therefore evaluated) \n",
    "       and the drop in performance assessed.\n",
    "            Parameters:\n",
    "                features (series): Features ranked according to their importance derived from a \n",
    "                                   machine learning algorithm. It is output of previous function.\n",
    "                threshold (float): Threshold for the drop in roc-auc. The default value is 0.001.\n",
    "            Returns:\n",
    "                features_to_keep (list): Important features derived from hybrid recursive feature selection. \n",
    "    \"\"\"  \n",
    "    # Convert ordered features into list\n",
    "    features_list = list(features.index) \n",
    "    \n",
    "    # Build a machine learning algorithm using only the most important feature\n",
    "\n",
    "    # Set the seed for reproducibility\n",
    "    seed_val = 1000000000\n",
    "    np.random.seed(seed_val)\n",
    "\n",
    "    # Build initial model using all the features\n",
    "    model_one_feature = xgb.XGBRegressor(\n",
    "        nthread=10, max_depth=4, n_estimators=500, learning_rate=0.05)\n",
    "\n",
    "    # Train using only the most important feature\n",
    "    model_one_feature.fit(X_train[features[0]].to_frame(), y_train)\n",
    "\n",
    "    # Calculate the roc-auc in the test set\n",
    "    y_pred_test = model_one_feature.predict(X_test[features[0]].to_frame())\n",
    "    r2_score_first = r2_score(y_test, y_pred_test)\n",
    "    print('Test one feature xgb R2=%f' % (r2_score_first))\n",
    "    \n",
    "    # Adding one at a time all the features, from the most to the least important, and build an xgboost at each round.\n",
    "    # Once we build the model, we calculate the new r2\n",
    "    # if the new r2 is bigger than the original one (with one feature), \n",
    "    # then that feature that was added was important, and we should keep it.\n",
    "    # Otherwise, we should remove the feature.\n",
    "\n",
    "    # recursive feature addition:\n",
    "\n",
    "    # Arbitrarily set the increase in r2\n",
    "    # if the increase is above this threshold, the feature will be kept\n",
    "    tol = threshold\n",
    "\n",
    "    print('doing recursive feature addition')\n",
    "\n",
    "    # Initialise a list where we will collect the features we should keep\n",
    "    features_to_keep = [features[0]]\n",
    "\n",
    "    # Set a counter to know how far ahead the loop is going\n",
    "    count = 1\n",
    "\n",
    "    # Loop over all the features, in order of importance:\n",
    "    # Remember that features is the list of ordered features by importance\n",
    "    for feature in features[1:]:\n",
    "        print()\n",
    "        print('testing feature: ', feature, ' which is feature ', count,\n",
    "              ' out of ', len(features))\n",
    "        count = count + 1\n",
    "\n",
    "        # Initialise model\n",
    "        model_int = xgb.XGBRegressor(nthread=10, max_depth=4, n_estimators=500, learning_rate=0.05)\n",
    "\n",
    "        # Fit model with the selected features and the feature to be evaluated\n",
    "        model_int.fit(X_train[features_to_keep + [feature] ], y_train)\n",
    "\n",
    "        # Make a prediction over the test set\n",
    "        y_pred_test = model_int.predict(X_test[features_to_keep + [feature] ])\n",
    "\n",
    "        # Calculate the new r2\n",
    "        r2_score_int = r2_score(y_test, y_pred_test)\n",
    "        print('New Test R2={}'.format((r2_score_int)))\n",
    "\n",
    "        # print the original r2 with all the features\n",
    "        print('All features Test R2={}'.format((r2_score_first)))\n",
    "\n",
    "        # Determine the drop in the roc-auc\n",
    "        diff_r2 = r2_score_int - r2_score_first\n",
    "\n",
    "        # Compare the increase in r2 with the tolerance which was set previously\n",
    "        if diff_r2 >= tol:\n",
    "            print('Increase in r2 = {}'.format(diff_r2))\n",
    "            print('keep: ', feature)\n",
    "            print\n",
    "            # If the increase in the r2 is bigger than the threshold\n",
    "            # we keep the feature and re-adjust the r2 to the new value considering the added feature\n",
    "            auc_score_first = auc_score_int\n",
    "\n",
    "            # Append the feature to keep to the list\n",
    "            features_to_keep.append(feature)\n",
    "        else:\n",
    "            # Ignore the feature\n",
    "            print('Increase in r2 = {}'.format(diff_r2))\n",
    "            print('remove: ', feature)\n",
    "            print\n",
    "\n",
    "    # loop is finished, all the features were evaluated.\n",
    "    print('DONE!!')\n",
    "    print('total features to keep: ', len(features_to_keep))\n",
    "    \n",
    "    return features_to_keep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we increase the threshold here, we should be able to reduce the number of features a bit further and potentially increase the performance of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. (Additional) Combining Feature Selectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_selectors(df, target_var, size=0.3, n=2):\n",
    "    \"\"\"\n",
    "    Combine diffrent set of algorithms's vote for feature importance and select the features \n",
    "    which have more votes than defined threshold.\n",
    "        Parameters:\n",
    "            df (DataFrame): Any dataframe\n",
    "            target_var (str): Target variable of the dataset.\n",
    "            size (float): Test size in train/test split. The default value is 0.3. \n",
    "            n (int): if we want to make sure we don't lose any information,\n",
    "                     we could select all features with at least one vote.\n",
    "                     For default, we chose to have at least two models voting for\n",
    "                     a feature in order to keep it. \n",
    "        Returns:\n",
    "            reduced_X (DataFrame): New dataframe after combining selectors and removing redundant features. \n",
    "            selected_feat (list): The list of selected features. \n",
    "    \"\"\"\n",
    "    # separate dataset into train and test\n",
    "    X = df.drop(labels=[target_var], axis=1)\n",
    "    y = df[target_var]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= size, random_state=0)\n",
    "    \n",
    "    # Data Shape\n",
    "    print(f'Before applying feature selection: X_train : {X_train.shape}, X_test: {X_test.shape}')\n",
    "    \n",
    "    # Feature selection with LassoCV \n",
    "    # The LassoCV class will use cross validation to try out different alpha \n",
    "    # settings and select the best one. \n",
    "    lcv = LassoCV()\n",
    "    lcv.fit(X_train, y_train)\n",
    "    lcv.score(X_test, y_test)\n",
    "    lcv_mask = lcv.coef_ != 0\n",
    "    features_to_keep = sum(lcv_mask) \n",
    "\n",
    "    # Feature selection with random forest\n",
    "    rfe_rf = RFE(estimator=RandomForestRegressor(), n_features_to_select=features_to_keep, step=5, verbose=1)\n",
    "    rfe_rf.fit(X_train, y_train)\n",
    "    rf_mask = rfe_rf.support_\n",
    "\n",
    "    # Feature selection with gradient boosting\n",
    "    rfe_gb = RFE(estimator=GradientBoostingRegressor(),\n",
    "    n_features_to_select=features_to_keep, step=5, verbose=1)\n",
    "    rfe_gb.fit(X_train, y_train)\n",
    "    gb_mask = rfe_gb.support_\n",
    "\n",
    "    # Combining the feature selectors\n",
    "    votes = np.sum([lcv_mask, rf_mask, gb_mask], axis=0)\n",
    "\n",
    "    # Masking by voting threshold\n",
    "    mask = votes >= n  \n",
    "    \n",
    "    X = df.\n",
    "    reduced_X = X.loc[:, mask]\n",
    "    \n",
    "    selected_feat = reduced_X.columns.tolist()\n",
    "    \n",
    "    return reduced_X, selected_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plug the reduced dataset into a linear regression pipeline\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.3, random_state=0)\n",
    "lm.fit(scaler.fit_transform(X_train), y_train)\n",
    "r_squared = lm.score(scaler.transform(X_test), y_test)\n",
    "print('The model can explain {0:.1%} of the variance in the test set using {1:} features.'.format(r_squared, len(lm.coef_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction - Principal Component Analysis (PCA) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating new features based on the existing ones while trying to lose as little information as possible. It creates news features, which are in fact combinations of the original ones.\n",
    "\n",
    "PCA:\n",
    "\n",
    "For this technique, it is important to scale the features first, so that their values are easier to compare.\n",
    "\n",
    "We can add a reference point to the very center of the point cloud, and then point a vector in the direction of this strongest pattern. We can add a second vector perpendicular to the first one to account for the rest of the variance in this dataset.\n",
    "\n",
    "Every point in the dataset could be described by multiplying and then summing two perpendicular vectors. We essentially created a new reference system aligned with the variance in the data. The coordinates that each point has in this new reference system are called principal components, and they are the foundation of principal component analysis (PCA).\n",
    "\n",
    "Principals share no duplicate information and that they are ranked from most to least important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Principal Component Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the principal components\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "std_df = scaler.fit_transform(df)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "print(pca.fit_transform(std_df))\n",
    "\n",
    "# Principal component explained variance ratio\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "pca.fit(std_df)\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "# PCA for dimensionality reduction\n",
    "pca = PCA()\n",
    "pca.fit(ansur_std_df)\n",
    "print(pca.explained_variance_ratio_.cumsum())\n",
    "\n",
    "# Understanding the components\n",
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pairplot to inspect ansur_df\n",
    "sns.pairplot(ansur_df)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create the scaler and standardize the data\n",
    "scaler = StandardScaler()\n",
    "ansur_std = scaler.fit_transform(ansur_df)\n",
    "\n",
    "# Create the PCA instance and fit and transform the data with pca\n",
    "pca = PCA()\n",
    "pc = pca.fit_transform(ansur_std)\n",
    "\n",
    "# This changes the numpy array output back to a dataframe\n",
    "pc_df = pd.DataFrame(pc, columns=['PC 1', 'PC 2', 'PC 3', 'PC 4'])\n",
    "\n",
    "# Create a pairplot of the principal component dataframe\n",
    "sns.pairplot(pc_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA on a larger dataset\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "ansur_std = scaler.fit_transform(ansur_df)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "pca.fit(ansur_std)\n",
    "\n",
    "# Inspect the explained variance ratio per component\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "# Print the cumulative sum of the explained variance ratio\n",
    "print(pca.explained_variance_ratio_.cumsum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PCA Applications**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding the components\n",
    "print(pca.components_)\n",
    "\n",
    "# PCA in a pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('reducer', PCA())])\n",
    "pc = pipe.fit_transform(ansur_df)\n",
    "print(pc[:,:2])\n",
    "\n",
    "# Checking the effect of categorical features\n",
    "ansur_categories['PC 1'] = pc[:,0]\n",
    "ansur_categories['PC 2'] = pc[:,1]\n",
    "sns.scatterplot(data=ansur_categories, x='PC 1', \n",
    "                y='PC 2',hue='Height_class', alpha=0.4)\n",
    "\n",
    "# PCA in a model pipeline\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('reducer', PCA(n_components=3)),\n",
    "    ('classifier', RandomForestClassifier())])\n",
    "pipe.fit(X_train, y_train)\n",
    "print(pipe.steps[1])\n",
    "\n",
    "# PCA in a model pipeline\n",
    "pipe.steps[1][1].explained_variance_ratio_.cumsum()\n",
    "print(pipe.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the pipeline\n",
    "pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                 ('reducer', PCA(n_components=2))])\n",
    "\n",
    "# Fit it to the dataset and extract the component vectors\n",
    "pipe.fit(poke_df)\n",
    "vectors = pipe.steps[1][1].components_.round(2)\n",
    "\n",
    "# Print feature effects\n",
    "print('PC 1 effects = ' + str(dict(zip(poke_df.columns, vectors[0]))))\n",
    "print('PC 2 effects = ' + str(dict(zip(poke_df.columns, vectors[1]))))\n",
    "\n",
    "# PCA for feature exploration\n",
    "# Build the pipeline\n",
    "pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                 ('reducer', PCA(n_components=2))])\n",
    "\n",
    "# Fit the pipeline to poke_df and transform the data\n",
    "pc = pipe.fit_transform(poke_df)\n",
    "\n",
    "print(pc)\n",
    "\n",
    "# Add the 2 components to poke_cat_df\n",
    "poke_cat_df['PC 1'] = pc[:, 0]\n",
    "poke_cat_df['PC 2'] = pc[:, 1]\n",
    "\n",
    "print(poke_cat_df.head())\n",
    "\n",
    "# Use the Type feature to color the PC 1 vs PC 2 scatterplot\n",
    "sns.scatterplot(data=poke_cat_df, \n",
    "                x='PC 1', y='PC 2', hue='Type')\n",
    "plt.show()\n",
    "\n",
    "### PCA in a model pipeline\n",
    "\n",
    "# Build the pipeline\n",
    "pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('reducer', PCA(n_components=2)),\n",
    "        ('classifier', RandomForestClassifier(random_state=0))])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Prints the explained variance ratio\n",
    "print(pipe.steps[1][1].explained_variance_ratio_)\n",
    "\n",
    "# Score the accuracy on the test set\n",
    "accuracy = pipe.score(X_test, y_test)\n",
    "\n",
    "# Prints the model accuracy\n",
    "print('{0:.1%} test set accuracy'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Principal Component Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting an explained variance threshold\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('reducer', PCA(n_components=0.9))])\n",
    "\n",
    "# Fit the pipe to the data\n",
    "pipe.fit(poke_df)\n",
    "print(len(pipe.steps[1][1].components_))\n",
    "\n",
    "# An optimal number of components --- > 'Elbow' in the plot\n",
    "pipe.fit(poke_df)\n",
    "var = pipe.steps[1][1].explained_variance_ratio_\n",
    "plt.plot(var)\n",
    "plt.xlabel('Principal component index')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.show()\n",
    "\n",
    "# Compressing images\n",
    "pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                 ('reducer', PCA(n_components=290))])\n",
    "pipe.fit(X_train)\n",
    "pc = pipe.fit_transform(X_test)\n",
    "print(pc.shape)\n",
    "\n",
    "# Rebuilding images\n",
    "pc = pipe.transform(X_test)\n",
    "print(pc.shape)\n",
    "\n",
    "X_rebuilt = pipe.inverse_transform(pc)\n",
    "print(X_rebuilt.shape)\n",
    "\n",
    "img_plotter(X_rebuilt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the proportion of variance to keep\n",
    "# Pipe a scaler to PCA selecting 80% of the variance\n",
    "pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                 ('reducer', PCA(n_components=0.8))])\n",
    "\n",
    "# Fit the pipe to the data\n",
    "pipe.fit(ansur_df)\n",
    "\n",
    "print('{} components selected'.format(len(pipe.steps[1][1].components_)))\n",
    "\n",
    "\n",
    "## Choosing the number of components ##\n",
    "\n",
    "# Pipeline a scaler and pca selecting 10 components\n",
    "pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                 ('reducer', PCA(n_components=10))])\n",
    "\n",
    "# Fit the pipe to the data\n",
    "pipe.fit(ansur_df)\n",
    "\n",
    "# Plot the explained variance ratio\n",
    "plt.plot(pipe.steps[1][1].explained_variance_ratio_)\n",
    "\n",
    "plt.xlabel('Principal component index')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.show()\n",
    "\n",
    "## PCA for image compression ## \n",
    "\n",
    "# Plot the MNIST sample\n",
    "plot_digits(X_test)\n",
    "\n",
    "\n",
    "# Transform the input data to principal components\n",
    "pc = pipe.transform(X_test)\n",
    "\n",
    "# Prints the number of features per dataset\n",
    "print(\"X_test has {} features\".format(X_test.shape[1]))\n",
    "print(\"pc has {} features\".format(pc.shape[1]))\n",
    "\n",
    "# Inverse transform the components to original feature space\n",
    "X_rebuilt = pipe.inverse_transform(pc)\n",
    "\n",
    "# Prints the number of features\n",
    "print(\"X_rebuilt has {} features\".format(X_rebuilt.shape[1]))\n",
    "\n",
    "\n",
    "# Transform the input data to principal components\n",
    "pc = pipe.transform(X_test)\n",
    "\n",
    "# Inverse transform the components to original feature space\n",
    "X_rebuilt = pipe.inverse_transform(pc)\n",
    "\n",
    "# Plot the reconstructed data\n",
    "plot_digits(X_rebuilt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

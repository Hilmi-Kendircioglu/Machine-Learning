{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring High Dimensional Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For nun-numeric data\n",
    "df.describe(exclude='number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('feature', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seaborn's pairplot is excellent to visually explore small to medium sized dataset\n",
    "sns.pairplot(df, hue='gender', diag_kind='hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**t-SNE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-Distributed Stochastic Neighbor Embedding or t-SNE\n",
    "# It is a powerful technique to visualize high dimensional data using feature extraction\n",
    "# t-SNE will maximize the distance in two-dimensional space between observations that are most different in a high-dimensional space. \n",
    "# Because of this, observations that are similar will be close to one another and may become clustered. \n",
    "# Not work with non-numeric data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-numerical columns in the dataset\n",
    "non_numeric = ['BMI_class','Height_class','Gender','Component','Branch']\n",
    "df_numeric = df.drop(non_numeric, axis=1)\n",
    "\n",
    "# Fitting t-SNE --> This project hight dimensional dataset onto a Numpy array with two dimensions.\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "# Create a t-SNE model with learning rate 50\n",
    "m = TSNE(learning_rate=50)\n",
    "\n",
    "# Fit and transform the t-SNE model on the numeric dataset\n",
    "tsne_features = m.fit_transform(df_numeric)\n",
    "print(tsne_features.shape)\n",
    "\n",
    "# Assign t-SNE features to the dataset\n",
    "df['x'] = tsne_features[:,0]\n",
    "df['y'] = tsne_features[:,1]\n",
    "\n",
    "# Plot t-SNE\n",
    "import seaborn as sns\n",
    "sns.scatterplot(x=\"x\", y=\"y\", data=df)\n",
    "plt.show()\n",
    "\n",
    "# Coloring points according to BMI category --> hue is categorical feature \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.scatterplot(x=\"x\", y=\"y\", hue='BMI_class', data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection I, selecting for feature information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split()\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Select the Gender column as the feature to be predicted (y)\n",
    "y = ansur_df['Gender']\n",
    "\n",
    "# Remove the Gender column to create the training data\n",
    "X = ansur_df.drop('Gender', axis=1)\n",
    "\n",
    "# Perform a 70% train and 30% test data split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "print(\"{} rows in test set vs. {} in training set. {} Features.\".format(X_test.shape[0], X_train.shape[0], X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SVC from sklearn.svm and accuracy_score from sklearn.metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create an instance of the Support Vector Classification class\n",
    "svc = SVC()\n",
    "\n",
    "# Fit the model to the training data\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# Calculate accuracy scores on both train and test data\n",
    "accuracy_train = accuracy_score(y_train, svc.predict(X_train))\n",
    "accuracy_test = accuracy_score(y_test, svc.predict(X_test))\n",
    "\n",
    "print(\"{0:.1%} accuracy on test set vs. {1:.1%} on training set\".format(accuracy_test, accuracy_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy after dimensionality reduction\n",
    "\n",
    "You'll reduce the overfit with the help of dimensionality reduction. In this case, you'll apply a rather drastic form of dimensionality reduction by only selecting a single column that has some good information to distinguish between genders. You'll repeat the train-test split, model fit and prediction steps to compare the accuracy on test vs. training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign just the 'neckcircumferencebase' column from ansur_df to X\n",
    "X = ansur_df[['neckcircumferencebase']]\n",
    "\n",
    "# Split the data, instantiate a classifier and fit the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "svc = SVC()\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# Calculate accuracy scores on both train and test data\n",
    "accuracy_train = accuracy_score(y_train, svc.predict(X_train))\n",
    "accuracy_test = accuracy_score(y_test, svc.predict(X_test))\n",
    "\n",
    "print(\"{0:.1%} accuracy on test set vs. {1:.1%} on training set\".format(accuracy_test, accuracy_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features with little variance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features with little variance \n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "sel = VarianceThreshold(threshold=1)\n",
    "sel.fit(df)\n",
    "mask = sel.get_support()\n",
    "print(mask)\n",
    "\n",
    "reduced_df = df.loc[:, mask]\n",
    "print(df.shape)\n",
    "\n",
    "# Create a box plot\n",
    "df.boxplot()\n",
    "plt.show()\n",
    "\n",
    "# Finding a good variance threshold\n",
    "# Normalize the data\n",
    "normalized_df = head_df / head_df.mean()\n",
    "normalized_df.boxplot()\n",
    "plt.show()\n",
    "# Print the variances of the normalized data\n",
    "print(normalized_df.var())\n",
    "# Note: Pick a value that is lower than the lowest value of \n",
    "# a feature you want to keep and higher than the highest value \n",
    "# for a feature you want to remove.\n",
    "\n",
    "\n",
    "#Normalizing the variance before feature selection\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Create a VarianceThreshold feature selector\n",
    "sel = VarianceThreshold(threshold=0.001)\n",
    "\n",
    "# Fit the selector to normalized head_df\n",
    "sel.fit(head_df / head_df.mean())\n",
    "\n",
    "# Create a boolean mask\n",
    "mask = sel.get_support()\n",
    "\n",
    "# Apply the mask to create a reduced dataframe\n",
    "reduced_df = head_df.loc[:, mask]\n",
    "\n",
    "print(\"Dimensionality reduced from {} to {}.\".format(head_df.shape[1], reduced_df.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features with missing values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features with missing values\n",
    "\n",
    "# Counting missing values\n",
    "pokemon_df.isna().sum() / len(pokemon_df)\n",
    "\n",
    "# Applying a missing value threshold\n",
    "\n",
    "# Fewer than 30% missing values = True value\n",
    "mask = pokemon_df.isna().sum() / len(pokemon_df) < 0.3\n",
    "print(mask)\n",
    "\n",
    "reduced_df = pokemon_df.loc[:, mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pairwise Correlation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "df.corr\n",
    "\n",
    "# Visualizing the correlation matrix\n",
    "cmap = sns.diverging_palette(h_neg=10,h_pos=240,as_cmap=True)\n",
    "sns.heatmap(weights_df.corr(), center=0, cmap=cmap, \n",
    "            linewidths=1, annot=True, fmt=\".2f\")\n",
    "\n",
    "\n",
    "# Removing duplicates\n",
    "\n",
    "# Create the correlation matrix\n",
    "corr = ansur_df.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle \n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Add the mask to the heatmap\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, center=0, linewidths=1, annot=True, fmt=\".2f\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing highly correlated features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix and take the absolute value\n",
    "corr_matrix = ansur_df.corr().abs()\n",
    "\n",
    "# Create a True/False mask and apply it\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "tri_df = corr_matrix.mask(mask)\n",
    "\n",
    "# List column names of highly correlated features (r > 0.95)\n",
    "to_drop = [c for c in tri_df.columns if any(tri_df[c] > 0.95)]\n",
    "\n",
    "# Drop the features in the to_drop list\n",
    "reduced_df = ansur_df.drop(to_drop, axis=1)\n",
    "\n",
    "print(\"The reduced_df dataframe has {} columns\".format(reduced_df.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Selection II, selecting for model accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Selecting features for model performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "\n",
    "# Creating a logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_std, y_train)\n",
    "X_test_std = scaler.transform(X_test)\n",
    "y_pred = lr.predict(X_test_std)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Inspecting the feature coefcients\n",
    "print(lr.coef_)\n",
    "\n",
    "print(dict(zip(X.columns, abs(lr.coef_[0]))))\n",
    "\n",
    "# Features that contribute little to a model\n",
    "X.drop('handlength', axis=1, inplace=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "lr.fit(scaler.fit_transform(X_train), y_train)\n",
    "print(accuracy_score(y_test, lr.predict(scaler.transform(X_test))))\n",
    "\n",
    "# Recursive Feature Elimination\n",
    "from sklearn.feature_selection import RFE\n",
    "rfe = RFE(estimator=LogisticRegression(), n_features_to_select=2, verbose=1)\n",
    "rfe.fit(X_train_std, y_train)\n",
    "\n",
    "# Inspecting the RFE results\n",
    "X.columns[rfe.support_]\n",
    "\n",
    "print(dict(zip(X.columns, rfe.ranking_)))\n",
    "print(accuracy_score(y_test, rfe.predict(X_test_std)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build a diabetes classifier #####\n",
    "\n",
    "# Fit the scaler on the training features and transform these in one go\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "\n",
    "# Fit the logistic regression model on the scaled training data\n",
    "lr.fit(X_train_std, y_train)\n",
    "\n",
    "# Scale the test features\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "# Predict diabetes presence on the scaled test set\n",
    "y_pred = lr.predict(X_test_std)\n",
    "\n",
    "# Prints accuracy metrics and feature coefficients\n",
    "print(\"{0:.1%} accuracy on test set.\".format(accuracy_score(y_test, y_pred))) \n",
    "print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))\n",
    "\n",
    "\n",
    "### Manual Recursive Feature Elimination ###\n",
    "\n",
    "# Remove the feature with the lowest model coefficient\n",
    "X = diabetes_df[['pregnant', 'glucose', 'diastolic', 'triceps', 'insulin', 'bmi', 'family', 'age']]\n",
    "\n",
    "# Performs a 25-75% train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# Scales features and fits the logistic regression model\n",
    "lr.fit(scaler.fit_transform(X_train), y_train)\n",
    "\n",
    "# Calculates the accuracy on the test set and prints coefficients\n",
    "acc = accuracy_score(y_test, lr.predict(scaler.transform(X_test)))\n",
    "print(\"{0:.1%} accuracy on test set.\".format(acc)) \n",
    "print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))\n",
    "\n",
    "### Automatic Recursive Feature Elimination ###\n",
    "\n",
    "# Create the RFE with a LogisticRegression estimator and 3 features to select\n",
    "rfe = RFE(estimator=LogisticRegression(), n_features_to_select=3, verbose=1)\n",
    "\n",
    "# Fit the eliminator to the data\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "# Print the features and their ranking (high = dropped early on)\n",
    "print(dict(zip(X.columns, rfe.ranking_)))\n",
    "\n",
    "# Print the features that are not eliminated\n",
    "print(X.columns[rfe.support_])\n",
    "\n",
    "# Calculates the test set accuracy\n",
    "acc = accuracy_score(y_test, rfe.predict(X_test))\n",
    "print(\"{0:.1%} accuracy on test set.\".format(acc)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tree-based Feature Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest classier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "print(accuracy_score(y_test, rf.predict(X_test)\n",
    "\n",
    "# Feature importance values              \n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "print(rf.feature_importances_)\n",
    "print(sum(rf.feature_importances_)) # ---> 1.0                     \n",
    "\n",
    "# Feature importance as a feature selector\n",
    "mask = rf.feature_importances_ > 0.1\n",
    "print(mask)\n",
    "                     \n",
    "X_reduced = X.loc[:, mask]\n",
    "print(X_reduced.columns)                     \n",
    "                     \n",
    "# RFE with random forests\n",
    "from sklearn.feature_selection import RFE\n",
    "rfe = RFE(estimator=RandomForestClassifier(),\n",
    "n_features_to_select=6, verbose=1)\n",
    "rfe.fit(X_train,y_train) \n",
    "                     \n",
    "print(accuracy_score(y_test, rfe.predict(X_test))\n",
    "                     \n",
    "print(X.columns[rfe.support_]                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Building a random forest model  ###\n",
    "\n",
    "# Perform a 75% training and 25% test data split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# Fit the random forest model to the training data\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the test set accuracy\n",
    "acc = accuracy_score(y_test, rf.predict(X_test))\n",
    "\n",
    "# Print the importances per feature\n",
    "print(dict(zip(X.columns, rf.feature_importances_.round(2))))\n",
    "\n",
    "# Print accuracy\n",
    "print(\"{0:.1%} accuracy on test set.\".format(acc)) \n",
    "\n",
    "### Random Forest for Feature Selection ###\n",
    "\n",
    "# Create a mask for features importances above the threshold\n",
    "mask = rf.feature_importances_ > 0.15\n",
    "\n",
    "# Prints out the mask\n",
    "print(mask)\n",
    "\n",
    "# Apply the mask to the feature dataset X\n",
    "reduced_X = X.loc[:, mask]\n",
    "\n",
    "# prints out the selected column names\n",
    "print(reduced_X.columns)\n",
    "\n",
    "### Recursive Feature Elimination with random forests  ####\n",
    "\n",
    "# Wrap the feature eliminator around the random forest model -- step parameter is important \n",
    "rfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=2, step=2, verbose=1)\n",
    "\n",
    "# Fit the model to the training data\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "# Create a mask using an attribute of rfe\n",
    "mask = rfe.support_\n",
    "\n",
    "# Apply the mask to the feature dataset X and print the result\n",
    "reduced_X = X.loc[:, mask]\n",
    "print(reduced_X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regularized Linear Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression in Python\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "# Actual coefficients = [5 2 0]\n",
    "print(lr.coef_)\n",
    "[ 4.95 1.83 -0.05]\n",
    "\n",
    "# Actual intercept = 20\n",
    "print(lr.intercept_)\n",
    "\n",
    "# Calculates R-squared\n",
    "print(lr.score(X_test, y_test))\n",
    "\n",
    "\n",
    "# Lasso regressor\n",
    "from sklearn.linear_model import Lasso\n",
    "la = Lasso()\n",
    "la.fit(X_train, y_train)\n",
    "# Actual coefficients = [5 2 0]\n",
    "print(la.coef_)\n",
    "\n",
    "print(la.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating a LASSO Regressor ###\n",
    "\n",
    "# Set the test size to 30% to get a 70-30% train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Fit the scaler on the training features and transform these in one go\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "\n",
    "# Create the Lasso model \n",
    "la = Lasso()\n",
    "\n",
    "# Fit it to the standardized training data\n",
    "la.fit(X_train_std, y_train)\n",
    "\n",
    "### Lasso Model Results  ###\n",
    "\n",
    "# Transform the test set with the pre-fitted scaler\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "# Calculate the coefficient of determination (R squared) on X_test_std\n",
    "r_squared = la.score(X_test_std, y_test)\n",
    "print(\"The model can predict {0:.1%} of the variance in the test set.\".format(r_squared))\n",
    "\n",
    "# Create a list that has True values when coefficients equal 0\n",
    "zero_coef = la.coef_ == 0\n",
    "\n",
    "# Calculate how many features have a zero coefficient\n",
    "n_ignored = sum(zero_coef)\n",
    "print(\"The model has ignored {} out of {} features.\".format(n_ignored, len(la.coef_)))\n",
    "\n",
    "\n",
    "### Adjusting the regularization strength ###\n",
    "\n",
    "# Your current Lasso model has an R2 score of 84.7%. When a model applies overly powerful regularization \n",
    "# it can suffer from high bias, hurting its predictive power.\n",
    "# Improve the balance between predictive power and model simplicity by tweaking the alpha parameter.\n",
    "\n",
    "# Find the highest alpha value with R-squared above 98%\n",
    "la = Lasso(alpha=0.1, random_state=0)\n",
    "\n",
    "# Fits the model and calculates performance stats\n",
    "la.fit(X_train_std, y_train)\n",
    "r_squared = la.score(X_test_std, y_test)\n",
    "n_ignored_features = sum(la.coef_ == 0)\n",
    "\n",
    "# Print peformance stats \n",
    "print(\"The model can predict {0:.1%} of the variance in the test set.\".format(r_squared))\n",
    "print(\"{} out of {} features were ignored.\".format(n_ignored_features, len(la.coef_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combining Feature Selectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso Regressor\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "la = Lasso(alpha=0.05) # Manually set alpha parameter to find a balance\n",
    "                       # between removing as much as features as possible\n",
    "                       # and model accuracy\n",
    "\n",
    "la.fit(X_train, y_train)\n",
    "# Actual coefficients = [5 2 0]\n",
    "print(la.coef_)\n",
    "\n",
    "print(la.score(X_test, y_test))\n",
    "\n",
    "# LassoCV Regressor ---> Find Optimal alpha value\n",
    "\n",
    "from sklearn.linear_model import LassoCV\n",
    "lcv = LassoCV()\n",
    "lcv.fit(X_train, y_train)\n",
    "print(lcv.alpha_)\n",
    "\n",
    "# LassoCV Regressor ---> Find O coefficients and reduce the feature by masking the dataset \n",
    "mask = lcv.coef_ != 0\n",
    "reduced_X = X.loc[:, mask]\n",
    "\n",
    "##### Use combination of models for feature selection  ######\n",
    "\n",
    "# Feature selection with LassoCV \n",
    "# The LassoCV class will use cross validation to try out different alpha \n",
    "# settings and select the best one. \n",
    "from sklearn.linear_model import LassoCV\n",
    "lcv = LassoCV()\n",
    "lcv.fit(X_train, y_train)\n",
    "lcv.score(X_test, y_test)\n",
    "lcv_mask = lcv.coef_ != 0\n",
    "sum(lcv_mask) # --- > return 66 features to keep\n",
    "\n",
    "# Feature selection with random forest\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rfe_rf = RFE(estimator=RandomForestRegressor(),\n",
    "n_features_to_select=66, step=5, verbose=1)\n",
    "rfe_rf.fit(X_train, y_train)\n",
    "rf_mask = rfe_rf.support_\n",
    "\n",
    "# Feature selection with gradient boosting\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "rfe_gb = RFE(estimator=GradientBoostingRegressor(),\n",
    "n_features_to_select=66, step=5, verbose=1)\n",
    "rfe_gb.fit(X_train, y_train)\n",
    "gb_mask = rfe_gb.support_\n",
    "\n",
    "# Combining the feature selectors\n",
    "import numpy as np\n",
    "votes = np.sum([lcv_mask, rf_mask, gb_mask], axis=0)\n",
    "print(votes)\n",
    "\n",
    "mask = votes >= 2 # if we want to make sure we don't lose any information,\n",
    "                  # we could select all features with at least one vote.\n",
    "                  # Here, we chose to have at least two models voting for\n",
    "                  # a feature in order to keep it. \n",
    "reduced_X = X.loc[:, mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Creating a LassoCV regressor ####\n",
    "\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# Create and fit the LassoCV model on the training set\n",
    "lcv = LassoCV()\n",
    "lcv.fit(X_train, y_train)\n",
    "print('Optimal alpha = {0:.3f}'.format(lcv.alpha_))\n",
    "\n",
    "# Calculate R squared on the test set\n",
    "r_squared = lcv.score(X_test, y_test)\n",
    "print('The model explains {0:.1%} of the test set variance'.format(r_squared))\n",
    "\n",
    "# Create a mask for coefficients not equal to zero\n",
    "lcv_mask = lcv.coef_ != 0\n",
    "print('{} features out of {} selected'.format(sum(lcv_mask), len(lcv_mask)))\n",
    "\n",
    "### Ensemble models for extra voters  ###\n",
    " \n",
    "# Gradient Boosting Regressor #\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Select 10 features with RFE on a GradientBoostingRegressor, drop 3 features on each step\n",
    "rfe_gb = RFE(estimator=GradientBoostingRegressor(), \n",
    "             n_features_to_select=10, step=3, verbose=1)\n",
    "rfe_gb.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the R squared on the test set\n",
    "r_squared = rfe_gb.score(X_test, y_test)\n",
    "print('The model can explain {0:.1%} of the variance in the test set'.format(r_squared))\n",
    "\n",
    "# Assign the support array to gb_mask\n",
    "gb_mask = rfe_gb.support_\n",
    "\n",
    "\n",
    "# Random Forest Regressor #\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Select 10 features with RFE on a RandomForestRegressor, drop 3 features on each step\n",
    "rfe_rf = RFE(estimator=RandomForestRegressor(), \n",
    "             n_features_to_select=10, step=3, verbose=1)\n",
    "rfe_rf.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the R squared on the test set\n",
    "r_squared = rfe_rf.score(X_test, y_test)\n",
    "print('The model can explain {0:.1%} of the variance in the test set'.format(r_squared))\n",
    "\n",
    "# Assign the support array to gb_mask\n",
    "rf_mask = rfe_rf.support_\n",
    "\n",
    "\n",
    "# Combining 3 feature selectors # \n",
    "\n",
    "# Sum the votes of the three models\n",
    "votes = np.sum([lcv_mask, rf_mask, gb_mask], axis=0)\n",
    "print(votes)\n",
    "\n",
    "# Create a mask for features selected by all 3 models\n",
    "meta_mask = votes >= 3\n",
    "print(meta_mask)\n",
    "\n",
    "# Apply the dimensionality reduction on X\n",
    "X_reduced = X.loc[:, meta_mask]\n",
    "print(X_reduced.columns)\n",
    "\n",
    "# Plug the reduced dataset into a linear regression pipeline\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.3, random_state=0)\n",
    "lm.fit(scaler.fit_transform(X_train), y_train)\n",
    "r_squared = lm.score(scaler.transform(X_test), y_test)\n",
    "print('The model can explain {0:.1%} of the variance in the test set using {1:} features.'.format(r_squared, len(lm.coef_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating new features based on the existing ones while trying to lose as little information as possible. It creates news features, which are in fact combinations of the original ones. \n",
    "\n",
    "PCA:\n",
    "\n",
    "For this technique, it is important to scale the features first, so that their values are easier to compare. \n",
    "\n",
    "We can add a reference point to the very center of the point cloud, and then point a vector in the direction of this strongest pattern. We can add a second vector perpendicular to the first one to account for the rest of the variance in this dataset. \n",
    "\n",
    "Every point in the dataset could be described by multiplying and then summing two perpendicular vectors. We essentially created a new reference system aligned with the variance in the data. The coordinates that each point has in this new reference system are called principal components, and they are the foundation of principal component analysis (PCA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Future generation -BMI\n",
    "df_body['BMI'] = df_body['Weight kg'] / df_body['Height m'] ** 2\n",
    "df_body.drop(['Weight kg','Height m'], axis=1)\n",
    "\n",
    "# Future generation - averages \n",
    "leg_df['leg mm'] = leg_df[['right leg mm','left leg mm']].mean(axis=1)\n",
    "\n",
    "# Into to PCA\n",
    "sns.scatterplot(data=df, x='handlength', y='footlength')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_std = pd.DataFrame(scaler.fit_transform(df), columns = df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Principal Componenet Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principals share no duplicate information and that they are ranked from most to least important. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the principal components\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "std_df = scaler.fit_transform(df)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "print(pca.fit_transform(std_df))\n",
    "\n",
    "# Principal component explained variance ratio\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "pca.fit(std_df)\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "# PCA for dimensionality reduction\n",
    "pca = PCA()\n",
    "pca.fit(ansur_std_df)\n",
    "print(pca.explained_variance_ratio_.cumsum())\n",
    "\n",
    "# Understanding the components\n",
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pairplot to inspect ansur_df\n",
    "sns.pairplot(ansur_df)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create the scaler and standardize the data\n",
    "scaler = StandardScaler()\n",
    "ansur_std = scaler.fit_transform(ansur_df)\n",
    "\n",
    "# Create the PCA instance and fit and transform the data with pca\n",
    "pca = PCA()\n",
    "pc = pca.fit_transform(ansur_std)\n",
    "\n",
    "# This changes the numpy array output back to a dataframe\n",
    "pc_df = pd.DataFrame(pc, columns=['PC 1', 'PC 2', 'PC 3', 'PC 4'])\n",
    "\n",
    "# Create a pairplot of the principal component dataframe\n",
    "sns.pairplot(pc_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA on a larger dataset\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "ansur_std = scaler.fit_transform(ansur_df)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "pca.fit(ansur_std)\n",
    "\n",
    "# Inspect the explained variance ratio per component\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "# Print the cumulative sum of the explained variance ratio\n",
    "print(pca.explained_variance_ratio_.cumsum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PCA applications**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding the components\n",
    "print(pca.components_)\n",
    "\n",
    "# PCA in a pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('reducer', PCA())])\n",
    "pc = pipe.fit_transform(ansur_df)\n",
    "print(pc[:,:2])\n",
    "\n",
    "# Checking the effect of categorical features\n",
    "ansur_categories['PC 1'] = pc[:,0]\n",
    "ansur_categories['PC 2'] = pc[:,1]\n",
    "sns.scatterplot(data=ansur_categories, x='PC 1', \n",
    "                y='PC 2',hue='Height_class', alpha=0.4)\n",
    "\n",
    "# PCA in a model pipeline\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('reducer', PCA(n_components=3)),\n",
    "    ('classifier', RandomForestClassifier())])\n",
    "pipe.fit(X_train, y_train)\n",
    "print(pipe.steps[1])\n",
    "\n",
    "# PCA in a model pipeline\n",
    "pipe.steps[1][1].explained_variance_ratio_.cumsum()\n",
    "print(pipe.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the pipeline\n",
    "pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                 ('reducer', PCA(n_components=2))])\n",
    "\n",
    "# Fit it to the dataset and extract the component vectors\n",
    "pipe.fit(poke_df)\n",
    "vectors = pipe.steps[1][1].components_.round(2)\n",
    "\n",
    "# Print feature effects\n",
    "print('PC 1 effects = ' + str(dict(zip(poke_df.columns, vectors[0]))))\n",
    "print('PC 2 effects = ' + str(dict(zip(poke_df.columns, vectors[1]))))\n",
    "\n",
    "# PCA for feature exploration\n",
    "# Build the pipeline\n",
    "pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                 ('reducer', PCA(n_components=2))])\n",
    "\n",
    "# Fit the pipeline to poke_df and transform the data\n",
    "pc = pipe.fit_transform(poke_df)\n",
    "\n",
    "print(pc)\n",
    "\n",
    "# Add the 2 components to poke_cat_df\n",
    "poke_cat_df['PC 1'] = pc[:, 0]\n",
    "poke_cat_df['PC 2'] = pc[:, 1]\n",
    "\n",
    "print(poke_cat_df.head())\n",
    "\n",
    "# Use the Type feature to color the PC 1 vs PC 2 scatterplot\n",
    "sns.scatterplot(data=poke_cat_df, \n",
    "                x='PC 1', y='PC 2', hue='Type')\n",
    "plt.show()\n",
    "\n",
    "### PCA in a model pipeline\n",
    "\n",
    "# Build the pipeline\n",
    "pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('reducer', PCA(n_components=2)),\n",
    "        ('classifier', RandomForestClassifier(random_state=0))])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Prints the explained variance ratio\n",
    "print(pipe.steps[1][1].explained_variance_ratio_)\n",
    "\n",
    "# Score the accuracy on the test set\n",
    "accuracy = pipe.score(X_test, y_test)\n",
    "\n",
    "# Prints the model accuracy\n",
    "print('{0:.1%} test set accuracy'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Principal Component Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting an explained variance threshold\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('reducer', PCA(n_components=0.9))])\n",
    "# Fit the pipe to the data\n",
    "pipe.fit(poke_df)\n",
    "print(len(pipe.steps[1][1].components_))\n",
    "\n",
    "# An optimal number of components --- > 'Elbow' in the plot\n",
    "pipe.fit(poke_df)\n",
    "var = pipe.steps[1][1].explained_variance_ratio_\n",
    "plt.plot(var)\n",
    "plt.xlabel('Principal component index')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.show()\n",
    "\n",
    "# Compressing images\n",
    "pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                 ('reducer', PCA(n_components=290))])\n",
    "pipe.fit(X_train)\n",
    "pc = pipe.fit_transform(X_test)\n",
    "print(pc.shape)\n",
    "\n",
    "# Rebuilding images\n",
    "pc = pipe.transform(X_test)\n",
    "print(pc.shape)\n",
    "\n",
    "X_rebuilt = pipe.inverse_transform(pc)\n",
    "print(X_rebuilt.shape)\n",
    "\n",
    "img_plotter(X_rebuilt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the proportion of variance to keep\n",
    "# Pipe a scaler to PCA selecting 80% of the variance\n",
    "pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                 ('reducer', PCA(n_components=0.8))])\n",
    "\n",
    "# Fit the pipe to the data\n",
    "pipe.fit(ansur_df)\n",
    "\n",
    "print('{} components selected'.format(len(pipe.steps[1][1].components_)))\n",
    "\n",
    "\n",
    "## Choosing the number of components ##\n",
    "\n",
    "# Pipeline a scaler and pca selecting 10 components\n",
    "pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                 ('reducer', PCA(n_components=10))])\n",
    "\n",
    "# Fit the pipe to the data\n",
    "pipe.fit(ansur_df)\n",
    "\n",
    "# Plot the explained variance ratio\n",
    "plt.plot(pipe.steps[1][1].explained_variance_ratio_)\n",
    "\n",
    "plt.xlabel('Principal component index')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.show()\n",
    "\n",
    "## PCA for image compression ## \n",
    "\n",
    "# Plot the MNIST sample\n",
    "plot_digits(X_test)\n",
    "\n",
    "\n",
    "# Transform the input data to principal components\n",
    "pc = pipe.transform(X_test)\n",
    "\n",
    "# Prints the number of features per dataset\n",
    "print(\"X_test has {} features\".format(X_test.shape[1]))\n",
    "print(\"pc has {} features\".format(pc.shape[1]))\n",
    "\n",
    "# Inverse transform the components to original feature space\n",
    "X_rebuilt = pipe.inverse_transform(pc)\n",
    "\n",
    "# Prints the number of features\n",
    "print(\"X_rebuilt has {} features\".format(X_rebuilt.shape[1]))\n",
    "\n",
    "\n",
    "# Transform the input data to principal components\n",
    "pc = pipe.transform(X_test)\n",
    "\n",
    "# Inverse transform the components to original feature space\n",
    "X_rebuilt = pipe.inverse_transform(pc)\n",
    "\n",
    "# Plot the reconstructed data\n",
    "plot_digits(X_rebuilt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
